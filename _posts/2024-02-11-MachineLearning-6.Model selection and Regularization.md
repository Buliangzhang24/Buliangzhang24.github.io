# Motivation 
- prediction accuracy
    avoid overfitting ç‰¹åˆ«æ˜¯p>n çš„æ—¶å€™ï¼Œå°±æ˜¯å˜é‡å¤§äºæ ·æœ¬é‡
- model interpreteability
- <mark class="hltr-blue">lower practical requirements:fewer measurements, less computation</mark>
-  <mark class="hltr-blue">visualization</mark>, if selecting 2-3 features
# Introduction
linear model, least squares minimization
## Solution
- feature subset selection: select
- feature extraction(PCA): map to 
- shrinkage:  adjust coefficients so that some features are used to a lesser extent (or not at all)
- regularization: reducing the model flexibility
# Feature subset selection
- Basic approach: select the best features
- <mark class="hltr-blue">Problem</mark>:  the k best individual features are not necessarily the best set of k features(bc, the corelated )
## Best Subset Selection(Exhaustive)
- Search space 2^p -1 subsets
- Why not try all possible subsets then? 
    è™½ç„¶å°è¯•æ‰€æœ‰å¯èƒ½çš„å­é›†æ˜¯ç†è®ºä¸Šçš„å®Œç¾æ–¹æ³•ï¼Œä½†åœ¨å®é™…æƒ…å†µä¸‹ï¼Œç”±äºæœç´¢ç©ºé—´çš„å·¨å¤§ï¼Œè¿™å‡ ä¹æ˜¯ä¸å¯è¡Œçš„
- algorithm: 
     ä¸ºäº†åœ¨è¿™ä¸ªåºå¤§çš„æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³çš„å­é›†ï¼Œæœ€ä½³å­é›†é€‰æ‹©æ–¹æ³•é€šå¸¸ä½¿ç”¨è´ªå©ªç®—æ³•ã€‚è¯¥ç®—æ³•ä»ç©ºé›†å¼€å§‹ï¼Œç„¶ååœ¨æ¯ä¸€æ­¥ä¸­æ·»åŠ ä¸€ä¸ªç‰¹å¾ï¼Œä»è€Œæ„å»ºä¸€ä¸ªç‰¹å¾å­é›†ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•è¯„ä¼°å½“å‰ç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©å¯¹æ€§èƒ½æœ‰æœ€å¤§è´¡çŒ®çš„ç‰¹å¾æ·»åŠ åˆ°å­é›†ä¸­ã€‚è¯¥è¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡kä¸ºæ­¢ã€‚
     - let M_0 denote the null model
     - for k =1,2....p>fit all models that use k predictors> M_k is the best of these(smallest RSS or largest R^2)> select a single best model from M_0....M_p based on CV error, C,AIC,BIC,adjusted R^2
- <mark class="hltr-blue">ç¼ºç‚¹</mark>
     - computationally intensive 
     - when ğ‘ is large, risk finding models that do well on training data but do <mark class="hltr-cyan">not generalize well</mark>
## Stepwise Selection
### Forward Selection
- start with no predictors, iteratively add predictors until a stop criterion is reached
- <mark class="hltr-blue">Computational advantage is clear, but may miss optimal subset</mark>

### Backward Selection
start with all predictors, iteratively removepredictors until a stop criterion is reached
- Computational advantage is clear, but may miss optimal subset
- Backward selection requires that <mark class="hltr-blue">ğ‘› > ğ‘ </mark>for fitting the full model, forward selection does not
### both 
- both add/remove in each step the predictor that gives the greatest improvement/smallest deterioration
- search through only 1+p(p+1)/2
- are not guaranteed to find the best mode
- Hybrid approaches (â€œ+l-râ€):take l steps forward,then r steps back
## Choosing the optimal model
- Full model will always have the smallest RSS and the largest ğ‘…2 â€“ on the <mark class="hltr-blue">training data</mark>would like to select model that performs best on <mark class="hltr-blue">test data</mark>
- æ€ä¹ˆé€‰æ‹©
    -  <mark class="hltr-blue">estimate test error indirectly</mark>, based on the training error: fast, but based on assumptions
    - <mark class="hltr-blue">estimate the test error directly</mark>,using a test set or cross-validation:more accurate, but slower
### Test set error estimates
Mallow's C_p 
AIC
BIC
Adjustes R^2
### One standard-error rule
![[38046a12348c1f0c3e0b13fcbd5f395.png]]
**Exercise**
1.We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2,...,p predictors. Explain your answers:
(a) Which of the three models with k predictors has the smallest
<mark class="hltr-blue">training RSS?</mark>
(a) best subset//on training
(b) Which of the three models with k predictors has the smallest
<mark class="hltr-blue">test RSS?</mark>
(b)it depends???<mark class="hltr-blue"> best subset</mark>
(c) True or False:
i. The predictors in the k-variable model identified by *forward stepwise* are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.
ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by backward stepwise selection.
iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection.
iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.
v.<mark class="hltr-blue"> The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection</mark>
(the best subset of k+1 does not necessarily contain the best subset of k variables)
(c)True,False(True),False,False,True(Falseä¼šå˜)
# Shrinkage
increased bias,
reduced variance
- adjust coefficients so that some features are used to a lesser extent (or not at all)
- Alternative to subset selection:shrink coefficients towards zero by constraining or regularizing 
## Ridge Regression
![[5f76602c2bd4d5840473f4bdfada468.png]]
- ç¬¬äºŒé¡¹æ˜¯shrinkage penalty: å½“Bæ¥è¿‘0ï¼Œ è¿™ä¸ªä¹Ÿå¾ˆå°
- Tuning parameter alamda control relative impact: ä¸º0 å°±æ˜¯æ™®é€šçš„least squares, æ— é™å¤§å°±all ğ›½=s will be zero (only intercept estimated)
- Selecting ğœ† is critical: use cross-validation
![[2f3a0d80981ccd473cdd2bbe362b468.png]]
### Scaling
- Standard least squares is scale equivariant: multiplying ğ‘‹ğ‘— by a constant ğ‘ means ğ›½ğ‘— scales by a factor of 1/ğ‘: ğ‘‹ğ‘— ğ›½ğ‘— will remain the same.
- Ridge regression is scale-sensitive, due to the penalty
- <mark class="hltr-blue">Solution: normalize predictors first</mark>
### Bias-variance
- Ridge regression reduces flexibility: decreases variance at the cost of increased bias
- ğœ† controls flexibility of predictor
![[d5dfa2fb2799146edd09a4994ea7a43.png]]
<mark class="hltr-blue">With increasing dataset size,variance component decreases, bias stays the same flexibility </mark>
variance æ˜¯å…³æ³¨æ•æ„Ÿåº¦ï¼Œbiasæ˜¯å…³æ³¨çµæ´»åº¦ï¼Œå¦‚æœæ•°æ®å˜å¤šï¼Œvarianceå°±ä¼šæ›´ä¸æ•æ„Ÿï¼Œæ‰€ä»¥å°±ä¸‹é™ã€‚
## The Lasso
- Disadvantage of ridge regression: all ğ‘ predictors are still used in the final mode
![[cd6f3e58f5d5e26790efb184abcdddd.png]]
- Also shrinks coefficients towards zero, but ğ‘™1 penalty forces some to be exactly zero
- actually performs variable subset selection
- yields sparse models, with just a small set of variables ç¨€ç–æ¨¡å‹
## Lasso vs. ridge
Why does lasso give coefficients of zero?
![[3e6d927a87d19d9b3a06d8b91a88ae5.png]]
2.For parts (a) through (c), indicate which of i. through iv. is correct.
Justify your answer.
(a) The lasso, relative to least squares, is:
i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.
<mark class="hltr-blue">ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. </mark>
<mark class="hltr-blue">iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.</mark>
iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.
(b) Repeat (a) for ridge regression relative to least squares.
(c) Repeat (a) for non-linear methods relative to least squares.
2.i, i, iv>>> iii,iii, ii

4.Suppose we estimate the regression coefficients in a linear regression
model by minimizing for a particular value of Î». For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer. 
i. Increase initially, and then eventually start decreasing in an
inverted U shape.
ii. Decrease initially, and then eventually start increasing in a U shape.
iii. Steadily increase.
iv. Steadily decrease.
v. Remain constant.
(a) As we increase Î» from 0, the training RSS will:iii
(b) Repeat (a) for test RSS. ii
(c) Repeat (a) for variance. iv
(d) Repeat (a) for (squared) bias. iii
(e) Repeat (a) for the irreducible error. v
4, iv, ii, ii, iv, v.>>><mark class="hltr-blue"> iii,ii,</mark> iv ,iii , v
(å°±æ˜¯lamdaå˜åŒ–ï¼Œ æ¨¡å‹çš„å¤æ‚åº¦ä¹Ÿä¼šå˜åŒ–ï¼Œç„¶åå†æƒ³è¿™ä¸ªä¸œè¥¿æ€ä¹ˆå˜åŒ–)
# High-dimensional spaces
Modern sensors yield many variables - often ğ‘ â‰« n
Most (statistical) approaches were developed for problems with few variables
## Overfitting
- Least squares does not work when ğ‘ > ğ‘› : too flexible
- Even when ğ‘ < ğ‘›, danger of overfitting
## Curse of dimensionality
å°±æ˜¯æœ¬æ¥è¶Šå¤šçš„features åº”è¯¥ç»™æˆ‘ä»¬æ›´å¤šçš„ä¿¡æ¯ï¼Œä½†æ˜¯ the number of <mark class="hltr-blue">parameters</mark> to estimate increases with the number of <mark class="hltr-blue">measurements</mark>
ä¸ºäº†ä¼°è®¡è¿™äº›Parameterså°±è¦æ›´å¤šçš„samples.
Consequence: given a certain number of samples there is an optimal number of features to use.
## Model performance vs. flexibility
 largest risk in modelling is overfittingå°±æ˜¯å› ä¸ºå¯¹è®­ç»ƒé›†æ‹Ÿåˆçš„å¤ªå¥½äº†ï¼Œå¯¼è‡´ä¸èƒ½å¾ˆå¥½çš„æ³›åŒ–åˆ«çš„æ•°æ®é›†äº†
 Model overfits when it performs (much) better on training data than on test data
 ![[424cf1397ff85d83912dbf1504b867b.png]]