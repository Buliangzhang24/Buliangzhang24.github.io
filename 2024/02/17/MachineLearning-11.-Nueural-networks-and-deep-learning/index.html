<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜11. Nueural networks and deep learning &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/17/MachineLearning-11.-Nueural-networks-and-deep-learning/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜11. Nueural networks and deep learning"><meta name="keywords" content="Nueural networks and deep learning"><meta name="og:keywords" content="Nueural networks and deep learning"><meta name="description" content="Neurons and gradient descent Neural nets are interconnected networks of simple processing units, a.k.a neurons It remains just a parallel, the artificial neuron is just an approximation!![[e1854a76c71801fa05b63cdcc8df20e.png]]The output y was modelled as a weighted linear combination of inputs xj. Moreover, a transfer function, or activation function was used to make a decision. Gradient descent–how to find the right weights? ![[fcd04d013b82d719b47394c807468bf.png]] We want to find the minimum of an error function We start with an initial guess of the parameter b We change its value in the direction of maximal slope (导数Derivatives are slopes) We continue until reaching a minimum Steps ![[406563f7a1612cd22c7d022a03ec8bf.png]] To update a weight b, we remove to its value the derivative a is called a learning rate»It decides by how much you multiply the step vector»Large a can lead to faster convergence than small» too large a can lead to disaster r is the iteration Why the minus sign? Because we want to move towards a minimum! Momentum ![[180660bff8543cd83c9572f963619a1.png]] Numerical example Learning rate controls oscillation and speedMomentum uses a bit of the previous step![[3d375e4b508a68b06aee5e1f50e64cd.png]]![[42d9ae86c4a881a9763de2dfe7a21a9.jpg]] the models using them multi-layer perceptron convolutional neural networks ( deep learning) Multilayer perceptron (MLP) (from linear classifier to nonlinear version) It’s a feed forward network: it goes from inputs to outputs, without loops Every neuron includes an activation function (e.g. sigmoid, see earlier slides)![[3948b8cc0b8edb04747f9b1872eb42e.png]] Chain rule How to compute gradient descent on a cascade级联 of operations? How to reach all trainable weights?![[7f0b6cde9605e7be94f1fef714e9d67.png]]![[245f99775be317ce5f517f38306268d.png]] The forward pass: you put a data point in and obtain the prediction The Backward pass: you update parameters by back-propagating errorsexerciseDescribe the architecture of the following network.● How many layers are there? 2● How many neurons per layer? 3,1● How many trainable parameters (weights and biases)?10 Convolutional Neural Networks(CNN) they proposed a network learning spatial filters on top of spatial filters. They were (and are) called convolutional neural networks The CNN was considered interesting, but very hard to train. It needed● Loads of training data &gt; nobody had them● A lot of computational power &gt; same Change big datagraphic processing How to work They are conceptually very similar to MLPs But their weights (b) are 2D convolutional filters For this, they are very well suited for images![[d46bfe4432033414ed2e96953c55ef5.png]]![[4fb227f9d791d464005deebfb4d4eea.png]] In convolutional neural networks, the filters are spatial (on 2D grids). ● local : they convolve the values of the image in a local window● shared: the same filter is applied everywhere in the image: why shared? “Recycling” the same operation allows to have much less parameters to learn! Learn By “learns”, I mean adapt filters weights to minimize prediction error (i.e. backpropagation) Steps Convolutional filters start with random numbers Iteratively they are improved: each coefficient is updated in the direction of largest gradient of the cost function At the end, the filters become quite meaningful! Summary Perceptrons are “neuron-inspired” linear discriminants Multilayer perceptrons are trainable, nonlinear discriminants Feed-forward neural networks in general can be used for classification, regression and feature extraction There is a large body of alternative neural nets Key problems in the application of ANNs are choosing the right size and good training parameters Convolutional neural networks have a constrained architecture encoding prior knowledge of the problem Deep learning is concerned with constructing extremely large neural networks that depend on:● special hardware (GPUs), to be able to train them● specific tricks (e.g. rectified linear units) to prevent overfitting"><meta name="og:description" content="Neurons and gradient descent Neural nets are interconnected networks of simple processing units, a.k.a neurons It remains just a parallel, the artificial neuron is just an approximation!![[e1854a76c71801fa05b63cdcc8df20e.png]]The output y was modelled as a weighted linear combination of inputs xj. Moreover, a transfer function, or activation function was used to make a decision. Gradient descent–how to find the right weights? ![[fcd04d013b82d719b47394c807468bf.png]] We want to find the minimum of an error function We start with an initial guess of the parameter b We change its value in the direction of maximal slope (导数Derivatives are slopes) We continue until reaching a minimum Steps ![[406563f7a1612cd22c7d022a03ec8bf.png]] To update a weight b, we remove to its value the derivative a is called a learning rate»It decides by how much you multiply the step vector»Large a can lead to faster convergence than small» too large a can lead to disaster r is the iteration Why the minus sign? Because we want to move towards a minimum! Momentum ![[180660bff8543cd83c9572f963619a1.png]] Numerical example Learning rate controls oscillation and speedMomentum uses a bit of the previous step![[3d375e4b508a68b06aee5e1f50e64cd.png]]![[42d9ae86c4a881a9763de2dfe7a21a9.jpg]] the models using them multi-layer perceptron convolutional neural networks ( deep learning) Multilayer perceptron (MLP) (from linear classifier to nonlinear version) It’s a feed forward network: it goes from inputs to outputs, without loops Every neuron includes an activation function (e.g. sigmoid, see earlier slides)![[3948b8cc0b8edb04747f9b1872eb42e.png]] Chain rule How to compute gradient descent on a cascade级联 of operations? How to reach all trainable weights?![[7f0b6cde9605e7be94f1fef714e9d67.png]]![[245f99775be317ce5f517f38306268d.png]] The forward pass: you put a data point in and obtain the prediction The Backward pass: you update parameters by back-propagating errorsexerciseDescribe the architecture of the following network.● How many layers are there? 2● How many neurons per layer? 3,1● How many trainable parameters (weights and biases)?10 Convolutional Neural Networks(CNN) they proposed a network learning spatial filters on top of spatial filters. They were (and are) called convolutional neural networks The CNN was considered interesting, but very hard to train. It needed● Loads of training data &gt; nobody had them● A lot of computational power &gt; same Change big datagraphic processing How to work They are conceptually very similar to MLPs But their weights (b) are 2D convolutional filters For this, they are very well suited for images![[d46bfe4432033414ed2e96953c55ef5.png]]![[4fb227f9d791d464005deebfb4d4eea.png]] In convolutional neural networks, the filters are spatial (on 2D grids). ● local : they convolve the values of the image in a local window● shared: the same filter is applied everywhere in the image: why shared? “Recycling” the same operation allows to have much less parameters to learn! Learn By “learns”, I mean adapt filters weights to minimize prediction error (i.e. backpropagation) Steps Convolutional filters start with random numbers Iteratively they are improved: each coefficient is updated in the direction of largest gradient of the cost function At the end, the filters become quite meaningful! Summary Perceptrons are “neuron-inspired” linear discriminants Multilayer perceptrons are trainable, nonlinear discriminants Feed-forward neural networks in general can be used for classification, regression and feature extraction There is a large body of alternative neural nets Key problems in the application of ANNs are choosing the right size and good training parameters Convolutional neural networks have a constrained architecture encoding prior knowledge of the problem Deep learning is concerned with constructing extremely large neural networks that depend on:● special hardware (GPUs), to be able to train them● specific tricks (e.g. rectified linear units) to prevent overfitting"><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/17/MachineLearning-11.-Nueural-networks-and-deep-learning/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-17"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜11. Nueural networks and deep learning</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/17 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 3471 字，约 10 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"> <img style="height:72px;width:72px" src="https://buliangzhang24.github.io/assets/images/qrcode.jpg" alt="Buliangzhang" /></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h1 id="neurons-and-gradient-descent">Neurons and gradient descent</h1><ul><li>Neural nets are <mark class="hltr-yellow">interconnected networks</mark> of simple processing units, a.k.a neurons</li><li>It remains just a parallel, the artificial neuron is just an approximation! ![[e1854a76c71801fa05b63cdcc8df20e.png]] The output y was modelled as a weighted linear combination of inputs xj. Moreover, a transfer function, or activation function was used to make a decision.<h2 id="gradient-descenthow-to-find-the-right-weights">Gradient descent–how to find the right weights?</h2><p>![[fcd04d013b82d719b47394c807468bf.png]]</p></li><li>We want to find the minimum of an error function</li><li>We start with an initial guess of the parameter b</li><li>We change its value in the direction of <mark class="hltr-cyan">maximal slope</mark> (导数Derivatives are slopes)</li><li>We continue until reaching a minimum<h3 id="steps">Steps</h3><p>![[406563f7a1612cd22c7d022a03ec8bf.png]]</p></li><li>To update a weight b, we remove to <mark class="hltr-cyan">its value the derivative</mark></li><li>a is called a <mark class="hltr-cyan">learning rate</mark>»It decides by how much you multiply the step vector»Large a can lead to faster convergence than small» too large a can lead to disaster</li><li>r is the <mark class="hltr-cyan">iteration</mark></li><li>Why the minus sign? Because we want to move towards a minimum!<h3 id="momentum">Momentum</h3><p>![[180660bff8543cd83c9572f963619a1.png]]</p><h3 id="numerical-example">Numerical example</h3><p>Learning rate controls oscillation and speed Momentum uses a bit of the previous step ![[3d375e4b508a68b06aee5e1f50e64cd.png]] ![[42d9ae86c4a881a9763de2dfe7a21a9.jpg]]</p><h1 id="the-models-using-them-multi-layer-perceptron-convolutional-neural-networks--deep-learning">the models using them multi-layer perceptron convolutional neural networks ( deep learning)</h1><h2 id="multilayer-perceptron-mlp">Multilayer perceptron (MLP)</h2><p>(from linear classifier to nonlinear version)</p></li><li>It’s a feed forward network: it goes from inputs to outputs, without loops</li><li>Every <mark class="hltr-yellow">neuron</mark> includes an activation function (e.g. sigmoid, see earlier slides) ![[3948b8cc0b8edb04747f9b1872eb42e.png]]<h3 id="chain-rule">Chain rule</h3></li><li>How to compute gradient descent on a cascade级联 of operations?</li><li>How to reach all trainable weights? ![[7f0b6cde9605e7be94f1fef714e9d67.png]] ![[245f99775be317ce5f517f38306268d.png]]<ol><li>The forward pass: you put a data point in and obtain the prediction</li><li>The Backward pass: you update parameters by back-propagating errors <strong>exercise</strong> Describe the architecture of the following network. ● How many layers are there? 2 ● How many neurons per layer? 3,1 ● How many trainable parameters (weights and biases)?10<h2 id="convolutional-neural-networkscnn">Convolutional Neural Networks(CNN)</h2></li></ol></li><li>they proposed a network learning spatial filters on top of spatial filters. They were (and are) called convolutional neural networks</li><li>The CNN was considered interesting, but very hard to train. It needed● Loads of training data &gt; nobody had them● A lot of computational power &gt; same<h3 id="change">Change</h3><p>big data graphic processing</p><h3 id="how-to-work">How to work</h3></li><li>They are conceptually very similar to MLPs</li><li>But their weights (b) are <mark class="hltr-green">2D convolutional</mark> filters</li><li>For this, they are very well suited for images ![[d46bfe4432033414ed2e96953c55ef5.png]] ![[4fb227f9d791d464005deebfb4d4eea.png]]</li><li>In convolutional neural networks, the filters are spatial (on 2D grids). ● local : they convolve the values of the image in a local window● shared: the same filter is applied everywhere in the image: why shared? “Recycling” the same operation allows to have much less parameters to learn!<h3 id="learn">Learn</h3></li><li>By “learns”, I mean adapt filters weights to minimize prediction error (i.e. backpropagation)<h3 id="steps-1">Steps</h3></li><li>Convolutional filters start with random numbers</li><li>Iteratively they are improved: each coefficient is updated <mark class="hltr-yellow">in the direction of largest gradient of the cost function</mark></li><li>At the end, the filters become quite meaningful!<h1 id="summary">Summary</h1></li><li>Perceptrons are “neuron-inspired” linear discriminants</li><li><mark class="hltr-cyan">Multilayer perceptrons</mark> are trainable, <mark class="hltr-cyan">nonlinear</mark> discriminants</li><li><mark class="hltr-cyan">Feed-forward neural networks</mark> in general can be used for <mark class="hltr-cyan">classification, regression and feature extraction</mark></li><li>There is a large body of alternative neural nets</li><li>Key problems in the application of ANNs are choosing the right size and good training parameters</li><li>Convolutional neural networks have a constrained architecture encoding prior knowledge of the problem</li><li>Deep learning is concerned with constructing extremely large neural networks that depend on:● special hardware (GPUs), to be able to train them● specific tricks (e.g. rectified linear units) to prevent overfitting</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/17/MachineLearning-11.-Nueural-networks-and-deep-learning/" target="_blank">https://buliangzhang24.github.io/2024/02/17/MachineLearning-11.-Nueural-networks-and-deep-learning/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727730341', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
