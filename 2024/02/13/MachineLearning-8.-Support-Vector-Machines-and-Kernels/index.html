<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜8. Support Vector Machines and Kernels &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/13/MachineLearning-8.-Support-Vector-Machines-and-Kernels/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜8. Support Vector Machines and Kernels"><meta name="keywords" content="Support Vector Machines and Kernels"><meta name="og:keywords" content="Support Vector Machines and Kernels"><meta name="description" content="![[cd673ddf50ef87de550f7a2cba69a53.png]]Separating Hyperplanes when you want to find a decision boundary, avoid estimating densities linear decision boundry,就是设置这个式子为0，就是它的decision boundryhyperplanes separate feature space into regions![[ca46b3d627febe82dbbee7126a1c74a.png]] for new X, 可以带入最后一个式子，然后看y=1,还是y=-1来分类exercise1. This problem involves hyperplanes in two dimensions.(a) Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set ofpoints for which 1 + 3X1 − X2 &gt; 0, as well as the set of pointsfor which 1 + 3X1 − X2 &lt; 0.(b) On the same plot, sketch the hyperplane −2 + X1 + 2X2 = 0.Indicate the set of points for which −2 + X1 + 2X2 &gt; 0, as wellas the set of points for which −2 + X1 + 2X2 &lt; 0. Maximum margin classifiers （怎么找一个最佳的hyperplane） generalization link to regularization for generalization, the decision boundary should lie between the class boundaries Maximize perpendicular distance(最大化垂直距离) between the decision boundary and the ==nearest observations==: the margin M the decision boundary then only depends on a few points on the margin, the support vectors:if you remove one from other ob, nothing changes 所以， leave one out will fail Construction maximize the margin（尝试最大边际化）, under the constraint that training observations are classified correctly Limitation ● separable classes● linear separability● two classes Problems Maximum margin classifier is prone to ==overfitting==: very sensitive to training set When classes ==overlap==, separating hyperplane does not exist We need to make a trade-off between errors on the training set and predicted performance on the test set (generalization) Solution: The soft margin 为了解决上面的问题 Solution: ==allow (some, small) errors on the training set,==introducing slack （松弛）variables ∊i ≥ 0»&gt;Add slack variables to maximum margin classifier, but limit total slack to C: the trade-off parameter 变化：M» M(1-e)，然后引入一个C，误差平方和&lt;=C C influence solution: There is no a prori best choice for C![[7b9e34d7c7461765eca88a763603261.png]] The multi-class case one-versus-one one- versus-allexercise ![[b5b665fde36990c4b2cb1fd1525e3e6.png]] Optimization(optional) 从最大化M(margin)到最小化一个系数w和常数b的平方和但是为了解决is a (large) quadratic programming (QP) problem又引入了lagrange multipliers alphai The nonlinear case Ideal: classes may become linearly separable if higher order terms are added(cf. nonlinear regression)![[22e573c5252d8db1948d586ddaa54dd.png]] Questions feature是要平方还是开方，等等，我不知道 efficiently train the SVC,就是p= 10 ,如果要3次，就会有286种组合方式了 The Kernel Trick ![[4a4865d0f12df10785319a4f7353e67.png]]训练阶段，我们需要计算所有训练样本之间的内积，如 xi 和 xi’ 之间的内积。训练完成后，当我们需要对新的样本进行分类时，我们只需要计算支持向量 (support vectors) 与新样本之间的内积。![[bc20a49a601a7f34eaf96a7d8938959.png]]引入一个用于泛化的核函数Think of kernel functions as similarities: large when the inputs are very alike, small when they are not![[ff68f02e96fdb61695f150a047643ef.png]]![[50b2ca5c56b26fa1a3d05635b85e2dd.png]]到这一步了，才有两种方法可以选择K（x_i, x） Polynomial kernel Radial kernel The support vector machine Choosing Kernels What kernel functions should we use?type: prior knowledge of problem, trial-and-errorparameters: cross-validation, like for Cexercise![[f915649b7e14afa4dc5ab759f53800c.png]] More Kernels A large number of kernels have been proposed,not limited to numerical/vector data!● Vector kernels● Set kernels● String kernels● Empirical kernel map● Kernel kernels● Kernel combination● Kernels on graphs● Kernels in graphs● Kernels on probabilistic models Recap Separating hyperplane: any plane that separates classes The support vector machine has evolved from fundamental work by Vapnik: separable, linear problems: maximum margin classifier non-separable, linear problems - add slack variables:support vector classifier non-separable, non-linear problems - use kernel trick:support vector machine Training involves quadratic programming (optimization) The final classifier only depends on the support vectors"><meta name="og:description" content="![[cd673ddf50ef87de550f7a2cba69a53.png]]Separating Hyperplanes when you want to find a decision boundary, avoid estimating densities linear decision boundry,就是设置这个式子为0，就是它的decision boundryhyperplanes separate feature space into regions![[ca46b3d627febe82dbbee7126a1c74a.png]] for new X, 可以带入最后一个式子，然后看y=1,还是y=-1来分类exercise1. This problem involves hyperplanes in two dimensions.(a) Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set ofpoints for which 1 + 3X1 − X2 &gt; 0, as well as the set of pointsfor which 1 + 3X1 − X2 &lt; 0.(b) On the same plot, sketch the hyperplane −2 + X1 + 2X2 = 0.Indicate the set of points for which −2 + X1 + 2X2 &gt; 0, as wellas the set of points for which −2 + X1 + 2X2 &lt; 0. Maximum margin classifiers （怎么找一个最佳的hyperplane） generalization link to regularization for generalization, the decision boundary should lie between the class boundaries Maximize perpendicular distance(最大化垂直距离) between the decision boundary and the ==nearest observations==: the margin M the decision boundary then only depends on a few points on the margin, the support vectors:if you remove one from other ob, nothing changes 所以， leave one out will fail Construction maximize the margin（尝试最大边际化）, under the constraint that training observations are classified correctly Limitation ● separable classes● linear separability● two classes Problems Maximum margin classifier is prone to ==overfitting==: very sensitive to training set When classes ==overlap==, separating hyperplane does not exist We need to make a trade-off between errors on the training set and predicted performance on the test set (generalization) Solution: The soft margin 为了解决上面的问题 Solution: ==allow (some, small) errors on the training set,==introducing slack （松弛）variables ∊i ≥ 0»&gt;Add slack variables to maximum margin classifier, but limit total slack to C: the trade-off parameter 变化：M» M(1-e)，然后引入一个C，误差平方和&lt;=C C influence solution: There is no a prori best choice for C![[7b9e34d7c7461765eca88a763603261.png]] The multi-class case one-versus-one one- versus-allexercise ![[b5b665fde36990c4b2cb1fd1525e3e6.png]] Optimization(optional) 从最大化M(margin)到最小化一个系数w和常数b的平方和但是为了解决is a (large) quadratic programming (QP) problem又引入了lagrange multipliers alphai The nonlinear case Ideal: classes may become linearly separable if higher order terms are added(cf. nonlinear regression)![[22e573c5252d8db1948d586ddaa54dd.png]] Questions feature是要平方还是开方，等等，我不知道 efficiently train the SVC,就是p= 10 ,如果要3次，就会有286种组合方式了 The Kernel Trick ![[4a4865d0f12df10785319a4f7353e67.png]]训练阶段，我们需要计算所有训练样本之间的内积，如 xi 和 xi’ 之间的内积。训练完成后，当我们需要对新的样本进行分类时，我们只需要计算支持向量 (support vectors) 与新样本之间的内积。![[bc20a49a601a7f34eaf96a7d8938959.png]]引入一个用于泛化的核函数Think of kernel functions as similarities: large when the inputs are very alike, small when they are not![[ff68f02e96fdb61695f150a047643ef.png]]![[50b2ca5c56b26fa1a3d05635b85e2dd.png]]到这一步了，才有两种方法可以选择K（x_i, x） Polynomial kernel Radial kernel The support vector machine Choosing Kernels What kernel functions should we use?type: prior knowledge of problem, trial-and-errorparameters: cross-validation, like for Cexercise![[f915649b7e14afa4dc5ab759f53800c.png]] More Kernels A large number of kernels have been proposed,not limited to numerical/vector data!● Vector kernels● Set kernels● String kernels● Empirical kernel map● Kernel kernels● Kernel combination● Kernels on graphs● Kernels in graphs● Kernels on probabilistic models Recap Separating hyperplane: any plane that separates classes The support vector machine has evolved from fundamental work by Vapnik: separable, linear problems: maximum margin classifier non-separable, linear problems - add slack variables:support vector classifier non-separable, non-linear problems - use kernel trick:support vector machine Training involves quadratic programming (optimization) The final classifier only depends on the support vectors"><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/13/MachineLearning-8.-Support-Vector-Machines-and-Kernels/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-13"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜8. Support Vector Machines and Kernels</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/13 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 3859 字，约 12 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"> <img style="height:72px;width:72px" src="https://buliangzhang24.github.io/assets/images/qrcode.jpg" alt="Buliangzhang" /></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><p>![[cd673ddf50ef87de550f7a2cba69a53.png]]</p><h1 id="separating-hyperplanes">Separating Hyperplanes</h1><ul><li>when you want to find a decision boundary, avoid estimating densities</li><li>linear decision boundry,就是设置这个式子为0，就是它的decision boundry hyperplanes separate feature space into regions ![[ca46b3d627febe82dbbee7126a1c74a.png]]</li><li>for new X, 可以带入最后一个式子，然后看y=1,还是y=-1来分类 <strong>exercise</strong>1. This problem involves hyperplanes in two dimensions. (a) Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set of points for which 1 + 3X1 − X2 &gt; 0, as well as the set of points for which 1 + 3X1 − X2 &lt; 0. (b) On the same plot, sketch the hyperplane −2 + X1 + 2X2 = 0. Indicate the set of points for which −2 + X1 + 2X2 &gt; 0, as well as the set of points for which −2 + X1 + 2X2 &lt; 0.<h1 id="maximum-margin-classifiers">Maximum margin classifiers</h1><p>（怎么找一个最佳的hyperplane）</p></li><li>generalization link to regularization</li><li>for generalization, the decision boundary should lie between the class boundaries</li><li>Maximize perpendicular distance(最大化垂直距离) between the decision boundary and the ==nearest observations==: the margin M the decision boundary then only depends on a few points on the margin, the support vectors:if you remove one from other ob, nothing changes</li><li>所以， leave one out will fail<h3 id="construction">Construction</h3><p>maximize the margin（尝试最大边际化）, under the constraint that training observations are classified correctly</p><h3 id="limitation">Limitation</h3><p>● separable classes ● linear separability ● two classes</p><h3 id="problems">Problems</h3></li><li>Maximum margin classifier is prone to ==overfitting==: very sensitive to training set</li><li>When classes ==overlap==, separating hyperplane does not exist</li><li>We need to make a trade-off between errors on the training set and predicted performance on the test set (generalization)<h3 id="solution-the-soft-margin">Solution: The soft margin</h3><p>为了解决上面的问题</p></li><li>Solution: ==allow (some, small) errors on the training set,==introducing slack （松弛）variables ∊i ≥ 0»&gt;Add slack variables to maximum margin classifier, but limit total slack to C: the trade-off parameter</li><li>变化：M» M(1-e)，然后引入一个C，误差平方和&lt;=C</li><li>C influence solution: There is no a prori best choice for C ![[7b9e34d7c7461765eca88a763603261.png]]<h3 id="the-multi-class-case">The multi-class case</h3></li><li>one-versus-one</li><li>one- versus-all <strong>exercise</strong> ![[b5b665fde36990c4b2cb1fd1525e3e6.png]]<h3 id="optimizationoptional">Optimization(optional)</h3><p>从最大化M(margin)到最小化一个系数w和常数b的平方和 但是为了解决is a (large) quadratic programming (QP) problem 又引入了lagrange multipliers alphai</p><h2 id="the-nonlinear-case">The nonlinear case</h2><p>Ideal: classes may become linearly separable if <mark class="hltr-green">higher order terms</mark> are added(cf. nonlinear regression) ![[22e573c5252d8db1948d586ddaa54dd.png]]</p><h3 id="questions">Questions</h3></li><li>feature是要平方还是开方，等等，我不知道</li><li>efficiently train the SVC,就是p= 10 ,如果要3次，就会有286种组合方式了<h3 id="the-kernel-trick">The Kernel Trick</h3><p>![[4a4865d0f12df10785319a4f7353e67.png]] 训练阶段，我们需要计算所有训练样本之间的内积，如 xi 和 xi’ 之间的内积。训练完成后，当我们需要对新的样本进行分类时，我们只需要计算支持向量 (support vectors) 与新样本之间的内积。 ![[bc20a49a601a7f34eaf96a7d8938959.png]] 引入一个用于泛化的核函数Think of kernel functions as similarities: large when the inputs are very alike, small when they are not ![[ff68f02e96fdb61695f150a047643ef.png]] ![[50b2ca5c56b26fa1a3d05635b85e2dd.png]] 到这一步了，才有两种方法可以选择K（x_i, x）</p></li><li>Polynomial kernel</li><li>Radial kernel<h3 id="the-support-vector-machine">The support vector machine</h3><h3 id="choosing-kernels">Choosing Kernels</h3><p>What kernel functions should we use? type: prior knowledge of problem, trial-and-error parameters: cross-validation, like for C <strong>exercise</strong> ![[f915649b7e14afa4dc5ab759f53800c.png]]</p><h2 id="more-kernels">More Kernels</h2><p>A large number of kernels have been proposed,not limited to numerical/vector data! ● Vector kernels ● Set kernels ● String kernels ● Empirical kernel map ● Kernel kernels ● Kernel combination ● Kernels on graphs ● Kernels in graphs ● Kernels on probabilistic models</p><h1 id="recap">Recap</h1></li><li>Separating hyperplane: any plane that separates classes</li><li>The support vector machine has evolved from fundamental work by Vapnik: separable, linear problems: maximum margin classifier non-separable, linear problems - add slack variables:support vector classifier non-separable, non-linear problems - use kernel trick:support vector machine</li><li>Training involves quadratic programming (optimization)</li><li><p>The final classifier only depends on the support vectors</p></li><li>SVMs are widely used and work well in many cases,but care needs to be taken in selecting C, the kernel type and its parameters (using cross-validation)</li><li>The kernel trick: replace inner products by more general kernel functions can be applied in many other algorithms</li><li>Many kernels have been proposed for non-vector data, i.e. sets, strings, graphs etc.: very useful in bioinformatics, vision, document analysis etc.</li><li>SVMs are linked to logistic regression (section 9.5, not discussed here)<h1 id="support-vector">Support Vector</h1><h1 id="support-classifiers">Support Classifiers</h1></li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/13/MachineLearning-8.-Support-Vector-Machines-and-Kernels/" target="_blank">https://buliangzhang24.github.io/2024/02/13/MachineLearning-8.-Support-Vector-Machines-and-Kernels/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727730795', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
