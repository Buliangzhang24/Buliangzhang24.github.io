<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜ 4.Bayesian Classification KNN LDA and QDA &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/07/MachineLearning-4.Bayesian-Classification-KNN,LDA-and-QDA/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜ 4.Bayesian Classification KNN LDA and QDA"><meta name="keywords" content="KNN"><meta name="og:keywords" content="KNN"><meta name="description" content="KNN,LDA and QDA are Bayesian classificationClassificationFinding class k that maximizing the conditional probability举例来说，如果你的任务是将一张图片分类为狗、猫、或鸟，那么你可能会得到每个类别的概率，比如： 狗：0.8 猫：0.1 鸟：0.05在这种情况下，P最大化的类别是狗，因为它有最高的概率。 K-nearest neighbor classifier non-parametric what probability the point belongs to the class j,as the fraction of points in ==N_0== whose reponse values equal ==j== N_0 contains the ==K== points that are closest to x_0 Example K= 3 表示有三个最近的点在这个sample 点的附近 the region of the cricle :the last nearest neighbor Size of the green circle varies, depending on the location of the black cross x1 10000 but x2 78. the scale , the dominated by x1 Decision boundary Corresponding KNN ==decision boundary== P(Y = blue | X) =P(Y = orange | X) = 0.5 if there are three classes，=0.33 迭代error, test overfiting on the nosiy We have the Bayes Decision boundary automatically How to choose K K= 1: decision boundary overly flexible high variance ,overfitting K=100: decision boundary close to linear high bias underfitting K= 10 just right in this case only ==validation set== to choose which K is the best Training and test error 1/K related to flexibility of kNNK smaller, 1/K bigger, more flexible "><meta name="og:description" content="KNN,LDA and QDA are Bayesian classificationClassificationFinding class k that maximizing the conditional probability举例来说，如果你的任务是将一张图片分类为狗、猫、或鸟，那么你可能会得到每个类别的概率，比如： 狗：0.8 猫：0.1 鸟：0.05在这种情况下，P最大化的类别是狗，因为它有最高的概率。 K-nearest neighbor classifier non-parametric what probability the point belongs to the class j,as the fraction of points in ==N_0== whose reponse values equal ==j== N_0 contains the ==K== points that are closest to x_0 Example K= 3 表示有三个最近的点在这个sample 点的附近 the region of the cricle :the last nearest neighbor Size of the green circle varies, depending on the location of the black cross x1 10000 but x2 78. the scale , the dominated by x1 Decision boundary Corresponding KNN ==decision boundary== P(Y = blue | X) =P(Y = orange | X) = 0.5 if there are three classes，=0.33 迭代error, test overfiting on the nosiy We have the Bayes Decision boundary automatically How to choose K K= 1: decision boundary overly flexible high variance ,overfitting K=100: decision boundary close to linear high bias underfitting K= 10 just right in this case only ==validation set== to choose which K is the best Training and test error 1/K related to flexibility of kNNK smaller, 1/K bigger, more flexible "><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/07/MachineLearning-4.Bayesian-Classification-KNN,LDA-and-QDA/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-07"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜ 4.Bayesian Classification KNN LDA and QDA</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/07 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 5774 字，约 17 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"> <img style="height:72px;width:72px" src="https://buliangzhang24.github.io/assets/images/qrcode.jpg" alt="Buliangzhang" /></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><p>KNN,LDA and QDA are Bayesian classification</p><h2 id="classification">Classification</h2><p>Finding class k that maximizing the conditional probability 举例来说，如果你的任务是将一张图片分类为狗、猫、或鸟，那么你可能会得到每个类别的概率，比如：</p><ul><li>狗：0.8</li><li>猫：0.1</li><li>鸟：0.05 在这种情况下，P最大化的类别是狗，因为它有最高的概率。<h1 id="k-nearest-neighbor-classifier">K-nearest neighbor classifier</h1><p><img src="https://buliangzhang24.github.io/images/posts/3e88de363372e84e89e86b1b6e3e538.png" alt="" /></p></li><li>non-parametric</li><li>what probability the point belongs to the class j,as the fraction of points in ==N_0== whose reponse values equal ==j==</li><li>N_0 contains the ==K== points that are closest to x_0<h3 id="example">Example</h3><p><img src="https://buliangzhang24.github.io/images/posts/8901136cb31a6453aa079dee0193561.png" alt="" /></p></li><li>K= 3 表示有三个最近的点在这个sample 点的附近</li><li>the region of the cricle :the last nearest neighbor</li><li>Size of the green circle varies, depending on the location of the black cross</li><li>x1 10000 but x2 78. the scale , the dominated by x1<h3 id="decision-boundary">Decision boundary</h3><p><img src="https://buliangzhang24.github.io/images/posts/f4558634bdc194c0c0d463969e5fa1d.png" alt="" /></p></li><li>Corresponding KNN ==decision boundary== P(Y = blue | X) =P(Y = orange | X) = 0.5 if there are three classes，=0.33</li><li>迭代error, test overfiting on the nosiy</li><li>We have the Bayes Decision boundary automatically<h4 id="how-to-choose-k">How to choose K</h4></li><li>K= 1: decision boundary overly flexible high variance ,overfitting</li><li>K=100: decision boundary close to linear high bias underfitting</li><li>K= 10 just right in this case only ==validation set== to choose which K is the best<h4 id="training-and-test-error">Training and test error</h4><p><img src="https://buliangzhang24.github.io/images/posts/8c4be1a107e87038c7033c72ebe0d80.png" alt="" /> 1/K related to flexibility of kNN K smaller, 1/K bigger, more flexible</p></li></ul><p><strong>exercise2.7 Compute the Euclidean distance between each observation and the test point X1 = X2 = X3 =0</strong> <img src="https://buliangzhang24.github.io/images/posts/Pasted image 20240215104122.png" alt="" /> d(x_i,x_j)=√(X_i1−X_j1)^2+(X_i2−X_j2)^2+(X_i3−X_j3)^2 if K= 1 , bigger than 1 is red if K= 3, bigger than 3 is red <strong>exercise 2.8</strong> <img src="https://buliangzhang24.github.io/images/posts/Pasted image 20240215104552.png" alt="" /> (在这个问题中，我们有一个数据集，将其分成相等大小的训练集和测试集，然后尝试两种不同的分类方法。首先，我们使用逻辑回归，得到训练数据的错误率为20%，测试数据的错误率为30%。接下来，我们使用1个最近邻方法，得到的平均错误率（在训练和测试数据集上进行平均）为18%。基于这些结果，我们应该更倾向于使用哪种方法来对新的观测进行分类？)</p><ul><li>鉴于1-最近邻方法在训练数据上没有错误，那么测试数据的错误率必须是36%（1-最近邻测试错误率=18%*2=36%）。在这种情况下，逻辑回归方法更可取，因为它的测试错误率更低，为30%。感谢您发现了这个错误。</li><li><img src="https://buliangzhang24.github.io/images/posts/64102d655b447fb1f664ea0742b1f6f.png" alt="" /><h1 id="bayesian-classification">Bayesian Classification</h1><h2 id="classification-1">Classification</h2><p>posterior probability</p><h3 id="class--conditional-probability">Class- conditional probability</h3><p>So, Pr(Y=k|X=x) is difficult to model The reverse, Pr(X=x|Y=k), can be estimated easily 第一句话是指对于给定的数据点x，预测它属于类别k的概率是多少，这是一个困难的问题。第二句话则是指给定一个类别k，估计数据点x属于该类别的概率是多少，这通常是一个相对容易的问题。 <strong>本来是这个颜色是不是属于苹果，变成了给定一个数据点属于苹果类别，预测它的颜色是什么</strong></p><h3 id="bayes-theorem-in-classification">Bayes’ theorem in classification</h3><p><img src="https://buliangzhang24.github.io/images/posts/26053c986aaef35d67bccec6fb29d5a.png" alt="" /> <img src="https://buliangzhang24.github.io/images/posts/dbfeebe5590b7a96acabe5847258a2d.png" alt="" /></p><h4 id="bayesian-plug-in-classifier">Bayesian plug-in classifier</h4><p><img src="https://buliangzhang24.github.io/images/posts/839fbd7e28217691b6ddb0d57c6c326.png" alt="" /></p></li><li>实线/ dash line的和是1</li><li>the interaction of dash line is 0.5</li><li>if &gt; dash line , it is class 1 assign class label with highest posterior<h4 id="influence-of-prior">influence of prior</h4><p><img src="https://buliangzhang24.github.io/images/posts/077332f8a3a195c4246fe2992b08f70.png" alt="" /></p><h4 id="bayeserror">Bayes’error</h4><p>Bayes’ error: theoretically minimum attainable error <img src="https://buliangzhang24.github.io/images/posts/67311154b7f5446707998dc0e7fee4e.png" alt="" /></p><h4 id="class-conditional-distributions">Class-conditional distributions</h4><p>simple solution: historgrams assume normal(Gaussian) distributions</p><h1 id="linear-discriminant-analysis">Linear Discriminant Analysis</h1><h2 id="normal-distributions">Normal distributions</h2><p><img src="https://buliangzhang24.github.io/images/posts/2d07ad72d164b1574d70e6407423197.png" alt="" /></p><h2 id="linear-discriminant-analysislda">Linear discriminant analysis(LDA)</h2><p><img src="https://buliangzhang24.github.io/images/posts/3afae7768aace32d3ff37bd6e4d748c.png" alt="" /> singma is same shape is same k classes maximize the function too <img src="https://buliangzhang24.github.io/images/posts/0442a9a035f3604ca35cd1854ef5b7c.png" alt="" /> 这个下面这一坨都是indepent of k，因为它对每一个k都是一样的 <strong>exercise 7</strong> <img src="https://buliangzhang24.github.io/images/posts/a4480ec426829397298f17dd0a22835.png" alt="" /></p><h2 id="nearest-mean-classifier">Nearest mean classifier</h2></li><li>Assume ==priors are equal==, pi1 = pi2 = pi, then decision boundary is at 就是class 1 variance= class 2 variance</li><li>use the Discriminant function: calculate the value of x at the decision boundry : x=(u_1 + u_2)/2 ,so the decison boundary between the two means</li><li>Assign x to class with nearest mean<h3 id="linear-discriminant-analysis-1">Linear discriminant analysis</h3></li><li>在实践中，我们使用样本均值和样本标准差来估计总体均值和总体标准差，然后使用这些估计值来计算每个类别的观测概率。然后，根据这个概率来做出预测。</li><li>==就是 u&gt; variance &gt; pi== ![[4fd5f1d7f7f7bc8157a83cc54a84018.png]]n1 the number of the samples</li><li>Assumes dataset is an independent identically distributed (iid) sample of underlying distribution(这意味着数据集中的每个样本都是来自相同的分布，并且每个样本都是相互独立的，没有重叠或重复的样本。)<h4 id="parameter-estimation">parameter estimation</h4><p>In general: the more samples, the better the estimates (dashed lines,就是mean， variance 的估计的形状更好) and the better the classifier</p><h3 id="the-multivariate-case">the Multivariate Case</h3><p>If p &gt; 1: same classifier, using multivariate Gaussian</p><h4 id="multivariate-lda">Multivariate LDA</h4><p><img src="https://buliangzhang24.github.io/images/posts/1a51c6fc1348ad22852dec663fdbb60.png" alt="" /></p><h1 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h1></li><li>==LDA== assumes each class has the ==same covariance matrix Σ; ==estimated by weighted averaging of per-class covariances Σk</li><li>==QDA== is the same classifier, with a Gaussian distribution with a ==separate covariance matrix Σk for each individual class==</li><li>Assign x to class k for which the ==(quadratic) discriminant function== is largest. ![[Pasted image 20240218153413.png]]</li><li>for a p-dimensional data set: <br /> mean_k: is a vector with ==p elements== covariance_k : is a matrix with ==0.5 p(p+1)== elements</li><li>need more samples</li><li>computational complexity:LDA:==K×p==, QDA:==K×p(p+1)/2 == <img src="https://buliangzhang24.github.io/images/posts/ed227359b380886b891aefb687c0d7f.png" alt="" /><h1 id="curse-of-dimensionality">Curse of Dimensionality</h1><p><strong>exercise 5</strong> <img src="https://buliangzhang24.github.io/images/posts/Pasted image 20240218160053.png" alt="" /> <img src="https://buliangzhang24.github.io/images/posts/Pasted image 20240218160058.png" alt="" /> a) QDA on the training set, LDA on the test set:QDA is likely to overfit here, LDA not. <strong>QDA在训练集上表现更好</strong>：QDA是一种更灵活的模型，允许每个类别有自己的协方差矩阵。当贝叶斯决策边界是线性的时候，QDA可以更好地拟合训练数据，并且可以有更高的训练精度。==但是在test set 上就QDA可能会过拟合，因为它有更多的参数需要估计。== <strong>LDA在测试集上表现更好</strong>：相反，LDA假设所有类别的协方差矩阵相同，更倾向于学习一个简单的线性决策边界。这使得LDA更容易泛化到新的数据上，从而在测试集上表现更好。 b) That depends on the exact form of the decision boundary,but in general LDA cannot find nonlinear decision boundaries, so given sufficient training data we expect QDA to do better on both. c) Improve, as there are ==more samples to estimate QDA’s additional parameters.== d) False: the additional parameters (for the flexibility) need to be estimated; as a result, QDA can overfit for limited sample size.</p><h2 id="concept">Concept</h2><p>When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. 当特征数 p 很大时，KNN 和其他利用邻近观测进行预测的本地方法的性能往往会下降。这种现象被称为维度诅咒，它与非参数方法在 p 很大时往往表现不佳有关。 ![[Pasted image 20240218161437.png]] if very few observations are “near” in feature space(the chances are very small), you will use “far away” neighbours instead ANSWER d) If you want 5 samples in the interval: p=1 =&gt; 50 samples p=2 =&gt; 500 samples p=3 =&gt; 5000 samples … We need exponentially more samples with increasing p</p><h1 id="performance-of-the-classifier">Performance of the classifier</h1><h2 id="sensitivity-vs-specificity">Sensitivity vs. specificity</h2><p>![[c03b60aefef546e28f4bc24095f7ac0.png]]</p><h3 id="threshold-of-the-posterior">Threshold of the posterior</h3><p>let us make the LDA classifier more sensitive, allowing the bank to detect more high-risk individuals (==less false negatives==),at the cost of rejecting the credit card applications of some additional low-risk individuals (==false positives==) <img src="https://buliangzhang24.github.io/images/posts/a1f65370ad40265f4feba4bfe3ccf51.png" alt="" /> <img src="https://buliangzhang24.github.io/images/posts/969fd06c50042a81513120ceccd11cf.png" alt="" /> 就是threshold的值减小，模型更敏感，FN下降，FP上升</p><h4 id="roc">ROC</h4><p><img src="https://buliangzhang24.github.io/images/posts/fe75ce815df397a4159f5b165794322.png" alt="" /></p></li><li>ROC curve：area is bigger , better</li><li>Black curve: overall error rate</li><li>Blue dashed line: false negative rate (1-sensitivity)</li><li>Brown dotted line: false positive rate (1-specificity)<h3 id="area-under-the-curve-auc">Area under the curve (AUC)</h3><p>If we do not know the optimal operating point (yet), we can summarize the ROC with the area below it: AUC</p></li><li>optimal: 1</li><li>random classifier: 0.5 <img src="https://buliangzhang24.github.io/images/posts/5e52b6a919ca434c2ea22a742cedc0a.png" alt="" /><h1 id="parametric-vs-non-parametric">Parametric vs. non-parametric</h1></li><li>==LDA and QDA are parametric methods==: assume a global model, estimate its parameters</li><li>Non-parametric methods estimate densities locally, which can be an advantage for ==non-linear ==problems and high-dimensional data</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/07/MachineLearning-4.Bayesian-Classification-KNN,LDA-and-QDA/" target="_blank">https://buliangzhang24.github.io/2024/02/07/MachineLearning-4.Bayesian-Classification-KNN,LDA-and-QDA/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1729630932', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
