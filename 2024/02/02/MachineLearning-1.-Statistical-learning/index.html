<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜1. Statistical learning &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/02/MachineLearning-1.-Statistical-learning/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜1. Statistical learning"><meta name="keywords" content="Statistical learning"><meta name="og:keywords" content="Statistical learning"><meta name="description" content="Y= f(X)+e Y: (response, dependent variable, predicted value) X: (predictor,independent variable,feature)f: unknow relationshipe: random error(1. with 平均值等于0 mean zero 2. 模拟了个体之间的差异）the multivariate case：Usually more than 2 input dimensions!p: number of input dimensions 要分析几个变量n: number of data points(samples in the data) in a sample 样本量Prediction是啥？就是找到Y就行了，f这个关系可以是一个黑盒Y and f are ==unknown== , so we estimate f in order to predict Y from konwn X values有帽子的f 和Y是estimated / predicted f is ==estimated== using ==training data==, consisting of X values and corresponding Y values. Then, the Y values can be ==predicted== for new X values如果说e 的平均值是0, 那么 Y^ = f(X)^Error of the model Y- Y with hat Can be estimated from the data set: mean squared error: 1/N sum(yi-yi with hat)^2 Reducible and irreducible error reducible : change the ==learning techniques and models ==and ==better training data== » minimized while estimating f (inference)irreducible: cannot be reduced because of ==unmeasured but relevant inputs ==or ==unmeasured variation(nosie)==» set upper bound on the accuracy of predicting Y(prediction) Inference 就是虽然俺们也预测了Y,但是重要的是找到f 关系的方程estimate f , but ==understanding how== X influence YDo not treat f with hat as black box Summary Prediction: estimating f^ to get good prediction of Y^Inference: estimating f ^ to get an understanding of the relationship between X1-Xp and Y Prediction accuracy vs. model interpretablity 预测准确度：预测准确度是指模型在测试集上的预测结果与实际观测值之间的一致程度模型解释度：线性回归模型通常具有很高的解释性，因为它可以明确地表示变量之间的线性关系，并且可以解释每个自变量对因变量的影响程度==linear models==: high interpretability and sometimes high accuracy»inference==highly non-linear==: low interpretability, high accuracy»predictionChoice depends：prediction or inference Parametric methods choose the functional form of f, then learn its parameters from training data,using least squares or a differenct method就是先假设确定了一个方法去作为f，然后求参数ad: much easier to estimate a set of parameters than to fit an arbitrary function ==less training data==dis: if the chosen functional form is too far from the truth, prediction and inference results can be poor &gt;we don’t know the relationship in advance Non parametric based on training data itself非参数方法不需要事先确定模型的函数形式或者参数的数量，而是从数据中直接学习模型的结构。这使得非参数方法在处理复杂的数据结构或者对数据分布了解不充分时更具有灵活性。ad: ==good fit== ,even if the input-output relations are complexdis: require much ==more training data== risk of ==overfitting== modelling 了 the nosie e (don’ t have enough data)Overfitting是指模型在训练数据上表现很好，但在测试数据（或新数据）上表现较差的现象，因为噪声等»指模型过于复杂，过度拟合了训练数据中的噪声和细微特征的情况。Overfitting 的原因有： 模型过于复杂，训练数据量不足，特征选择不当，训练数据和测试数据的分布不一致 Supervised vs Unsupervised Learning 最大区别在有没有标签 Assessing Model Accuracy Measuring the quality of fit 回归和分类的区别回归：预测连续的变量分类：预测离散的变量 Mean squared error(regression) training MSE isnot important: 增加模型的灵活性可以使其更容易适应复杂的数据模式和关系，从而可能降低训练 MSE。more flexiblity less taining MSE。 test MSE(unseen) different parts of field ==goal: select the model with the smallest test MSE==Underfitting:模型过于简单，无法捕捉数据中的真实模式和关系的情况。欠拟合的模型通常对训练数据和测试数据的表现都较差Overfitting:模型过于复杂，过度拟合了训练数据中的噪声和细微特征的情况。过拟合的模型在训练数据上表现很好，但在未见过的测试数据上表现较差 Bias vs. variance 因为Biase和variance，所以MSE才是U形的 Bias : model too simple Bias refers to the error that is introduced by approximating a real-life problem by a too simpler model真实值和期望值之间的差异 无论我们使用多少训练数据，这个误差都会存在 more flexible methods have less bias Variance: model too complex 如果换成不同的training sample， f的预测有多大的变化 如果model 有High variance，就是很小的变化都会在f的预测结果上产生很大的变化方差越大意味着模型对数据的变化更敏感。 more flexible methods have higher variance==more flexible, less bias, more variance==Good test set performance requires ==low variance as well as low squared bias.==(a) inflexible biase »fiexible Flexible is generally better. A flexible method has many degrees of freedom, so it can follow the patterns in the data, even if they are highly non-linear. If the data is more linear, there are enough data points to train the parameters so that the model turns out more linear as well. If flexibility is chosen extremely large, overfitting could still occur.大样本量：对于极大的样本量，灵活的模型往往表现更好，因为它们有更多的数据可供学习predictor 量少：当预测变量的数量较少时，意味着数据可能具有简单的关系，这种关系可以由灵活的模型充分捕获而无需担心过拟合(b) inflexible There is a high risk of overfitting.(c) flexible(d) inflexible (flexible model» there lots of nosiy  » fiting the nosiy)就是用对了，就是low,low。 For Classification Setting Instead of MSE, we get ==error rate==:I= 1 if the pefect modelAgain, there is a ==training error rate and a test error rate==. They express the fraction of incorrect classifications不正确分类的比例 Training set and test set 就是用training set 去训练模型，然后在调整参数的时候用validation set 找哪个参数合适，同时也找哪个模型是更适合这个数据的，再去test set里面验证这个模型好不好 Training set to ==train the model== Validation set to ==optimize the hyper parameters== Test set to test the performance of the model on an ==independent ==part of the data set. To get an estimate on how good it will ==work in practice== with limited amount of data 少量数据 可以用 cross validation "><meta name="og:description" content="Y= f(X)+e Y: (response, dependent variable, predicted value) X: (predictor,independent variable,feature)f: unknow relationshipe: random error(1. with 平均值等于0 mean zero 2. 模拟了个体之间的差异）the multivariate case：Usually more than 2 input dimensions!p: number of input dimensions 要分析几个变量n: number of data points(samples in the data) in a sample 样本量Prediction是啥？就是找到Y就行了，f这个关系可以是一个黑盒Y and f are ==unknown== , so we estimate f in order to predict Y from konwn X values有帽子的f 和Y是estimated / predicted f is ==estimated== using ==training data==, consisting of X values and corresponding Y values. Then, the Y values can be ==predicted== for new X values如果说e 的平均值是0, 那么 Y^ = f(X)^Error of the model Y- Y with hat Can be estimated from the data set: mean squared error: 1/N sum(yi-yi with hat)^2 Reducible and irreducible error reducible : change the ==learning techniques and models ==and ==better training data== » minimized while estimating f (inference)irreducible: cannot be reduced because of ==unmeasured but relevant inputs ==or ==unmeasured variation(nosie)==» set upper bound on the accuracy of predicting Y(prediction) Inference 就是虽然俺们也预测了Y,但是重要的是找到f 关系的方程estimate f , but ==understanding how== X influence YDo not treat f with hat as black box Summary Prediction: estimating f^ to get good prediction of Y^Inference: estimating f ^ to get an understanding of the relationship between X1-Xp and Y Prediction accuracy vs. model interpretablity 预测准确度：预测准确度是指模型在测试集上的预测结果与实际观测值之间的一致程度模型解释度：线性回归模型通常具有很高的解释性，因为它可以明确地表示变量之间的线性关系，并且可以解释每个自变量对因变量的影响程度==linear models==: high interpretability and sometimes high accuracy»inference==highly non-linear==: low interpretability, high accuracy»predictionChoice depends：prediction or inference Parametric methods choose the functional form of f, then learn its parameters from training data,using least squares or a differenct method就是先假设确定了一个方法去作为f，然后求参数ad: much easier to estimate a set of parameters than to fit an arbitrary function ==less training data==dis: if the chosen functional form is too far from the truth, prediction and inference results can be poor &gt;we don’t know the relationship in advance Non parametric based on training data itself非参数方法不需要事先确定模型的函数形式或者参数的数量，而是从数据中直接学习模型的结构。这使得非参数方法在处理复杂的数据结构或者对数据分布了解不充分时更具有灵活性。ad: ==good fit== ,even if the input-output relations are complexdis: require much ==more training data== risk of ==overfitting== modelling 了 the nosie e (don’ t have enough data)Overfitting是指模型在训练数据上表现很好，但在测试数据（或新数据）上表现较差的现象，因为噪声等»指模型过于复杂，过度拟合了训练数据中的噪声和细微特征的情况。Overfitting 的原因有： 模型过于复杂，训练数据量不足，特征选择不当，训练数据和测试数据的分布不一致 Supervised vs Unsupervised Learning 最大区别在有没有标签 Assessing Model Accuracy Measuring the quality of fit 回归和分类的区别回归：预测连续的变量分类：预测离散的变量 Mean squared error(regression) training MSE isnot important: 增加模型的灵活性可以使其更容易适应复杂的数据模式和关系，从而可能降低训练 MSE。more flexiblity less taining MSE。 test MSE(unseen) different parts of field ==goal: select the model with the smallest test MSE==Underfitting:模型过于简单，无法捕捉数据中的真实模式和关系的情况。欠拟合的模型通常对训练数据和测试数据的表现都较差Overfitting:模型过于复杂，过度拟合了训练数据中的噪声和细微特征的情况。过拟合的模型在训练数据上表现很好，但在未见过的测试数据上表现较差 Bias vs. variance 因为Biase和variance，所以MSE才是U形的 Bias : model too simple Bias refers to the error that is introduced by approximating a real-life problem by a too simpler model真实值和期望值之间的差异 无论我们使用多少训练数据，这个误差都会存在 more flexible methods have less bias Variance: model too complex 如果换成不同的training sample， f的预测有多大的变化 如果model 有High variance，就是很小的变化都会在f的预测结果上产生很大的变化方差越大意味着模型对数据的变化更敏感。 more flexible methods have higher variance==more flexible, less bias, more variance==Good test set performance requires ==low variance as well as low squared bias.==(a) inflexible biase »fiexible Flexible is generally better. A flexible method has many degrees of freedom, so it can follow the patterns in the data, even if they are highly non-linear. If the data is more linear, there are enough data points to train the parameters so that the model turns out more linear as well. If flexibility is chosen extremely large, overfitting could still occur.大样本量：对于极大的样本量，灵活的模型往往表现更好，因为它们有更多的数据可供学习predictor 量少：当预测变量的数量较少时，意味着数据可能具有简单的关系，这种关系可以由灵活的模型充分捕获而无需担心过拟合(b) inflexible There is a high risk of overfitting.(c) flexible(d) inflexible (flexible model» there lots of nosiy  » fiting the nosiy)就是用对了，就是low,low。 For Classification Setting Instead of MSE, we get ==error rate==:I= 1 if the pefect modelAgain, there is a ==training error rate and a test error rate==. They express the fraction of incorrect classifications不正确分类的比例 Training set and test set 就是用training set 去训练模型，然后在调整参数的时候用validation set 找哪个参数合适，同时也找哪个模型是更适合这个数据的，再去test set里面验证这个模型好不好 Training set to ==train the model== Validation set to ==optimize the hyper parameters== Test set to test the performance of the model on an ==independent ==part of the data set. To get an estimate on how good it will ==work in practice== with limited amount of data 少量数据 可以用 cross validation "><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/02/MachineLearning-1.-Statistical-learning/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-02"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜1. Statistical learning</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/02 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 4288 字，约 13 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"> <img style="height:72px;width:72px" src="https://buliangzhang24.github.io/assets/images/qrcode.jpg" alt="Buliangzhang" /></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><p>Y= f(X)+e <mark class="hltr-yellow"> Y: (response, dependent variable, predicted value) </mark> <mark class="hltr-yellow">X: (predictor,independent variable,feature)</mark> f: unknow relationship e: random error(1. with 平均值等于0 mean zero 2. 模拟了个体之间的差异） the multivariate case：Usually more than 2 input dimensions! p: number of input dimensions 要分析几个变量 n: number of data points(samples in the data) in a sample 样本量</p><h2 id="prediction">Prediction</h2><p>是啥？就是找到Y就行了，f这个关系可以是一个黑盒 Y and f are ==unknown== , so we estimate f in order to predict Y from konwn X values 有帽子的f 和Y是estimated / predicted f is ==estimated== using ==training data==, consisting of X values and corresponding Y values. Then, the Y values can be ==predicted== for new X values 如果说e 的平均值是0, 那么 Y^ = f(X)^</p><h3 id="error-of-the-model">Error of the model</h3><ul><li><table><tbody><tr><td>Y- Y with hat</td></tr></tbody></table></li><li>Can be estimated from the data set: mean squared error: 1/N sum(yi-yi with hat)^2<h4 id="reducible-and-irreducible-error">Reducible and irreducible error</h4><p>reducible : change the ==learning techniques and models ==and ==better training data== » minimized while estimating f (inference) irreducible: cannot be reduced because of ==unmeasured but relevant inputs ==or ==unmeasured variation(nosie)==» set upper bound on the accuracy of predicting Y(prediction)</p><h2 id="inference">Inference</h2><p>就是虽然俺们也预测了Y,但是重要的是找到f 关系的方程 estimate f , but ==understanding how== X influence Y Do not treat f with hat as black box</p><h2 id="summary">Summary</h2><p>Prediction: estimating f^ to get good prediction of Y^ Inference: estimating f ^ to get an understanding of the relationship between X1-Xp and Y</p><h2 id="prediction-accuracy-vs-model-interpretablity">Prediction accuracy vs. model interpretablity</h2><p>预测准确度：预测准确度是指模型在测试集上的预测结果与实际观测值之间的一致程度 模型解释度：线性回归模型通常具有很高的解释性，因为它可以明确地表示变量之间的线性关系，并且可以解释每个自变量对因变量的影响程度 ==linear models==: high interpretability and sometimes high accuracy»inference ==highly non-linear==: low interpretability, high accuracy»prediction Choice depends：prediction or inference</p><h2 id="parametric-methods">Parametric methods</h2><p>choose the functional form of f, then learn its parameters from training data,using least squares or a differenct method 就是先假设确定了一个方法去作为f，然后求参数 ad:</p><ol><li>much easier to estimate a set of parameters than to fit an arbitrary function</li><li>==less training data== dis: if the chosen functional form is too far from the truth, prediction and inference results can be poor &gt;we don’t know the relationship in advance<h2 id="non-parametric">Non parametric</h2><p>based on training data itself非参数方法不需要事先确定模型的函数形式或者参数的数量，而是从数据中直接学习模型的结构。这使得非参数方法在处理复杂的数据结构或者对数据分布了解不充分时更具有灵活性。 ad: ==good fit== ,even if the input-output relations are complex dis:</p></li><li>require much ==more training data==</li><li>risk of ==overfitting== modelling 了 the nosie e (don’ t have enough data) Overfitting是指模型在训练数据上表现很好，但在测试数据（或新数据）上表现较差的现象，因为噪声等»指模型过于复杂，过度拟合了训练数据中的噪声和细微特征的情况。 Overfitting 的原因有： 模型过于复杂，训练数据量不足，特征选择不当，训练数据和测试数据的分布不一致 <img src="https://buliangzhang24.github.io/images/posts/737a24037115d610d6b55c525374370.png" alt="" /><h2 id="supervised-vs-unsupervised-learning">Supervised vs Unsupervised Learning</h2><p>最大区别在有没有标签</p><h1 id="assessing-model-accuracy">Assessing Model Accuracy</h1><h2 id="measuring-the-quality-of-fit">Measuring the quality of fit</h2><p>回归和分类的区别 回归：预测连续的变量 分类：预测离散的变量</p><h3 id="mean-squared-errorregression">Mean squared error(regression)</h3></li></ol></li><li>training MSE isnot important: 增加模型的灵活性可以使其更容易适应复杂的数据模式和关系，从而可能降低训练 MSE。more flexiblity less taining MSE。</li><li>test MSE(unseen) different parts of field</li><li>==goal: select the model with the smallest<mark class="hltr-green"> test MSE</mark>== <img src="https://buliangzhang24.github.io/images/posts/8edabe3d0987a7a37cb37de6b2494ca.png" alt="" /> Underfitting:模型过于简单，无法捕捉数据中的真实模式和关系的情况。欠拟合的模型通常对训练数据和测试数据的表现都较差 Overfitting:模型过于复杂，过度拟合了训练数据中的噪声和细微特征的情况。过拟合的模型在训练数据上表现很好，但在未见过的测试数据上表现较差<h2 id="bias-vs-variance">Bias vs. variance</h2><p><img src="https://buliangzhang24.github.io/images/posts/860be21fdebfc9318057e0881fc7d5d.png" alt="" /> 因为Biase和variance，所以MSE才是U形的</p><h3 id="bias--model-too-simple">Bias : model too simple</h3></li><li>Bias refers to the error that is introduced by approximating a real-life problem by a too simpler model真实值和期望值之间的差异</li><li>无论我们使用多少训练数据，这个误差都会存在</li><li>more flexible methods have less bias<h3 id="variance-model-too-complex">Variance: model too complex</h3></li><li>如果换成不同的training sample， f的预测有多大的变化</li><li>如果model 有High variance，就是很小的变化都会在f的预测结果上产生很大的变化方差越大意味着模型对数据的变化更敏感。</li><li>more flexible methods have higher variance ==more flexible, less bias, more variance== Good test set performance requires ==low variance as well as low squared bias.== <img src="https://buliangzhang24.github.io/images/posts/04ce4e8bb2e16bc6e413ec0d16581f9.png" alt="" /> (a) inflexible biase »<mark class="hltr-red">fiexible </mark> Flexible is generally better. A flexible method has many degrees of freedom, so it can follow the patterns in the data, even if they are highly non-linear. If the data is more linear, there are enough data points to train the parameters so that the model turns out more linear as well. If flexibility is chosen extremely large, overfitting could still occur. 大样本量：对于极大的样本量，灵活的模型往往表现更好，因为它们有更多的数据可供学习 predictor 量少：当预测变量的数量较少时，意味着数据可能具有简单的关系，这种关系可以由灵活的模型充分捕获而无需担心过拟合 (b) inflexible There is a high risk of overfitting. (c) flexible (d) inflexible (flexible model» there lots of nosiy  » fiting the nosiy) 就是用对了，就是low,low。 <img src="https://buliangzhang24.github.io/images/posts/b67f650f37b2f3d0d6f112246fcc8f5.png" alt="" /><h3 id="for-classification-setting">For Classification Setting</h3><p>Instead of MSE, we get ==error rate==:I= 1 if the pefect model Again, there is a ==training error rate and a test error rate==. They express the fraction of incorrect classifications不正确分类的比例</p><h2 id="training-set-and-test-set">Training set and test set</h2><p>就是用training set 去训练模型，然后在调整参数的时候用validation set 找哪个参数合适，同时也找哪个模型是更适合这个数据的，再去test set里面验证这个模型好不好</p></li><li>Training set to ==train the model==</li><li>Validation set to ==optimize the hyper parameters==</li><li>Test set to test the performance of the model on an ==independent ==part of the data set. To get an estimate on how good it will ==work in practice==<h3 id="with-limited-amount-of-data-少量数据">with limited amount of data 少量数据</h3><p>可以用 cross validation <img src="https://buliangzhang24.github.io/images/posts/74721a6689f929962208e7a3b8ca670.png" alt="" /></p></li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/02/MachineLearning-1.-Statistical-learning/" target="_blank">https://buliangzhang24.github.io/2024/02/02/MachineLearning-1.-Statistical-learning/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1733845599', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
