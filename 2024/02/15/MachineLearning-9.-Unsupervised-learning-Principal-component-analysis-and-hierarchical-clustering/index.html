<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜9. Unsupervised learning-Principal component analysis and hierarchical clustering &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/15/MachineLearning-9.-Unsupervised-learning-Principal-component-analysis-and-hierarchical-clustering/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜9. Unsupervised learning-Principal component analysis and hierarchical clustering"><meta name="keywords" content="Unsupervised learning-Principal component analysis and hierarchical clustering"><meta name="og:keywords" content="Unsupervised learning-Principal component analysis and hierarchical clustering"><meta name="description" content="supervised learning 的缺点： little labelled data available generating labels is expensive Unsupervised learning Discover structure in only X1, X2, …, Xp Part of explorative data analysis, very useful in data science Main problem: no clear way to check the answer Two types of approaches: dimensionality reduction, clustering Principal component analysis(PCA) find a linear low-dimensional representation that captures as much variation in the data as possible( projection from p dimensions to d &lt; p) Dimensionality reduction How to investigate high-dimensional data? scatterplots, correlation or covariance, select most interesting 概念 ![[f4e712394e144947fe34b48dd920f71.png]] Projecting points on a line Zi (principal component, PC): maximum variation along the line Zi数据在这个方向上的分布变化幅度最大,也就是minimum distance to the line Zi通过最大化方差来选择主成分方向，等价于通过最小化距离来选择低维子空间。 Principal components are ordered: Z1 is the direction of most variation,Z2 is the direction of most variation perpendicular to Z1, Z3 is the direction of most variation perpendicular to Z1 and Z2 Find the first principal component ![[a58dd338acf0a8836c37d61d757bd9f.png]]![[da80bc059c23d325614c10cc5caef2e.png]]通过使载荷向量的长度最大化(1)来最大化方差 size does matter Scaling to unit standard deviation is useful if features have different ranges Scaling is essential if measured in different units, If units are the same, maybe we do not want to scale to unit variance as we may inflate noise这是因为单位方差假设所有特征的方差应该相等，但在这种情况下，这种假设可能不合适。 biplot PVE(proportion of variance explained) "><meta name="og:description" content="supervised learning 的缺点： little labelled data available generating labels is expensive Unsupervised learning Discover structure in only X1, X2, …, Xp Part of explorative data analysis, very useful in data science Main problem: no clear way to check the answer Two types of approaches: dimensionality reduction, clustering Principal component analysis(PCA) find a linear low-dimensional representation that captures as much variation in the data as possible( projection from p dimensions to d &lt; p) Dimensionality reduction How to investigate high-dimensional data? scatterplots, correlation or covariance, select most interesting 概念 ![[f4e712394e144947fe34b48dd920f71.png]] Projecting points on a line Zi (principal component, PC): maximum variation along the line Zi数据在这个方向上的分布变化幅度最大,也就是minimum distance to the line Zi通过最大化方差来选择主成分方向，等价于通过最小化距离来选择低维子空间。 Principal components are ordered: Z1 is the direction of most variation,Z2 is the direction of most variation perpendicular to Z1, Z3 is the direction of most variation perpendicular to Z1 and Z2 Find the first principal component ![[a58dd338acf0a8836c37d61d757bd9f.png]]![[da80bc059c23d325614c10cc5caef2e.png]]通过使载荷向量的长度最大化(1)来最大化方差 size does matter Scaling to unit standard deviation is useful if features have different ranges Scaling is essential if measured in different units, If units are the same, maybe we do not want to scale to unit variance as we may inflate noise这是因为单位方差假设所有特征的方差应该相等，但在这种情况下，这种假设可能不合适。 biplot PVE(proportion of variance explained) "><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/15/MachineLearning-9.-Unsupervised-learning-Principal-component-analysis-and-hierarchical-clustering/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-15"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜9. Unsupervised learning-Principal component analysis and hierarchical clustering</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/15 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 4667 字，约 14 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h2 id="supervised-learning-的缺点">supervised learning 的缺点：</h2><ul><li>little labelled data available</li><li>generating labels is expensive<h1 id="unsupervised-learning">Unsupervised learning</h1></li><li>Discover structure in <mark class="hltr-orange">only </mark>X1, X2, …, Xp</li><li>Part of explorative data analysis, very useful in data science</li><li>Main problem: no clear way to check the answer</li><li>Two types of approaches: dimensionality reduction, clustering<h1 id="principal-component-analysispca">Principal component analysis(PCA)</h1><p>find a linear low-dimensional representation that captures as much variation in the data as possible( projection from p dimensions to d &lt; p)</p><h2 id="dimensionality-reduction">Dimensionality reduction</h2></li><li>How to investigate high-dimensional data? scatterplots, correlation or covariance, select most interesting<h2 id="概念">概念</h2><p>![[f4e712394e144947fe34b48dd920f71.png]]</p></li><li>Projecting points on a line Zi (principal component, PC): maximum variation along the line Zi数据在这个方向上的分布变化幅度最大,也就是<mark class="hltr-orange">minimum distance to the line Zi</mark>通过最大化方差来选择主成分方向，等价于通过最小化距离来选择低维子空间。</li><li>Principal components are ordered: Z1 is the direction of most variation,Z2 is the direction of most variation perpendicular to Z1, Z3 is the direction of most variation perpendicular to Z1 and Z2<h2 id="find-the-first-principal-component">Find the first principal component</h2><p>![[a58dd338acf0a8836c37d61d757bd9f.png]] ![[da80bc059c23d325614c10cc5caef2e.png]] 通过使载荷向量的长度最大化(1)来最大化方差</p><h3 id="size-does-matter">size does matter</h3></li><li>Scaling to unit standard deviation is useful if features have different ranges</li><li>Scaling is essential if measured in different units,</li><li>If units are the same, maybe we do not want to scale to unit variance as we may inflate noise这是因为单位方差假设所有特征的方差应该相等，但在这种情况下，这种假设可能不合适。<h3 id="biplot">biplot</h3><h3 id="pveproportion-of-variance-explained">PVE(proportion of variance explained)</h3></li></ul><p>![[a231610ce8f6af47eafbf207cd98180.png]] 就是那个loading vector就是PC的系数值， 同时也是pca.component_(如果在df才拟合的话)，也就是z</p><h3 id="how-to-choose-number-of-pcs">How to choose number of PCs?</h3><ul><li>As few PCs as possible, retaining as much information as possible</li><li>no clear solution: scree plot(PVE90%-95%); regard as tuning parameter, optimize by cross-validation; use additional information; visualization(2D,3D)<h3 id="another-interpretion-of-pca">Another interpretion of PCA</h3></li><li>Projecting points back into original space</li><li>PCA minimizes reconstruction error<h2 id="limitation-and-solutions">Limitation and Solutions</h2></li><li>Linearity: kernel PCA (last week)</li><li>Orthogonality:independent component analysis (ICA), maximizes independence of components<h2 id="clustering-vs-classification">Clustering vs. classification</h2></li><li>Classification:build predictors for categories based on an example dataset with labels</li><li>Clustering:<mark class="hltr-orange">find ”natural” groups in data without using labels</mark></li><li>Classes and clusters do not necessarily coincide!</li><li>Classes can consist of multiple clusters, clusters can combine classes ![[18d6fe7596d3687d334c898005a0ee5.png]]</li><li>Define what is “<mark class="hltr-orange">far apart</mark>” and “<mark class="hltr-orange">close together</mark>”: need a distance (or dissimilarity) measure to capture what we think is important for the grouping the choice for a certain distance measure is often the most important choice in clustering!</li><li>Clustering is an ill-defined problem –there is no such thing as the objective clustering<h1 id="hierarchical-clustering">Hierarchical clustering</h1><p>![[223220e0932c49db11e05bb4cce346f.png]]</p><h2 id="hierarchical-clustering-1">Hierarchical clustering</h2></li><li>create dendrogram (tree) assuming clusters are nested</li><li>bottom-up or agglomerative (as opposed to top-down, e.g. as in decision tree or K-means)</li><li>Cutting the dendrogram(树状图) decides on number of clusters（簇） ![[4c86068c8ea033c8d2e2f7084e69b73.png]]<h3 id="algorithm">Algorithm</h3></li><li>start: all objects of X in a separate cluster</li><li>clustering: combine (fuse) the 2 clusters with the shortest distance in dissimilarity matrix D</li><li>distance between clusters is based on <mark class="hltr-orange">link type:</mark></li><li>single, complete, average, centroid, …</li><li>repeat until only 1 cluster is left<h4 id="steps">Steps</h4></li><li>Step 1: Find the most similar pair of clusters: here x2 and x</li><li>Step 2: Fuse x2 and x3 into a cluster [x2, x3]</li><li>Step 3: Recompute D –what is the dissimilarity between [x2, x3] and the rest?<ol><li>single link: the smallest dissimilarity (nearest neighbor)</li><li>complete link: the largest dissimilarity</li><li>average link: the average dissimilarity</li><li>centroid link: the dissimilarity to the cluster centroid ![[9adf1035b3bb7a85162f0636d433ad5.png]] ![[5ad171017b117bd03b6489f5cca65c5.png]]</li></ol></li><li>Finally, cut the dendrogram to obtain clusters(Number of clusters: cut largest “gap” in tree (cf. elbow)) ![[b51d009bfa6c584f4d2b8f6d0aae2a7.png]] exercise ![[6c90858de0ca4ad2b5e4d8ae46ee40c.png]]![[bffcfda2516a3730c5b112f45aef77e.png]]<h2 id="linkage">Linkage</h2><p>No single best option:depends on data and intended use</p></li><li><mark class="hltr-orange">Average/centroid and complete</mark> linkage generally give more balanced trees</li><li><mark class="hltr-orange">Single linkage can find non-globular clusters, but is sensitive to outliers</mark></li><li>Centroid linkage can give inversions<h3 id="scaling">Scaling</h3></li><li>Depending on dissimilarity measure used, feature scaling can influence results (e.g. Euclidean distance) or not (e.g. correlation)</li><li>When using Euclidean distance<ol><li><mark class="hltr-orange">centering to mean zero</mark> to focus on <mark class="hltr-orange">trends </mark>instead of absolute value</li><li>different measurement units may be a reason to scale to unit standard deviation<h1 id="recap">Recap</h1></li></ol></li><li>Unsupervised learning ● discover structure in only X, without a Y</li><li>Dimensionality reduction: ● find low-dimensional representation that captures as much information as possible ● linear vs. nonlinear</li><li>Principal component analysis: ● find orthogonal directions that maximize preserved variation or (equivalently) minimize reconstruction error ● scores, loadings, biplots</li><li>Clustering: ● finding natural groups in data ● requires clustering algorithm and dissimilarity measure ● essentially subjective, no “proof”</li><li>Hierarchical clustering: ● repeatedly fuse clusters, create dendrogram ● choice of linkage: single, complete, average, centroid ● cut dendrogram to find actual clusters</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/15/MachineLearning-9.-Unsupervised-learning-Principal-component-analysis-and-hierarchical-clustering/" target="_blank">https://buliangzhang24.github.io/2024/02/15/MachineLearning-9.-Unsupervised-learning-Principal-component-analysis-and-hierarchical-clustering/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727806439', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
