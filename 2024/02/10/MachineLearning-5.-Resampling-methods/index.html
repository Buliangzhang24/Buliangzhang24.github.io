<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜5. Resampling methods &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/10/MachineLearning-5.-Resampling-methods/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜5. Resampling methods"><meta name="keywords" content="Resampling methods"><meta name="og:keywords" content="Resampling methods"><meta name="description" content="MotivationBias and VariancePrediction errors are due to Bias: the inability of the model to capture essential interactions, processes, or dynamics of the true system. Bias is associated with ==non-flexibility==, or ==lacking information in the data set== Variance: sensitivity of coefficients towards errors or variations in the data. Variance is associated with ==flexibility.== Irreducible errors: measurement errors that cannot be predicted为什么 test 会U形，training 随着flexibility 下降 The challenge lies in finding a method for which both the variance and the squared bias are low. 两个评价方面 Model assessment (performance) Model selection (flexibility) Test error and training error test error:used in its training training error: not used in its training 其实下面这个问题就是那个图 The training error rate often is quite different from the test error rate, and in particular the training error can considerably ==underestimate== the test error What is the most likely cause, bias or variance? Variance, a sensitivity towards the configuration of the data (training error is small, test error is large) Best estimation of test error: ==a large== designated test set. (大量的test set) Sometimes not available mathematical ==adjustment== to the training error rate in order to estimate the test error rate For example: R2adjusted. Others: Cp statistic, AIC and BIC In this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations The Validation Set Approach Here we ==randomly== divide the available set of samples into two parts: a ==training set ==and a ==validation or hold-out set== Drawbacks test error is highly variable, and only half of the obsevations » hence, the validation set error tends to overestimate the test error 50% of the data: inability to incorporate all information from the data set (bias) Leave one out cross validation Steps spliting the set of ob into two parts single ob is used for the validation set remaining ob make up the training set prediction y^_1 ,using x_1 Since (x1, y1) was not used in the fitting process, MSE1 = (y1 − y ̂1)2 provides an approximately unbiased estimate for the test error repeat MSE1….MSEn- average these: CV(n) Advantages and Disadvantages (compared to the validation set approach) It tends not to overestimate the test error rate as much as the validation set (there is no randomness in the training/validation set splits) LOOCV has the potential to be ==expensive== to implement, since the model has to be fit n times (if n is large, and if each individual model is slow to fit) k-Fold Cross-Validation step divide data into K roughly equal-sized parts We leave out part k, fit the model to the other K − 1 parts (combined), and then obtain predictions for the left-out kth part CV(K) Algorithm ![[ff05b5c22a054f8e811eee32315b7f9.png]] we are interested only in the location of the minimum point in the estimated test MSE curve K-fold cross-validation Since each training set is only (K − 1)/K as big as the original training set, the prediction error will typically be biased upwards. Why? ==Bias is increased by reducing the data set == This ==bias is minimized when K = n (LOOCV), but this estimate has high variance ==(since all training sets are nearly equal, so the MSE’s are highly correlated, and depend much on which data set is used) K = 5 or 10 provides a good compromise for the bias-variance trade-off![[8f1efdf723e520b4db9ecbbc0d4050b.png]] The Bootstrap data set is too small The bootstrap can be used to ==estimate the uncertainty associated with a given estimator== or statistical learning method The bootstrap works by==repeatedly== sampling from the same data set, thereby creating multiple data sets就是有放回抽样![[f6a78ccd91fcac3b53a4b82b8704b55.png]] minimize the risk (反正就有个公式) Time series if the data is a time series, we can’t simply sample the observations with replacement. Why not? The data is correlated over time; this correlation represents the dynamics. Randomly resampling will completely alter the dynamics"><meta name="og:description" content="MotivationBias and VariancePrediction errors are due to Bias: the inability of the model to capture essential interactions, processes, or dynamics of the true system. Bias is associated with ==non-flexibility==, or ==lacking information in the data set== Variance: sensitivity of coefficients towards errors or variations in the data. Variance is associated with ==flexibility.== Irreducible errors: measurement errors that cannot be predicted为什么 test 会U形，training 随着flexibility 下降 The challenge lies in finding a method for which both the variance and the squared bias are low. 两个评价方面 Model assessment (performance) Model selection (flexibility) Test error and training error test error:used in its training training error: not used in its training 其实下面这个问题就是那个图 The training error rate often is quite different from the test error rate, and in particular the training error can considerably ==underestimate== the test error What is the most likely cause, bias or variance? Variance, a sensitivity towards the configuration of the data (training error is small, test error is large) Best estimation of test error: ==a large== designated test set. (大量的test set) Sometimes not available mathematical ==adjustment== to the training error rate in order to estimate the test error rate For example: R2adjusted. Others: Cp statistic, AIC and BIC In this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations The Validation Set Approach Here we ==randomly== divide the available set of samples into two parts: a ==training set ==and a ==validation or hold-out set== Drawbacks test error is highly variable, and only half of the obsevations » hence, the validation set error tends to overestimate the test error 50% of the data: inability to incorporate all information from the data set (bias) Leave one out cross validation Steps spliting the set of ob into two parts single ob is used for the validation set remaining ob make up the training set prediction y^_1 ,using x_1 Since (x1, y1) was not used in the fitting process, MSE1 = (y1 − y ̂1)2 provides an approximately unbiased estimate for the test error repeat MSE1….MSEn- average these: CV(n) Advantages and Disadvantages (compared to the validation set approach) It tends not to overestimate the test error rate as much as the validation set (there is no randomness in the training/validation set splits) LOOCV has the potential to be ==expensive== to implement, since the model has to be fit n times (if n is large, and if each individual model is slow to fit) k-Fold Cross-Validation step divide data into K roughly equal-sized parts We leave out part k, fit the model to the other K − 1 parts (combined), and then obtain predictions for the left-out kth part CV(K) Algorithm ![[ff05b5c22a054f8e811eee32315b7f9.png]] we are interested only in the location of the minimum point in the estimated test MSE curve K-fold cross-validation Since each training set is only (K − 1)/K as big as the original training set, the prediction error will typically be biased upwards. Why? ==Bias is increased by reducing the data set == This ==bias is minimized when K = n (LOOCV), but this estimate has high variance ==(since all training sets are nearly equal, so the MSE’s are highly correlated, and depend much on which data set is used) K = 5 or 10 provides a good compromise for the bias-variance trade-off![[8f1efdf723e520b4db9ecbbc0d4050b.png]] The Bootstrap data set is too small The bootstrap can be used to ==estimate the uncertainty associated with a given estimator== or statistical learning method The bootstrap works by==repeatedly== sampling from the same data set, thereby creating multiple data sets就是有放回抽样![[f6a78ccd91fcac3b53a4b82b8704b55.png]] minimize the risk (反正就有个公式) Time series if the data is a time series, we can’t simply sample the observations with replacement. Why not? The data is correlated over time; this correlation represents the dynamics. Randomly resampling will completely alter the dynamics"><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/10/MachineLearning-5.-Resampling-methods/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-10"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜5. Resampling methods</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/10 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 5069 字，约 15 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"> <img style="height:72px;width:72px" src="https://buliangzhang24.github.io/assets/images/qrcode.jpg" alt="Buliangzhang" /></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h1 id="motivation">Motivation</h1><h2 id="bias-and-variance">Bias and Variance</h2><h3 id="prediction-errors-are-due-to">Prediction errors are due to</h3><ul><li>Bias: the inability of the model to capture essential interactions, processes, or dynamics of the true system. Bias is associated with ==non-flexibility==, or ==lacking information in the data set==</li><li>Variance: sensitivity of coefficients towards errors or variations in the data. Variance is associated with ==flexibility.==</li><li>Irreducible errors: measurement errors that cannot be predicted 为什么 test 会U形，training 随着flexibility 下降</li><li>The challenge lies in finding a method for which both the variance and the squared bias are low.<h3 id="两个评价方面">两个评价方面</h3></li><li>Model assessment (performance)</li><li>Model selection (flexibility)<h3 id="test-error-and-training-error">Test error and training error</h3></li><li>test error:used in its training</li><li>training error: not used in its training</li><li>其实下面这个问题就是那个图 The training error rate often is quite different from the test error rate, and in particular the training error can considerably ==underestimate== the test error<ul><li>What is the most likely cause, bias or variance? Variance, a sensitivity towards the configuration of the data (training error is small, test error is large)</li></ul></li><li>Best estimation of test error: ==a large== designated test set. (大量的test set) Sometimes not available</li><li>mathematical ==adjustment== to the training error rate in order to estimate the test error rate For example: R2adjusted. Others: Cp statistic, AIC and BIC In this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations<h1 id="the-validation-set-approach">The Validation Set Approach</h1><p>Here we ==randomly== divide the available set of samples into two parts: a ==training set ==and a ==validation or hold-out set==</p><h3 id="drawbacks">Drawbacks</h3></li><li>test error is highly variable, and only half of the obsevations » hence, the <mark class="hltr-blue">validation set error tends to overestimate the test error</mark> 50% of the data: inability to incorporate all information from the data set (bias)<h1 id="leave-one-out-cross-validation">Leave one out cross validation</h1><h2 id="steps">Steps</h2></li><li>spliting the set of ob into two parts</li><li>single ob is used for the validation set</li><li>remaining ob make up the training set</li><li>prediction y^_1 ,using x_1</li><li>Since (x1, y1) was not used in the fitting process, MSE1 = (y1 − y ̂1)2 provides an approximately unbiased estimate for the test error</li><li>repeat MSE1….MSEn -<mark class="hltr-purple"> average these: CV(n)</mark><h2 id="advantages-and-disadvantages">Advantages and Disadvantages</h2><p>(compared to the validation set approach)</p></li><li>It tends <mark class="hltr-blue">not to overestimate the test error rate as much as the validation se</mark>t (there is no randomness in the training/validation set splits)</li><li>LOOCV has the potential to be ==expensive== to implement, since the model has to be fit n times (if n is large, and if each individual model is slow to fit)<h1 id="k-fold-cross-validation">k-Fold Cross-Validation</h1><h2 id="step">step</h2></li><li>divide data into K roughly equal-sized parts</li><li>We leave out part k, fit the model to the other K − 1 parts (combined), and then obtain predictions for the left-out kth part</li><li>CV(K)<h2 id="algorithm">Algorithm</h2><p>![[ff05b5c22a054f8e811eee32315b7f9.png]]</p></li><li>we are interested only in the location of the minimum point in the estimated test MSE curve<h2 id="k-fold-cross-validation-1">K-fold cross-validation</h2></li><li>Since each training set is only (K − 1)/K as big as the original training set, the prediction error will typically be biased upwards. Why? ==Bias is increased by reducing the data set ==</li><li>This ==bias is minimized when K = n (LOOCV), but this estimate has high variance ==(since all training sets are nearly equal, so the MSE’s are highly correlated, and depend much on which data set is used)</li><li>K = 5 or 10 provides a good compromise for the bias-variance trade-off ![[8f1efdf723e520b4db9ecbbc0d4050b.png]]<h1 id="the-bootstrap">The Bootstrap</h1></li><li>data set is too small</li><li>The bootstrap can be used to ==estimate the uncertainty associated with a given estimator== or statistical learning method</li><li>The bootstrap works by==repeatedly== sampling from the same data set, thereby creating multiple data sets 就是有放回抽样 ![[f6a78ccd91fcac3b53a4b82b8704b55.png]]</li><li>minimize the risk (反正就有个公式)<h3 id="time-series">Time series</h3><p>if the data is a time series, we can’t simply sample the observations with replacement. Why not?</p></li><li><p>The data is correlated over time; this correlation represents the dynamics. Randomly resampling will completely alter the dynamics</p></li><li><strong>exercise 5.3</strong> a) Spot the mistake: k-fold cross-validation is implemented by taking the set of n observations and randomly splitting into k nonoverlapping groups. Each of these groups acts as a validation set and the remainder as a training set. The test error is estimated by taking the largest of the k resulting MSE estimates b) What are the (dis)advantages of LOOCV compared to a validation set approach Regarding computational requirements? Regarding bias? c) What are the (dis)advantages of k-fold cross-validation relative to The validation set approach? LOOCV?</li></ul><p>(a)False average not largest (b) LOOCV computational more, <mark class="hltr-purple">bias less</mark>(using the more data to validate) (c)</p><ol><li>validation error not overestimate test error</li><li>K fold easy to compute less expensive, <mark class="hltr-purple">K fold bias higher</mark></li></ol><ul><li><strong>Cross-validation: right and wrong</strong> Consider a simple classifier applied to a two-class data set:<ol><li>Starting with 5000 predictors and 50 samples, find the 100 predictors having the largest correlation with the class labels</li><li>We choose a classifier, such as logistic regression, using only these 100 predictors</li><li>We apply cross-validation to estimate the MSE of this classifier</li></ol></li></ul><p>Is this allowed? ????</p><p><strong>exercise 5.1</strong> ![[5d8d3fff1e9e95a10ae6e1c49182aeb.png]] 有个斜率为0， df(x)/dx <strong>exercise 5.2</strong> Suppose that we obtain a bootstrap sample from a set of n observations What is the probability that the first bootstrap observation is not the jth observation from the original sample?What is the probability that the second bootstrap observation is not the jth observation from the original sample? 1-1/n 1-(1/n-1)!!! not the same 有放回抽样</p><p><strong>Suppose that we obtain a bootstrap sample from a set of n observations Argue that the probability that the jth observation is not in the bootstrap sample is (1−1/n)^n</strong> ？？？ When n=5, what is the probability that the jth observation is in the bootstrap sample? 排列组合</p><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/10/MachineLearning-5.-Resampling-methods/" target="_blank">https://buliangzhang24.github.io/2024/02/10/MachineLearning-5.-Resampling-methods/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727729756', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
