<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearningï½œ6.Model selection and Regularization &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/11/MachineLearning-6.Model-selection-and-Regularization/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearningï½œ6.Model selection and Regularization"><meta name="keywords" content="Model selection and Regularization"><meta name="og:keywords" content="Model selection and Regularization"><meta name="description" content="Motivation prediction accuracy avoid overfitting ç‰¹åˆ«æ˜¯p&gt;n çš„æ—¶å€™ï¼Œå°±æ˜¯å˜é‡å¤§äºæ ·æœ¬é‡ model interpreteability lower practical requirements:fewer measurements, less computation visualization, if selecting 2-3 features Introduction linear model, least squares minimization Solution feature subset selection: select feature extraction(PCA): map to shrinkage: adjust coefficients so that some features are used to a lesser extent (or not at all) regularization: reducing the model flexibility Feature subset selection Basic approach: select the best features Problem: the k best individual features are not necessarily the best set of k features(bc, the corelated ) Best Subset Selection(Exhaustive) Search space 2^p -1 subsets Why not try all possible subsets then? è™½ç„¶å°è¯•æ‰€æœ‰å¯èƒ½çš„å­é›†æ˜¯ç†è®ºä¸Šçš„å®Œç¾æ–¹æ³•ï¼Œä½†åœ¨å®é™…æƒ…å†µä¸‹ï¼Œç”±äºæœç´¢ç©ºé—´çš„å·¨å¤§ï¼Œè¿™å‡ ä¹æ˜¯ä¸å¯è¡Œçš„ algorithm: ä¸ºäº†åœ¨è¿™ä¸ªåºå¤§çš„æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³çš„å­é›†ï¼Œæœ€ä½³å­é›†é€‰æ‹©æ–¹æ³•é€šå¸¸ä½¿ç”¨è´ªå©ªç®—æ³•ã€‚è¯¥ç®—æ³•ä»ç©ºé›†å¼€å§‹ï¼Œç„¶ååœ¨æ¯ä¸€æ­¥ä¸­æ·»åŠ ä¸€ä¸ªç‰¹å¾ï¼Œä»è€Œæ„å»ºä¸€ä¸ªç‰¹å¾å­é›†ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•è¯„ä¼°å½“å‰ç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©å¯¹æ€§èƒ½æœ‰æœ€å¤§è´¡çŒ®çš„ç‰¹å¾æ·»åŠ åˆ°å­é›†ä¸­ã€‚è¯¥è¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡kä¸ºæ­¢ã€‚ let M_0 denote the null model for k =1,2â€¦.p&gt;fit all models that use k predictors&gt; M_k is the best of these(smallest RSS or largest R^2)&gt; select a single best model from M_0â€¦.M_p based on CV error, C,AIC,BIC,adjusted R^2 ç¼ºç‚¹ computationally intensive when ğ‘ is large, risk finding models that do well on training data but do not generalize well Stepwise Selection Forward Selection start with no predictors, iteratively add predictors until a stop criterion is reached Computational advantage is clear, but may miss optimal subset"><meta name="og:description" content="Motivation prediction accuracy avoid overfitting ç‰¹åˆ«æ˜¯p&gt;n çš„æ—¶å€™ï¼Œå°±æ˜¯å˜é‡å¤§äºæ ·æœ¬é‡ model interpreteability lower practical requirements:fewer measurements, less computation visualization, if selecting 2-3 features Introduction linear model, least squares minimization Solution feature subset selection: select feature extraction(PCA): map to shrinkage: adjust coefficients so that some features are used to a lesser extent (or not at all) regularization: reducing the model flexibility Feature subset selection Basic approach: select the best features Problem: the k best individual features are not necessarily the best set of k features(bc, the corelated ) Best Subset Selection(Exhaustive) Search space 2^p -1 subsets Why not try all possible subsets then? è™½ç„¶å°è¯•æ‰€æœ‰å¯èƒ½çš„å­é›†æ˜¯ç†è®ºä¸Šçš„å®Œç¾æ–¹æ³•ï¼Œä½†åœ¨å®é™…æƒ…å†µä¸‹ï¼Œç”±äºæœç´¢ç©ºé—´çš„å·¨å¤§ï¼Œè¿™å‡ ä¹æ˜¯ä¸å¯è¡Œçš„ algorithm: ä¸ºäº†åœ¨è¿™ä¸ªåºå¤§çš„æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³çš„å­é›†ï¼Œæœ€ä½³å­é›†é€‰æ‹©æ–¹æ³•é€šå¸¸ä½¿ç”¨è´ªå©ªç®—æ³•ã€‚è¯¥ç®—æ³•ä»ç©ºé›†å¼€å§‹ï¼Œç„¶ååœ¨æ¯ä¸€æ­¥ä¸­æ·»åŠ ä¸€ä¸ªç‰¹å¾ï¼Œä»è€Œæ„å»ºä¸€ä¸ªç‰¹å¾å­é›†ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•è¯„ä¼°å½“å‰ç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©å¯¹æ€§èƒ½æœ‰æœ€å¤§è´¡çŒ®çš„ç‰¹å¾æ·»åŠ åˆ°å­é›†ä¸­ã€‚è¯¥è¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡kä¸ºæ­¢ã€‚ let M_0 denote the null model for k =1,2â€¦.p&gt;fit all models that use k predictors&gt; M_k is the best of these(smallest RSS or largest R^2)&gt; select a single best model from M_0â€¦.M_p based on CV error, C,AIC,BIC,adjusted R^2 ç¼ºç‚¹ computationally intensive when ğ‘ is large, risk finding models that do well on training data but do not generalize well Stepwise Selection Forward Selection start with no predictors, iteratively add predictors until a stop criterion is reached Computational advantage is clear, but may miss optimal subset"><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/11/MachineLearning-6.Model-selection-and-Regularization/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-11"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearningï½œ6.Model selection and Regularization</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/11 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> å…± 6635 å­—ï¼Œçº¦ 19 åˆ†é’Ÿ </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h1 id="motivation">Motivation</h1><ul><li>prediction accuracy avoid overfitting ç‰¹åˆ«æ˜¯p&gt;n çš„æ—¶å€™ï¼Œå°±æ˜¯å˜é‡å¤§äºæ ·æœ¬é‡</li><li>model interpreteability</li><li><mark class="hltr-blue">lower practical requirements:fewer measurements, less computation</mark></li><li><mark class="hltr-blue">visualization</mark>, if selecting 2-3 features<h1 id="introduction">Introduction</h1><p>linear model, least squares minimization</p><h2 id="solution">Solution</h2></li><li>feature subset selection: select</li><li>feature extraction(PCA): map to</li><li>shrinkage: adjust coefficients so that some features are used to a lesser extent (or not at all)</li><li>regularization: reducing the model flexibility<h1 id="feature-subset-selection">Feature subset selection</h1></li><li>Basic approach: select the best features</li><li><mark class="hltr-blue">Problem</mark>: the k best individual features are not necessarily the best set of k features(bc, the corelated )<h2 id="best-subset-selectionexhaustive">Best Subset Selection(Exhaustive)</h2></li><li>Search space 2^p -1 subsets</li><li>Why not try all possible subsets then? è™½ç„¶å°è¯•æ‰€æœ‰å¯èƒ½çš„å­é›†æ˜¯ç†è®ºä¸Šçš„å®Œç¾æ–¹æ³•ï¼Œä½†åœ¨å®é™…æƒ…å†µä¸‹ï¼Œç”±äºæœç´¢ç©ºé—´çš„å·¨å¤§ï¼Œè¿™å‡ ä¹æ˜¯ä¸å¯è¡Œçš„</li><li>algorithm: ä¸ºäº†åœ¨è¿™ä¸ªåºå¤§çš„æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³çš„å­é›†ï¼Œæœ€ä½³å­é›†é€‰æ‹©æ–¹æ³•é€šå¸¸ä½¿ç”¨è´ªå©ªç®—æ³•ã€‚è¯¥ç®—æ³•ä»ç©ºé›†å¼€å§‹ï¼Œç„¶ååœ¨æ¯ä¸€æ­¥ä¸­æ·»åŠ ä¸€ä¸ªç‰¹å¾ï¼Œä»è€Œæ„å»ºä¸€ä¸ªç‰¹å¾å­é›†ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•è¯„ä¼°å½“å‰ç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©å¯¹æ€§èƒ½æœ‰æœ€å¤§è´¡çŒ®çš„ç‰¹å¾æ·»åŠ åˆ°å­é›†ä¸­ã€‚è¯¥è¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡kä¸ºæ­¢ã€‚<ul><li>let M_0 denote the null model</li><li>for k =1,2â€¦.p&gt;fit all models that use k predictors&gt; M_k is the best of these(smallest RSS or largest R^2)&gt; select a single best model from M_0â€¦.M_p based on CV error, C,AIC,BIC,adjusted R^2</li></ul></li><li><mark class="hltr-blue">ç¼ºç‚¹</mark><ul><li>computationally intensive</li><li>when ğ‘ is large, risk finding models that do well on training data but do <mark class="hltr-cyan">not generalize well</mark><h2 id="stepwise-selection">Stepwise Selection</h2><h3 id="forward-selection">Forward Selection</h3></li></ul></li><li>start with no predictors, iteratively add predictors until a stop criterion is reached</li><li><mark class="hltr-blue">Computational advantage is clear, but may miss optimal subset</mark></li></ul><h3 id="backward-selection">Backward Selection</h3><p>start with all predictors, iteratively removepredictors until a stop criterion is reached</p><ul><li>Computational advantage is clear, but may miss optimal subset</li><li>Backward selection requires that <mark class="hltr-blue">ğ‘› &gt; ğ‘ </mark>for fitting the full model, forward selection does not<h3 id="both">both</h3></li><li>both add/remove in each step the predictor that gives the greatest improvement/smallest deterioration</li><li>search through only 1+p(p+1)/2</li><li>are not guaranteed to find the best mode</li><li>Hybrid approaches (â€œ+l-râ€):take l steps forward,then r steps back<h2 id="choosing-the-optimal-model">Choosing the optimal model</h2></li><li>Full model will always have the smallest RSS and the largest ğ‘…2 â€“ on the <mark class="hltr-blue">training data</mark>would like to select model that performs best on <mark class="hltr-blue">test data</mark></li><li>æ€ä¹ˆé€‰æ‹©<ul><li><mark class="hltr-blue">estimate test error indirectly</mark>, based on the training error: fast, but based on assumptions</li><li><mark class="hltr-blue">estimate the test error directly</mark>,using a test set or cross-validation:more accurate, but slower<h3 id="test-set-error-estimates">Test set error estimates</h3><p>Mallowâ€™s C_p AIC BIC Adjustes R^2</p><h3 id="one-standard-error-rule">One standard-error rule</h3><p>![[38046a12348c1f0c3e0b13fcbd5f395.png]] <strong>Exercise</strong> 1.We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2,â€¦,p predictors. Explain your answers: (a) Which of the three models with k predictors has the smallest <mark class="hltr-blue">training RSS?</mark> (a) best subset//on training (b) Which of the three models with k predictors has the smallest <mark class="hltr-blue">test RSS?</mark> (b)it depends???<mark class="hltr-blue"> best subset</mark> (c) True or False: i. The predictors in the k-variable model identified by <em>forward stepwise</em> are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection. ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by backward stepwise selection. iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection. iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection. v.<mark class="hltr-blue"> The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection</mark> (the best subset of k+1 does not necessarily contain the best subset of k variables) (c)True,False(True),False,False,True(Falseä¼šå˜)</p><h1 id="shrinkage">Shrinkage</h1><p>increased bias, reduced variance</p></li></ul></li><li>adjust coefficients so that some features are used to a lesser extent (or not at all)</li><li>Alternative to subset selection:shrink coefficients towards zero by constraining or regularizing<h2 id="ridge-regression">Ridge Regression</h2><p>![[5f76602c2bd4d5840473f4bdfada468.png]]</p></li><li>ç¬¬äºŒé¡¹æ˜¯shrinkage penalty: å½“Bæ¥è¿‘0ï¼Œ è¿™ä¸ªä¹Ÿå¾ˆå°</li><li>Tuning parameter alamda control relative impact: ä¸º0 å°±æ˜¯æ™®é€šçš„least squares, æ— é™å¤§å°±all ğ›½=s will be zero (only intercept estimated)</li><li>Selecting ğœ† is critical: use cross-validation ![[2f3a0d80981ccd473cdd2bbe362b468.png]]<h3 id="scaling">Scaling</h3></li><li>Standard least squares is scale equivariant: multiplying ğ‘‹ğ‘— by a constant ğ‘ means ğ›½ğ‘— scales by a factor of 1/ğ‘: ğ‘‹ğ‘— ğ›½ğ‘— will remain the same.</li><li>Ridge regression is scale-sensitive, due to the penalty</li><li><mark class="hltr-blue">Solution: normalize predictors first</mark><h3 id="bias-variance">Bias-variance</h3></li><li>Ridge regression reduces flexibility: decreases variance at the cost of increased bias</li><li>ğœ† controls flexibility of predictor ![[d5dfa2fb2799146edd09a4994ea7a43.png]] <mark class="hltr-blue">With increasing dataset size,variance component decreases, bias stays the same flexibility </mark> variance æ˜¯å…³æ³¨æ•æ„Ÿåº¦ï¼Œbiasæ˜¯å…³æ³¨çµæ´»åº¦ï¼Œå¦‚æœæ•°æ®å˜å¤šï¼Œvarianceå°±ä¼šæ›´ä¸æ•æ„Ÿï¼Œæ‰€ä»¥å°±ä¸‹é™ã€‚<h2 id="the-lasso">The Lasso</h2></li><li>Disadvantage of ridge regression: all ğ‘ predictors are still used in the final mode ![[cd6f3e58f5d5e26790efb184abcdddd.png]]</li><li>Also shrinks coefficients towards zero, but ğ‘™1 penalty forces some to be exactly zero</li><li>actually performs variable subset selection</li><li>yields sparse models, with just a small set of variables ç¨€ç–æ¨¡å‹<h2 id="lasso-vs-ridge">Lasso vs. ridge</h2><p>Why does lasso give coefficients of zero? ![[3e6d927a87d19d9b3a06d8b91a88ae5.png]] 2.For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer. (a) The lasso, relative to least squares, is: i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. <mark class="hltr-blue">ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. </mark> <mark class="hltr-blue">iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.</mark> iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. (b) Repeat (a) for ridge regression relative to least squares. (c) Repeat (a) for non-linear methods relative to least squares. 2.i, i, ivÂ»&gt; iii,iii, ii</p></li></ul><p>4.Suppose we estimate the regression coefficients in a linear regression model by minimizing for a particular value of Î». For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer. i. Increase initially, and then eventually start decreasing in an inverted U shape. ii. Decrease initially, and then eventually start increasing in a U shape. iii. Steadily increase. iv. Steadily decrease. v. Remain constant. (a) As we increase Î» from 0, the training RSS will:iii (b) Repeat (a) for test RSS. ii (c) Repeat (a) for variance. iv (d) Repeat (a) for (squared) bias. iii (e) Repeat (a) for the irreducible error. v 4, iv, ii, ii, iv, v.Â»&gt;<mark class="hltr-blue"> iii,ii,</mark> iv ,iii , v (å°±æ˜¯lamdaå˜åŒ–ï¼Œ æ¨¡å‹çš„å¤æ‚åº¦ä¹Ÿä¼šå˜åŒ–ï¼Œç„¶åå†æƒ³è¿™ä¸ªä¸œè¥¿æ€ä¹ˆå˜åŒ–)</p><h1 id="high-dimensional-spaces">High-dimensional spaces</h1><p>Modern sensors yield many variables - often ğ‘ â‰« n Most (statistical) approaches were developed for problems with few variables</p><h2 id="overfitting">Overfitting</h2><ul><li>Least squares does not work when ğ‘ &gt; ğ‘› : too flexible</li><li>Even when ğ‘ &lt; ğ‘›, danger of overfitting<h2 id="curse-of-dimensionality">Curse of dimensionality</h2><p>å°±æ˜¯æœ¬æ¥è¶Šå¤šçš„features åº”è¯¥ç»™æˆ‘ä»¬æ›´å¤šçš„ä¿¡æ¯ï¼Œä½†æ˜¯ the number of <mark class="hltr-blue">parameters</mark> to estimate increases with the number of <mark class="hltr-blue">measurements</mark> ä¸ºäº†ä¼°è®¡è¿™äº›Parameterså°±è¦æ›´å¤šçš„samples. Consequence: given a certain number of samples there is an optimal number of features to use.</p><h2 id="model-performance-vs-flexibility">Model performance vs. flexibility</h2><p>largest risk in modelling is overfittingå°±æ˜¯å› ä¸ºå¯¹è®­ç»ƒé›†æ‹Ÿåˆçš„å¤ªå¥½äº†ï¼Œå¯¼è‡´ä¸èƒ½å¾ˆå¥½çš„æ³›åŒ–åˆ«çš„æ•°æ®é›†äº† Model overfits when it performs (much) better on training data than on test data ![[424cf1397ff85d83912dbf1504b867b.png]]</p></li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>æ–‡æ¡£ä¿¡æ¯</h3><ul><li>æœ¬æ–‡ä½œè€…ï¼š<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>æœ¬æ–‡é“¾æ¥ï¼š<a href="https://buliangzhang24.github.io/2024/02/11/MachineLearning-6.Model-selection-and-Regularization/" target="_blank">https://buliangzhang24.github.io/2024/02/11/MachineLearning-6.Model-selection-and-Regularization/</a></li><li>ç‰ˆæƒå£°æ˜ï¼šè‡ªç”±è½¬è½½-éå•†ç”¨-éè¡ç”Ÿ-ä¿æŒç½²åï¼ˆ<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">åˆ›æ„å…±äº«3.0è®¸å¯è¯</a>ï¼‰</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727806131', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> Â© 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="å›åˆ°é¡¶éƒ¨"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
