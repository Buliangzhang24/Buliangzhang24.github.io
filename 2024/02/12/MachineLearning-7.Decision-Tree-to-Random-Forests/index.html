<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearningï½œ7.Decision Tree to Random Forests &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearningï½œ7.Decision Tree to Random Forests"><meta name="keywords" content="Decision Tree to Random Forests"><meta name="og:keywords" content="Decision Tree to Random Forests"><meta name="description" content="Decision TreesWe categorize data through chain of simple decisions.a binary tree, which is a graph data structure in which each node has at most two childrenTerminologyinput features: data to be classifiedroot node: starts the decision processdecision nodes: Each decision node (split node) implements a test function with discrete outcomesThe test function of each decision node splits the input space into regionsleaf nodes:A leaf node indicates the end of a sequence of decisionsA single (output) class is associated to each leaf nodeAlgorithm to make predictions![[66c68d76a596659140c2d38d0326bbb.png]]Increasing node purityevery decision should increase the node purityQuantifying Node PurityMeasuring Node Purity Low Entropy: more pure;Higher Entropy: less pureÂ»H(p) how flat Gini Index: higher gini index ,less pure![[f365b77a8cb7cc4a22d13ad62d2231e.png]]![[c4449526b1dcb9a2458ef1c41d9fc19.png]]0.25* 0.75 4 times0.45* 0.55 2 times + 0.05 * 0.95 2 times Regression Trees leaf nodes return the averagesample variance as node purity,high variance before splitting, low variacne after splitting Classification versus Regression Trees How to find the thresholde t? Try different thresholdsoptimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]] Binary Splitting we test as many thresholds as we have points![[ab2533bf07de0f47d6c10fc874cc20f.png]] Growing a Decision tree with Recursive Binary Splitting: perform binary splitting recursively for each decision node to maximize node purity We end uo with one sample per node = a 1-NN regressor/classifier Overfitting of DTs Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Regularization of DTs Restricting Depth Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Tree Pruning Slow Growsï¼ˆé™åˆ¶ç”Ÿé•¿ï¼Œgreedyï¼‰ Strategy: Grow first the tree slowly with additional checks during growingâ€¢ Simple checks: maximum depth,max number of leaf nodes.â€¢ Add decision node only if increase in node purity is above a certain threshold Fast Cutters(å…ˆé•¿å†åˆ‡, non-greedy) Strategy: Grow the full tree and then remove not important nodes later.â€¢ Cost complexity pruning: we identify the weakest link in the tree to remove Cost Complexity Pruning Remove leafs to reduce the complexity, while maintaining a low training error, i.e., cost. The hyperparameter ğ›¼ balances both objectives. Clamada(T) = C(T) + lamada T å°±æ˜¯cost(training error)+complexity(number leafs) higher alpha: produces shallower tree with less nodes å¦‚ä½•åˆ©ç”¨ cost complexity pruningæ¥æ„å»ºæ ‘ ![[638c2052311e5c539c5d454e45e1a73.png]] Advantages and Disadvantages of Trees Advantages:â— Inexpensive to constructâ— Extremely fast at classifying unknown recordsâ— Easy to interpret for small-sized treesâ— Robust to noise (especially when methods to avoid overfitting are employed)â— Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)Disadvantages: â— Space of possible decision trees is exponentially large. Greedy approaches are often unable to find the best tree.â— Does not take into account interactions between attributesâ— Each decision boundary involves only a single attribute â— too deep ,more sensitive to noise Bagging, Forest,Boosting Baggingæ˜¯åœ¨å¹²å•¥ï¼š we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess low bias each tree fiting vey well high variance &gt; difference between different trees higher Bias - Variance Trade-off: Finding a balance between fitting and overfitting for tree-like classifiiers Strategy 1: Reduce DT complexity Greedy (restrict DT complexity) Non-greedy (post-hoc pruning of the decision tree) Stragegy 2 Ensemble multiple Decision Trees togetheré›†æˆå¤šä¸ªå†³ç­–æ ‘ï¼šæ£®æ—ï¼ï¼ Step 1 Bagging split different data subset: randomly sampled subsets (bootstrap) ensemble prediction: majority voting for classification output averaging for regression effect: Decision boundaries become blurry: more realistic boundary Out-of-Bag Error Estimation Process: Fit decision tree DT_i on ğ’Ÿ_i , Test its performance on the out-of-bag set ğ’Ÿ_oob Repeat for each tree in the bag Idea:- This OOB error approximates the test error. Similar idea to cross-validation, but for every DT separately Re-gaining Interpretability Bagging trees improves accuracy over a single tree through increasing diversity in the decisions æ„æ€å°±æ˜¯éšæœºæ€§å¢åŠ äº†ï¼Œæ‰€ä»¥å¢åŠ äº†ç²¾åº¦ But we loose interpretability: Each tree has different decision rules Interpreting many decision rules in parallel is difficult Strategy: Measure the increase in node purity across trees ![[7010e107b505f8fef371cbe4e3f6219.png]] å°±æ˜¯çœ‹æ¯ä¸ªæ„Ÿå…´è¶£ç‚¹ï¼ˆæ©˜è‰²ç‚¹ï¼‰çš„node purityï¼Œç„¶åæ±‚å¹³å‡ if bagging trees are still highly correlated with each other ![[Pasted image 20240226091312.png]] "><meta name="og:description" content="Decision TreesWe categorize data through chain of simple decisions.a binary tree, which is a graph data structure in which each node has at most two childrenTerminologyinput features: data to be classifiedroot node: starts the decision processdecision nodes: Each decision node (split node) implements a test function with discrete outcomesThe test function of each decision node splits the input space into regionsleaf nodes:A leaf node indicates the end of a sequence of decisionsA single (output) class is associated to each leaf nodeAlgorithm to make predictions![[66c68d76a596659140c2d38d0326bbb.png]]Increasing node purityevery decision should increase the node purityQuantifying Node PurityMeasuring Node Purity Low Entropy: more pure;Higher Entropy: less pureÂ»H(p) how flat Gini Index: higher gini index ,less pure![[f365b77a8cb7cc4a22d13ad62d2231e.png]]![[c4449526b1dcb9a2458ef1c41d9fc19.png]]0.25* 0.75 4 times0.45* 0.55 2 times + 0.05 * 0.95 2 times Regression Trees leaf nodes return the averagesample variance as node purity,high variance before splitting, low variacne after splitting Classification versus Regression Trees How to find the thresholde t? Try different thresholdsoptimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]] Binary Splitting we test as many thresholds as we have points![[ab2533bf07de0f47d6c10fc874cc20f.png]] Growing a Decision tree with Recursive Binary Splitting: perform binary splitting recursively for each decision node to maximize node purity We end uo with one sample per node = a 1-NN regressor/classifier Overfitting of DTs Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Regularization of DTs Restricting Depth Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Tree Pruning Slow Growsï¼ˆé™åˆ¶ç”Ÿé•¿ï¼Œgreedyï¼‰ Strategy: Grow first the tree slowly with additional checks during growingâ€¢ Simple checks: maximum depth,max number of leaf nodes.â€¢ Add decision node only if increase in node purity is above a certain threshold Fast Cutters(å…ˆé•¿å†åˆ‡, non-greedy) Strategy: Grow the full tree and then remove not important nodes later.â€¢ Cost complexity pruning: we identify the weakest link in the tree to remove Cost Complexity Pruning Remove leafs to reduce the complexity, while maintaining a low training error, i.e., cost. The hyperparameter ğ›¼ balances both objectives. Clamada(T) = C(T) + lamada T å°±æ˜¯cost(training error)+complexity(number leafs) higher alpha: produces shallower tree with less nodes å¦‚ä½•åˆ©ç”¨ cost complexity pruningæ¥æ„å»ºæ ‘ ![[638c2052311e5c539c5d454e45e1a73.png]] Advantages and Disadvantages of Trees Advantages:â— Inexpensive to constructâ— Extremely fast at classifying unknown recordsâ— Easy to interpret for small-sized treesâ— Robust to noise (especially when methods to avoid overfitting are employed)â— Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)Disadvantages: â— Space of possible decision trees is exponentially large. Greedy approaches are often unable to find the best tree.â— Does not take into account interactions between attributesâ— Each decision boundary involves only a single attribute â— too deep ,more sensitive to noise Bagging, Forest,Boosting Baggingæ˜¯åœ¨å¹²å•¥ï¼š we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess low bias each tree fiting vey well high variance &gt; difference between different trees higher Bias - Variance Trade-off: Finding a balance between fitting and overfitting for tree-like classifiiers Strategy 1: Reduce DT complexity Greedy (restrict DT complexity) Non-greedy (post-hoc pruning of the decision tree) Stragegy 2 Ensemble multiple Decision Trees togetheré›†æˆå¤šä¸ªå†³ç­–æ ‘ï¼šæ£®æ—ï¼ï¼ Step 1 Bagging split different data subset: randomly sampled subsets (bootstrap) ensemble prediction: majority voting for classification output averaging for regression effect: Decision boundaries become blurry: more realistic boundary Out-of-Bag Error Estimation Process: Fit decision tree DT_i on ğ’Ÿ_i , Test its performance on the out-of-bag set ğ’Ÿ_oob Repeat for each tree in the bag Idea:- This OOB error approximates the test error. Similar idea to cross-validation, but for every DT separately Re-gaining Interpretability Bagging trees improves accuracy over a single tree through increasing diversity in the decisions æ„æ€å°±æ˜¯éšæœºæ€§å¢åŠ äº†ï¼Œæ‰€ä»¥å¢åŠ äº†ç²¾åº¦ But we loose interpretability: Each tree has different decision rules Interpreting many decision rules in parallel is difficult Strategy: Measure the increase in node purity across trees ![[7010e107b505f8fef371cbe4e3f6219.png]] å°±æ˜¯çœ‹æ¯ä¸ªæ„Ÿå…´è¶£ç‚¹ï¼ˆæ©˜è‰²ç‚¹ï¼‰çš„node purityï¼Œç„¶åæ±‚å¹³å‡ if bagging trees are still highly correlated with each other ![[Pasted image 20240226091312.png]] "><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-12"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearningï½œ7.Decision Tree to Random Forests</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/12 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> å…± 5217 å­—ï¼Œçº¦ 15 åˆ†é’Ÿ </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h1 id="decision-trees">Decision Trees</h1><p>We categorize data through <mark class="hltr-orange">chain of simple decisions.</mark> <mark class="hltr-orange">a binary tree,</mark> which is a graph data structure in which each node has at most two children</p><h2 id="terminology">Terminology</h2><p>input features: data to be classified root node: starts the decision process decision nodes: Each decision node (split node) implements a test function with discrete outcomes The test function of each decision node splits the input space into regions leaf nodes: A leaf node indicates the end of a sequence of decisions A single (output) class is associated to each leaf node</p><h2 id="algorithm-to-make-predictions">Algorithm to make predictions</h2><p>![[66c68d76a596659140c2d38d0326bbb.png]]</p><h2 id="increasing-node-purity">Increasing node purity</h2><p>every decision should increase the node purity</p><h3 id="quantifying-node-purity">Quantifying Node Purity</h3><p>Measuring Node Purity</p><ul><li>Low Entropy: more pure;Higher Entropy: less pureÂ»H(p) how flat</li><li>Gini Index: higher gini index ,less pure ![[f365b77a8cb7cc4a22d13ad62d2231e.png]] ![[c4449526b1dcb9a2458ef1c41d9fc19.png]] 0.25* 0.75 4 times 0.45* 0.55 2 times + 0.05 * 0.95 2 times<h2 id="regression-trees">Regression Trees</h2><p>leaf nodes return the average sample variance as node purity,high variance before splitting, low variacne after splitting</p><h2 id="classification-versus-regression-trees">Classification versus Regression Trees</h2><h2 id="how-to-find-the-thresholde-t">How to find the thresholde t?</h2><p>Try different thresholds optimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]]</p><h2 id="binary-splitting">Binary Splitting</h2><p>we test as many thresholds as we have points ![[ab2533bf07de0f47d6c10fc874cc20f.png]]</p><h2 id="growing-a-decision-tree-with-recursive-binary-splitting">Growing a Decision tree with Recursive Binary Splitting:</h2></li><li>perform binary splitting</li><li>recursively for each decision node</li><li>to maximize node purity</li><li>We end uo with one sample per node = a 1-NN regressor/classifier<h3 id="overfitting-of-dts">Overfitting of DTs</h3></li><li>Tree fits training data perfectly (zero variance)</li><li>Tree extremely large, and uninterpretable<h3 id="regularization-of-dts">Regularization of DTs</h3><h4 id="restricting-depth">Restricting Depth</h4></li><li>Tree fits training data perfectly (zero variance)</li><li>Tree extremely large, and uninterpretable<h4 id="tree-pruning">Tree Pruning</h4><h5 id="slow-growsé™åˆ¶ç”Ÿé•¿greedy">Slow Growsï¼ˆé™åˆ¶ç”Ÿé•¿ï¼Œgreedyï¼‰</h5><p><mark class="hltr-blue">Strategy</mark>: Grow first the tree slowly with additional checks during growing â€¢ Simple checks: <mark class="hltr-blue">maximum depth</mark>,max number of <mark class="hltr-blue">leaf </mark>nodes. â€¢ Add decision node only if<mark class="hltr-blue"> increase</mark> <mark class="hltr-blue">in node purity</mark> is above a certain threshold</p><h5 id="fast-cutterså…ˆé•¿å†åˆ‡-non-greedy">Fast Cutters(å…ˆé•¿å†åˆ‡, non-greedy)</h5><p><mark class="hltr-blue">Strategy</mark>: Grow the full tree and then remove not important nodes later. <mark class="hltr-blue">â€¢ Cost complexity pruning</mark>: we identify the weakest link in the tree to remove</p><h5 id="cost-complexity-pruning">Cost Complexity Pruning</h5></li><li><mark class="hltr-blue">Remove leafs to reduce the complexity,</mark> while maintaining a low training error, i.e., cost. The hyperparameter ğ›¼ balances both objectives.</li><li><table><tbody><tr><td>Clamada(T) = C(T) + lamada</td><td>T</td><td>å°±æ˜¯cost(training error)+complexity(number leafs)</td></tr></tbody></table></li><li>higher alpha: produces shallower tree with less nodes<h6 id="å¦‚ä½•åˆ©ç”¨-cost-complexity-pruningæ¥æ„å»ºæ ‘">å¦‚ä½•åˆ©ç”¨ cost complexity pruningæ¥æ„å»ºæ ‘</h6><p>![[638c2052311e5c539c5d454e45e1a73.png]]</p><h2 id="advantages-and-disadvantages-of-trees">Advantages and Disadvantages of Trees</h2><p>Advantages: â—<mark class="hltr-purple"> Inexpensive</mark> to construct â— Extremely<mark class="hltr-purple"> fast </mark>at classifying unknown records â— Easy to interpret <mark class="hltr-purple">for small-sized trees</mark> â— <mark class="hltr-purple">Robust to noise</mark> (especially when methods to avoid overfitting are employed) â— Can easily handle <mark class="hltr-purple">redundant or irrelevant attributes</mark> (unless the attributes are interacting) Disadvantages: â— Space of possible decision trees is <mark class="hltr-purple">exponentially large</mark>. Greedy approaches are often unable to find the <mark class="hltr-purple">best tree.</mark> â— Does not take into account <mark class="hltr-purple">interaction</mark>s between attributes â— Each <mark class="hltr-purple">decision boundary involves only a single attribute </mark> â—<mark class="hltr-purple"> too deep ,more sensitive to noise</mark></p><h1 id="bagging-forestboosting">Bagging, Forest,Boosting</h1></li><li><p>Baggingæ˜¯åœ¨å¹²å•¥ï¼š we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess</p></li><li>low bias each tree fiting vey well</li><li>high variance &gt; difference between different trees higher<h2 id="bias---variance-trade-off-finding-a-balance-between-fitting-and-overfitting">Bias - Variance Trade-off: Finding a balance between fitting and overfitting</h2><p>for tree-like classifiiers</p><h3 id="strategy-1-reduce-dt-complexity">Strategy 1: Reduce DT complexity</h3></li><li>Greedy (restrict DT complexity)</li><li>Non-greedy (post-hoc pruning of the decision tree)<h3 id="stragegy-2-ensemble-multiple-decision-trees-togetheré›†æˆå¤šä¸ªå†³ç­–æ ‘æ£®æ—">Stragegy 2 Ensemble multiple Decision Trees togetheré›†æˆå¤šä¸ªå†³ç­–æ ‘ï¼šæ£®æ—ï¼ï¼</h3><h4 id="step-1-bagging">Step 1 Bagging</h4></li><li>split different data subset: randomly sampled subsets (bootstrap)</li><li>ensemble prediction: majority voting for classification output averaging for regression</li><li>effect: Decision boundaries become blurry: more realistic boundary<h5 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h5><p>Process:</p><ol><li>Fit decision tree DT_i on ğ’Ÿ_i ,</li><li>Test its performance on the out-of-bag set ğ’Ÿ_oob</li><li>Repeat for each tree in the bag Idea: -<mark class="hltr-blue"> This OOB error approximates the test error. </mark></li></ol></li><li>Similar idea to cross-validation, but for every DT separately<h5 id="re-gaining-interpretability">Re-gaining Interpretability</h5></li><li>Bagging trees <mark class="hltr-blue">improves accuracy</mark> over a single tree through increasing <mark class="hltr-blue">diversity in the decisions</mark> æ„æ€å°±æ˜¯éšæœºæ€§å¢åŠ äº†ï¼Œæ‰€ä»¥å¢åŠ äº†ç²¾åº¦</li><li>But we <mark class="hltr-blue">loose interpretability</mark>: Each tree has different decision rules Interpreting many decision rules in parallel is difficult</li><li>Strategy: Measure the <mark class="hltr-blue">increase in node purity across trees </mark></li><li>![[7010e107b505f8fef371cbe4e3f6219.png]] å°±æ˜¯çœ‹æ¯ä¸ªæ„Ÿå…´è¶£ç‚¹ï¼ˆæ©˜è‰²ç‚¹ï¼‰çš„node purityï¼Œç„¶åæ±‚å¹³å‡<h5 id="if-bagging-trees-are-still-highly-correlated-with-each-other">if bagging trees are still highly correlated with each other</h5><p>![[Pasted image 20240226091312.png]]</p></li></ul><h4 id="step-2-random-forest">Step 2 Random forest</h4><ul><li>Idea: increase diversity by removing features from individual trees (removing the root node , to increase diversity)</li><li>Implementation:we take a random sample of predictors for each tree (usually æ ¹å·ä¸‹ ğ¹)(for instance, 4 features out of 15)</li><li>for each decision tree : <mark class="hltr-blue">éšæœºtrained with a dataset subsetå’Œ subset of features</mark></li><li>Bags and Random Forest:<ul><li>each tree is trained on a different <mark class="hltr-blue">â€œbootstrapped</mark>â€ set of train data</li><li>trees are trained separately from each other<mark class="hltr-blue"> in parallel </mark></li></ul></li><li>Random Forests: additionally each tree is trained with a different <mark class="hltr-blue">subset of features</mark><h2 id="boosting">Boosting</h2><h3 id="random-forest-versus-boosting-philosophy">Random Forest versus Boosting Philosophy</h3></li><li>Random Forest: accuracy through classifier diversity all trees grown in parallel</li><li>Boosting: many small â€œweakâ€ classifiers trees grown sequentially<h3 id="principles">Principles</h3></li><li>Train one classifier after another</li><li>Reweight the data so that wrongly classified data is more important (grow it one by one, there is order of importance)<h3 id="adaboost">AdaBoost</h3><h4 id="three-core-ideas">Three Core Ideas:</h4></li><li>Combines many â€œweak learnersâ€</li><li>some stumps are weighted higher than others</li><li>each stump takes the previous stumps into account ![[6122295845e0fc7d25e6f98889b33de.png]] ![[ffc41bc78dda2f930d891bd74eb8cdd.png]] ![[d7ec32aa303a927332cc68cfd93669e.png]]</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>æ–‡æ¡£ä¿¡æ¯</h3><ul><li>æœ¬æ–‡ä½œè€…ï¼š<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>æœ¬æ–‡é“¾æ¥ï¼š<a href="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/" target="_blank">https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/</a></li><li>ç‰ˆæƒå£°æ˜ï¼šè‡ªç”±è½¬è½½-éå•†ç”¨-éè¡ç”Ÿ-ä¿æŒç½²åï¼ˆ<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">åˆ›æ„å…±äº«3.0è®¸å¯è¯</a>ï¼‰</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727735013', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> Â© 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="å›åˆ°é¡¶éƒ¨"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
