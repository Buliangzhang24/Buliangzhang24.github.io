<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜7.Decision Tree to Random Forests &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜7.Decision Tree to Random Forests"><meta name="keywords" content="Decision Tree to Random Forests"><meta name="og:keywords" content="Decision Tree to Random Forests"><meta name="description" content="Decision TreesWe categorize data through chain of simple decisions.a binary tree, which is a graph data structure in which each node has at most two childrenTerminologyinput features: data to be classifiedroot node: starts the decision processdecision nodes: Each decision node (split node) implements a test function with discrete outcomesThe test function of each decision node splits the input space into regionsleaf nodes:A leaf node indicates the end of a sequence of decisionsA single (output) class is associated to each leaf nodeAlgorithm to make predictions![[66c68d76a596659140c2d38d0326bbb.png]]Increasing node purityevery decision should increase the node purityQuantifying Node PurityMeasuring Node Purity Low Entropy: more pure;Higher Entropy: less pure»H(p) how flat Gini Index: higher gini index ,less pure![[f365b77a8cb7cc4a22d13ad62d2231e.png]]![[c4449526b1dcb9a2458ef1c41d9fc19.png]]0.25* 0.75 4 times0.45* 0.55 2 times + 0.05 * 0.95 2 times Regression Trees leaf nodes return the averagesample variance as node purity,high variance before splitting, low variacne after splitting Classification versus Regression Trees How to find the thresholde t? Try different thresholdsoptimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]] Binary Splitting we test as many thresholds as we have points![[ab2533bf07de0f47d6c10fc874cc20f.png]] Growing a Decision tree with Recursive Binary Splitting: perform binary splitting recursively for each decision node to maximize node purity We end uo with one sample per node = a 1-NN regressor/classifier Overfitting of DTs Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Regularization of DTs Restricting Depth Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Tree Pruning Slow Grows（限制生长，greedy） Strategy: Grow first the tree slowly with additional checks during growing• Simple checks: maximum depth,max number of leaf nodes.• Add decision node only if increase in node purity is above a certain threshold Fast Cutters(先长再切, non-greedy) Strategy: Grow the full tree and then remove not important nodes later.• Cost complexity pruning: we identify the weakest link in the tree to remove Cost Complexity Pruning Remove leafs to reduce the complexity, while maintaining a low training error, i.e., cost. The hyperparameter 𝛼 balances both objectives. Clamada(T) = C(T) + lamada T 就是cost(training error)+complexity(number leafs) higher alpha: produces shallower tree with less nodes 如何利用 cost complexity pruning来构建树 ![[638c2052311e5c539c5d454e45e1a73.png]] Advantages and Disadvantages of Trees Advantages:● Inexpensive to construct● Extremely fast at classifying unknown records● Easy to interpret for small-sized trees● Robust to noise (especially when methods to avoid overfitting are employed)● Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)Disadvantages: ● Space of possible decision trees is exponentially large. Greedy approaches are often unable to find the best tree.● Does not take into account interactions between attributes● Each decision boundary involves only a single attribute ● too deep ,more sensitive to noise Bagging, Forest,Boosting Bagging是在干啥： we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess low bias each tree fiting vey well high variance &gt; difference between different trees higher Bias - Variance Trade-off: Finding a balance between fitting and overfitting for tree-like classifiiers Strategy 1: Reduce DT complexity Greedy (restrict DT complexity) Non-greedy (post-hoc pruning of the decision tree) Stragegy 2 Ensemble multiple Decision Trees together集成多个决策树：森林！！ Step 1 Bagging split different data subset: randomly sampled subsets (bootstrap) ensemble prediction: majority voting for classification output averaging for regression effect: Decision boundaries become blurry: more realistic boundary Out-of-Bag Error Estimation Process: Fit decision tree DT_i on 𝒟_i , Test its performance on the out-of-bag set 𝒟_oob Repeat for each tree in the bag Idea:- This OOB error approximates the test error. Similar idea to cross-validation, but for every DT separately Re-gaining Interpretability Bagging trees improves accuracy over a single tree through increasing diversity in the decisions 意思就是随机性增加了，所以增加了精度 But we loose interpretability: Each tree has different decision rules Interpreting many decision rules in parallel is difficult Strategy: Measure the increase in node purity across trees ![[7010e107b505f8fef371cbe4e3f6219.png]] 就是看每个感兴趣点（橘色点）的node purity，然后求平均 if bagging trees are still highly correlated with each other ![[Pasted image 20240226091312.png]] "><meta name="og:description" content="Decision TreesWe categorize data through chain of simple decisions.a binary tree, which is a graph data structure in which each node has at most two childrenTerminologyinput features: data to be classifiedroot node: starts the decision processdecision nodes: Each decision node (split node) implements a test function with discrete outcomesThe test function of each decision node splits the input space into regionsleaf nodes:A leaf node indicates the end of a sequence of decisionsA single (output) class is associated to each leaf nodeAlgorithm to make predictions![[66c68d76a596659140c2d38d0326bbb.png]]Increasing node purityevery decision should increase the node purityQuantifying Node PurityMeasuring Node Purity Low Entropy: more pure;Higher Entropy: less pure»H(p) how flat Gini Index: higher gini index ,less pure![[f365b77a8cb7cc4a22d13ad62d2231e.png]]![[c4449526b1dcb9a2458ef1c41d9fc19.png]]0.25* 0.75 4 times0.45* 0.55 2 times + 0.05 * 0.95 2 times Regression Trees leaf nodes return the averagesample variance as node purity,high variance before splitting, low variacne after splitting Classification versus Regression Trees How to find the thresholde t? Try different thresholdsoptimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]] Binary Splitting we test as many thresholds as we have points![[ab2533bf07de0f47d6c10fc874cc20f.png]] Growing a Decision tree with Recursive Binary Splitting: perform binary splitting recursively for each decision node to maximize node purity We end uo with one sample per node = a 1-NN regressor/classifier Overfitting of DTs Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Regularization of DTs Restricting Depth Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Tree Pruning Slow Grows（限制生长，greedy） Strategy: Grow first the tree slowly with additional checks during growing• Simple checks: maximum depth,max number of leaf nodes.• Add decision node only if increase in node purity is above a certain threshold Fast Cutters(先长再切, non-greedy) Strategy: Grow the full tree and then remove not important nodes later.• Cost complexity pruning: we identify the weakest link in the tree to remove Cost Complexity Pruning Remove leafs to reduce the complexity, while maintaining a low training error, i.e., cost. The hyperparameter 𝛼 balances both objectives. Clamada(T) = C(T) + lamada T 就是cost(training error)+complexity(number leafs) higher alpha: produces shallower tree with less nodes 如何利用 cost complexity pruning来构建树 ![[638c2052311e5c539c5d454e45e1a73.png]] Advantages and Disadvantages of Trees Advantages:● Inexpensive to construct● Extremely fast at classifying unknown records● Easy to interpret for small-sized trees● Robust to noise (especially when methods to avoid overfitting are employed)● Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)Disadvantages: ● Space of possible decision trees is exponentially large. Greedy approaches are often unable to find the best tree.● Does not take into account interactions between attributes● Each decision boundary involves only a single attribute ● too deep ,more sensitive to noise Bagging, Forest,Boosting Bagging是在干啥： we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess low bias each tree fiting vey well high variance &gt; difference between different trees higher Bias - Variance Trade-off: Finding a balance between fitting and overfitting for tree-like classifiiers Strategy 1: Reduce DT complexity Greedy (restrict DT complexity) Non-greedy (post-hoc pruning of the decision tree) Stragegy 2 Ensemble multiple Decision Trees together集成多个决策树：森林！！ Step 1 Bagging split different data subset: randomly sampled subsets (bootstrap) ensemble prediction: majority voting for classification output averaging for regression effect: Decision boundaries become blurry: more realistic boundary Out-of-Bag Error Estimation Process: Fit decision tree DT_i on 𝒟_i , Test its performance on the out-of-bag set 𝒟_oob Repeat for each tree in the bag Idea:- This OOB error approximates the test error. Similar idea to cross-validation, but for every DT separately Re-gaining Interpretability Bagging trees improves accuracy over a single tree through increasing diversity in the decisions 意思就是随机性增加了，所以增加了精度 But we loose interpretability: Each tree has different decision rules Interpreting many decision rules in parallel is difficult Strategy: Measure the increase in node purity across trees ![[7010e107b505f8fef371cbe4e3f6219.png]] 就是看每个感兴趣点（橘色点）的node purity，然后求平均 if bagging trees are still highly correlated with each other ![[Pasted image 20240226091312.png]] "><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-12"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜7.Decision Tree to Random Forests</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/12 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 5217 字，约 15 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h1 id="decision-trees">Decision Trees</h1><p>We categorize data through <mark class="hltr-orange">chain of simple decisions.</mark> <mark class="hltr-orange">a binary tree,</mark> which is a graph data structure in which each node has at most two children</p><h2 id="terminology">Terminology</h2><p>input features: data to be classified root node: starts the decision process decision nodes: Each decision node (split node) implements a test function with discrete outcomes The test function of each decision node splits the input space into regions leaf nodes: A leaf node indicates the end of a sequence of decisions A single (output) class is associated to each leaf node</p><h2 id="algorithm-to-make-predictions">Algorithm to make predictions</h2><p>![[66c68d76a596659140c2d38d0326bbb.png]]</p><h2 id="increasing-node-purity">Increasing node purity</h2><p>every decision should increase the node purity</p><h3 id="quantifying-node-purity">Quantifying Node Purity</h3><p>Measuring Node Purity</p><ul><li>Low Entropy: more pure;Higher Entropy: less pure»H(p) how flat</li><li>Gini Index: higher gini index ,less pure ![[f365b77a8cb7cc4a22d13ad62d2231e.png]] ![[c4449526b1dcb9a2458ef1c41d9fc19.png]] 0.25* 0.75 4 times 0.45* 0.55 2 times + 0.05 * 0.95 2 times<h2 id="regression-trees">Regression Trees</h2><p>leaf nodes return the average sample variance as node purity,high variance before splitting, low variacne after splitting</p><h2 id="classification-versus-regression-trees">Classification versus Regression Trees</h2><h2 id="how-to-find-the-thresholde-t">How to find the thresholde t?</h2><p>Try different thresholds optimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]]</p><h2 id="binary-splitting">Binary Splitting</h2><p>we test as many thresholds as we have points ![[ab2533bf07de0f47d6c10fc874cc20f.png]]</p><h2 id="growing-a-decision-tree-with-recursive-binary-splitting">Growing a Decision tree with Recursive Binary Splitting:</h2></li><li>perform binary splitting</li><li>recursively for each decision node</li><li>to maximize node purity</li><li>We end uo with one sample per node = a 1-NN regressor/classifier<h3 id="overfitting-of-dts">Overfitting of DTs</h3></li><li>Tree fits training data perfectly (zero variance)</li><li>Tree extremely large, and uninterpretable<h3 id="regularization-of-dts">Regularization of DTs</h3><h4 id="restricting-depth">Restricting Depth</h4></li><li>Tree fits training data perfectly (zero variance)</li><li>Tree extremely large, and uninterpretable<h4 id="tree-pruning">Tree Pruning</h4><h5 id="slow-grows限制生长greedy">Slow Grows（限制生长，greedy）</h5><p><mark class="hltr-blue">Strategy</mark>: Grow first the tree slowly with additional checks during growing • Simple checks: <mark class="hltr-blue">maximum depth</mark>,max number of <mark class="hltr-blue">leaf </mark>nodes. • Add decision node only if<mark class="hltr-blue"> increase</mark> <mark class="hltr-blue">in node purity</mark> is above a certain threshold</p><h5 id="fast-cutters先长再切-non-greedy">Fast Cutters(先长再切, non-greedy)</h5><p><mark class="hltr-blue">Strategy</mark>: Grow the full tree and then remove not important nodes later. <mark class="hltr-blue">• Cost complexity pruning</mark>: we identify the weakest link in the tree to remove</p><h5 id="cost-complexity-pruning">Cost Complexity Pruning</h5></li><li><mark class="hltr-blue">Remove leafs to reduce the complexity,</mark> while maintaining a low training error, i.e., cost. The hyperparameter 𝛼 balances both objectives.</li><li><table><tbody><tr><td>Clamada(T) = C(T) + lamada</td><td>T</td><td>就是cost(training error)+complexity(number leafs)</td></tr></tbody></table></li><li>higher alpha: produces shallower tree with less nodes<h6 id="如何利用-cost-complexity-pruning来构建树">如何利用 cost complexity pruning来构建树</h6><p>![[638c2052311e5c539c5d454e45e1a73.png]]</p><h2 id="advantages-and-disadvantages-of-trees">Advantages and Disadvantages of Trees</h2><p>Advantages: ●<mark class="hltr-purple"> Inexpensive</mark> to construct ● Extremely<mark class="hltr-purple"> fast </mark>at classifying unknown records ● Easy to interpret <mark class="hltr-purple">for small-sized trees</mark> ● <mark class="hltr-purple">Robust to noise</mark> (especially when methods to avoid overfitting are employed) ● Can easily handle <mark class="hltr-purple">redundant or irrelevant attributes</mark> (unless the attributes are interacting) Disadvantages: ● Space of possible decision trees is <mark class="hltr-purple">exponentially large</mark>. Greedy approaches are often unable to find the <mark class="hltr-purple">best tree.</mark> ● Does not take into account <mark class="hltr-purple">interaction</mark>s between attributes ● Each <mark class="hltr-purple">decision boundary involves only a single attribute </mark> ●<mark class="hltr-purple"> too deep ,more sensitive to noise</mark></p><h1 id="bagging-forestboosting">Bagging, Forest,Boosting</h1></li><li><p>Bagging是在干啥： we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess</p></li><li>low bias each tree fiting vey well</li><li>high variance &gt; difference between different trees higher<h2 id="bias---variance-trade-off-finding-a-balance-between-fitting-and-overfitting">Bias - Variance Trade-off: Finding a balance between fitting and overfitting</h2><p>for tree-like classifiiers</p><h3 id="strategy-1-reduce-dt-complexity">Strategy 1: Reduce DT complexity</h3></li><li>Greedy (restrict DT complexity)</li><li>Non-greedy (post-hoc pruning of the decision tree)<h3 id="stragegy-2-ensemble-multiple-decision-trees-together集成多个决策树森林">Stragegy 2 Ensemble multiple Decision Trees together集成多个决策树：森林！！</h3><h4 id="step-1-bagging">Step 1 Bagging</h4></li><li>split different data subset: randomly sampled subsets (bootstrap)</li><li>ensemble prediction: majority voting for classification output averaging for regression</li><li>effect: Decision boundaries become blurry: more realistic boundary<h5 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h5><p>Process:</p><ol><li>Fit decision tree DT_i on 𝒟_i ,</li><li>Test its performance on the out-of-bag set 𝒟_oob</li><li>Repeat for each tree in the bag Idea: -<mark class="hltr-blue"> This OOB error approximates the test error. </mark></li></ol></li><li>Similar idea to cross-validation, but for every DT separately<h5 id="re-gaining-interpretability">Re-gaining Interpretability</h5></li><li>Bagging trees <mark class="hltr-blue">improves accuracy</mark> over a single tree through increasing <mark class="hltr-blue">diversity in the decisions</mark> 意思就是随机性增加了，所以增加了精度</li><li>But we <mark class="hltr-blue">loose interpretability</mark>: Each tree has different decision rules Interpreting many decision rules in parallel is difficult</li><li>Strategy: Measure the <mark class="hltr-blue">increase in node purity across trees </mark></li><li>![[7010e107b505f8fef371cbe4e3f6219.png]] 就是看每个感兴趣点（橘色点）的node purity，然后求平均<h5 id="if-bagging-trees-are-still-highly-correlated-with-each-other">if bagging trees are still highly correlated with each other</h5><p>![[Pasted image 20240226091312.png]]</p></li></ul><h4 id="step-2-random-forest">Step 2 Random forest</h4><ul><li>Idea: increase diversity by removing features from individual trees (removing the root node , to increase diversity)</li><li>Implementation:we take a random sample of predictors for each tree (usually 根号下 𝐹)(for instance, 4 features out of 15)</li><li>for each decision tree : <mark class="hltr-blue">随机trained with a dataset subset和 subset of features</mark></li><li>Bags and Random Forest:<ul><li>each tree is trained on a different <mark class="hltr-blue">“bootstrapped</mark>” set of train data</li><li>trees are trained separately from each other<mark class="hltr-blue"> in parallel </mark></li></ul></li><li>Random Forests: additionally each tree is trained with a different <mark class="hltr-blue">subset of features</mark><h2 id="boosting">Boosting</h2><h3 id="random-forest-versus-boosting-philosophy">Random Forest versus Boosting Philosophy</h3></li><li>Random Forest: accuracy through classifier diversity all trees grown in parallel</li><li>Boosting: many small “weak” classifiers trees grown sequentially<h3 id="principles">Principles</h3></li><li>Train one classifier after another</li><li>Reweight the data so that wrongly classified data is more important (grow it one by one, there is order of importance)<h3 id="adaboost">AdaBoost</h3><h4 id="three-core-ideas">Three Core Ideas:</h4></li><li>Combines many “weak learners”</li><li>some stumps are weighted higher than others</li><li>each stump takes the previous stumps into account ![[6122295845e0fc7d25e6f98889b33de.png]] ![[ffc41bc78dda2f930d891bd74eb8cdd.png]] ![[d7ec32aa303a927332cc68cfd93669e.png]]</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/" target="_blank">https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727735013', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
