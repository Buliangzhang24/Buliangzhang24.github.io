<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜ ExamNotes2 &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/25/MachineLearning-Exam/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜ ExamNotes2"><meta name="keywords" content="Exam"><meta name="og:keywords" content="Exam"><meta name="description" content=" F»T Adding more variables to a model will most likely decrease the training errors 增加P,会减少training errors T K=n应该是最少bias ? T»F Adding more data in the training set will likely decrease the total sum of squares (TSS) 不变 增加n，TSS不变 T »F The likelihood function gives the probability of the model parameters given the data.似然函数是在给定模型参数的情况下观察到数据的概率，而不是给定数据的情况下模型参数的概率 F？LDA 不止用于普通二项分布 ? F 加data of training set 对TSS，没有变化 T?For a problem with p features, any separating hyperplane has p+1 parameters. F?Weight decay regularizes neural networks by minimizing the number of errors Weight decay是通过惩罚模型的权重大小来正则化神经网络 T For a model of the form y= β0+ β1x1 + β2x2 it holds that β-&gt; 2β implies y-&gt;2y. In other words,doubling the parameter vector will double the explained variable The linear SVC (support vector classifier) just has a single user-defined parameter. T The curse of dimensionality states that the more features, the more computation is required. F而是说他精度会减少 Bayesian classification assigns the label of the class with the highest prior 这个应该是后验probability. F F We have 2 models with the same number of variables, but with a different training error. The model with the highest training error is more likely to have the smallest test error of the 2."><meta name="og:description" content=" F»T Adding more variables to a model will most likely decrease the training errors 增加P,会减少training errors T K=n应该是最少bias ? T»F Adding more data in the training set will likely decrease the total sum of squares (TSS) 不变 增加n，TSS不变 T »F The likelihood function gives the probability of the model parameters given the data.似然函数是在给定模型参数的情况下观察到数据的概率，而不是给定数据的情况下模型参数的概率 F？LDA 不止用于普通二项分布 ? F 加data of training set 对TSS，没有变化 T?For a problem with p features, any separating hyperplane has p+1 parameters. F?Weight decay regularizes neural networks by minimizing the number of errors Weight decay是通过惩罚模型的权重大小来正则化神经网络 T For a model of the form y= β0+ β1x1 + β2x2 it holds that β-&gt; 2β implies y-&gt;2y. In other words,doubling the parameter vector will double the explained variable The linear SVC (support vector classifier) just has a single user-defined parameter. T The curse of dimensionality states that the more features, the more computation is required. F而是说他精度会减少 Bayesian classification assigns the label of the class with the highest prior 这个应该是后验probability. F F We have 2 models with the same number of variables, but with a different training error. The model with the highest training error is more likely to have the smallest test error of the 2."><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/25/MachineLearning-Exam/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-25"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜ ExamNotes2</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/25 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 4842 字，约 14 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><ol><li>F»T Adding more variables to a model will most likely decrease the training errors 增加P,会减少training errors</li><li>T K=n应该是最少bias</li><li>? T»F Adding more data in the training set will likely decrease the total sum of squares (TSS) 不变 增加n，TSS不变</li><li>T »F The likelihood function gives the probability of the model parameters given the data.似然函数是在<mark class="hltr-green">给定模型参数</mark>的情况下观察到<mark class="hltr-green">数据的概率</mark>，而不是给定数据的情况下模型参数的概率</li><li>F？LDA 不止用于普通二项分布</li><li>? F 加data of training set 对TSS，没有变化</li><li>T?For a problem with p features, any separating hyperplane has p+1 parameters.</li><li>F?Weight decay regularizes neural networks by minimizing the number of errors Weight decay是通过惩罚模型的权重大小来正则化神经网络</li><li><mark class="hltr-green">T For a model of the form y= β0+ β1x1 + β2x2 it holds that β-&gt; 2β implies y-&gt;2y. In other words,doubling the parameter vector will double the explained variable </mark></li><li>The linear SVC (support vector classifier) just has a single user-defined parameter. T</li><li>The curse of dimensionality states that the more features, the more computation is required. F而是说他精度会减少</li><li>Bayesian classification assigns the label of the class with the highest prior 这个应该是后验probability. F</li><li><p>F We have 2 models with the same number of variables, but with a different training error. The model with the highest training error is more likely to have the smallest test error of the 2.</p><p>T We have 2 models with the same number of variables, but with a different training error. The model with the highest training error is more likely to have the largest test error of the 2. <mark class="hltr-red">就是有同样的数量的p， 有最大的training error的就有最大的test error</mark></p></li><li>F We have 2 models with the same training error, but with a different number of variables. The model with the smallest number of variables is more likely to have the largest test error of the 2.因此，两个具有相同训练误差的模型中，较少变量的模型更可能在测试集上表现更好，因为它更可能避免过度拟合，具有更好的泛化能力。<mark class="hltr-red">就是有相同的training error，p的数量越小，test errror 越小。</mark></li><li>The Akaike Information Criterion is suitable for model selection, in case no test data is available. T</li><li>In classification, the <mark class="hltr-green">Bayes error rate</mark> is the lowest error rate one can theoretically obtain. T</li><li>T》F就算K无限大，这个分类误差也不一定always exceed 50%</li><li>T》F On a linear dataset, LDA will have a better training MSE than QDA.</li><li>In a classification problem (assuming that cases with identical feature values have identical labels), it is always possible for a classification tree to reduce training error to zero.对于分类树来说，误差降到0是always的</li><li>T The separating hyperplane formula is the same for the SVM and for the perceptron.</li><li>T The perceptron is a linear classifier.</li><li>Bias relates to underfitting and variance to overfitting</li><li><p>A. When the p-value for the F-statistic is &lt;0.05不是，是F大于1是推翻H0 (C.) When the p-value for the t-statistic is &lt; 0.05</p></li><li><p>Regression Linear cannot work with a <mark class="hltr-green">qualitative response</mark></p></li><li>When a sample has a completely different response value compared to the other samples, it has A high studentized residual当一个样本的Y与其他样本完全不同时，可能会导致高的学生化残差。</li><li>When a sample has a completely different predictor value compared to the other samples, it has A high leverage</li><li><p>LDA 和QDA的区别： same covariance 和different covariance</p></li><li><p>What is true for a kNN classifier:(C.) Training error is 0 when k=1 因为当 k = 1 时，模型会完全记住训练数据，因此每个训练样本都将与其最近的邻居匹配，从而产生 0 的训练误差</p></li><li>A decision tree(C.) is sensitive to noise</li><li><mark class="hltr-red"> Decision tree 不基于不放回抽样来选择点</mark>，但是RF和bagging 都是bootstrap</li></ol><p>Which TWO things are true about Ridge regression (RR) on a noise dataset? A. RR will likely increase the test MSE (B.) RR will likely decrease the test MSE Ridge会减少test上的错误 (C.) RR will likely increase the training MSE D. RR will likely decrease the training MSE</p><p>When you want to simplify a linear regression model using fewer predictors, one should NOT use A. The Lasso适合 (B.) Ridge regression不适合少量数据 C. Forward selection D. Backward selection</p><p>Consider a dataset composed of five samples. In the figure below, the visual distance between the points represents the actual distance. Which of the following value of K will have least leave-one-out cross validation accuracy? (A.) 1NN B. 3NN C. 4NN D. All have same leave one out error 对于1NN，每个样本的最近都是它自己，所以没有样本都会被分类错</p><p>Consider a dataset composed of five samples. In the figure below, the visual distance between the points represents the actual distance. Which of the following is leave-one-out cross-validation accuracy for 3-NN (3 nearest neighbor)? A. 0.0 B. 0.4 (C.) 0.8 D. 1.0 五个样本四个分对了</p><p><mark class="hltr-red">31. logistic 的fuction是e，但是decision boundary是linear 同时两个classes的概率是相等的。</mark></p><ol><li>What is the meaning of “margin” in the maximal margin classifier? The minimal distance from the observations to a given separating hyperplane</li></ol><p>The Lasso <mark class="hltr-red">(A.) Lasso产生稀疏模型。</mark> Lasso回归倾向于产生稀疏模型，即只有少数几个预测变量的系数不为零，而其他的系数都为零 B. Lasso将不会包括最终模型中的所有预测变量。有些就直接是0 了 C.随着lambda值的增加，Lasso回归会施加更强的正则化惩罚，导致许多参数的值变得更小甚至为零。但并不是所有参数的值都会降低，因为对于某些重要的预测变量，它们的系数可能会保持在一个较大的非零值。 D. Lasso只能应用于p » n的情况。 这个说法也是错误的。虽然Lasso对于高维数据（即预测变量数量远远大于样本数量）特别有用，但并不是只有在这种情况下才能应用。Lasso可以应用于任何数量的预测变量，但它在处理高维数据时尤为有效，因为它可以帮助排除不相关的预测变量，从而提高模型的泛化能力</p><p>The Ridge regression <mark class="hltr-red">(B.) 岭回归将在最终模型中包含所有的预测变量。</mark> 这个说法是正确的。与Lasso回归不同，岭回归通过L2正则化来惩罚系数的平方和，但不将系数减少到零 岭回归在处理高维数据时尤为有效，但它并不是只能应用于这种情况下。 随着lambda值的增加，岭回归会施加更强的正则化惩罚，导致系数的大小减小，但并不是所有参数的值都会降低。某些重要的预测变量可能会保持较大的非零值。</p><p>n很大，p很小的时候，flexible n很小，p很大，inflexible</p><ol><li>K= n 是最小的bias, 因为每次验证的数据更多，所以K-1，bias会上升</li><li>Add training data 的数量，TSS会上升或者不变</li><li>P 相同， training error 上升， test error 上升</li><li>training error相同， P上升， test error上升</li><li>twice parameter 就是2P</li><li>logistic regression: log(p/1-p) = b0+b1*X1 + b2 *X2</li><li>在1D的时候，LDA与nearest mean classifier就是一样的</li><li>cross conditional 就是f(x),记得要平方</li><li>减枝的时候会， variance 下降，因为variance is sensitive to noise</li><li>Cross Validation的问题是： bias， 因为没有用全部数据，overestimate CV error了；X不是独立的，underestimate CV error.</li><li>怎么看一个tree 好不好？ the prediction at every tree÷ tree的数量</li><li>M step： 就是最大化： mixture coeffient, density parameter are recalculated</li></ol><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/25/MachineLearning-Exam/" target="_blank">https://buliangzhang24.github.io/2024/02/25/MachineLearning-Exam/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727735917', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
