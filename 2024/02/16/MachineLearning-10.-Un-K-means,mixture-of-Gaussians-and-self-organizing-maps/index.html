<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>MachineLearning｜10. Un-K-means,mixture-of-Gaussians and self-organizing maps &mdash; Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/2024/02/16/MachineLearning-10.-Un-K-means,mixture-of-Gaussians-and-self-organizing-maps/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta property="og:title" content="MachineLearning｜10. Un-K-means,mixture-of-Gaussians and self-organizing maps"><meta name="keywords" content="Un-K-means,mixture-of-Gaussians and self-organizing maps"><meta name="og:keywords" content="Un-K-means,mixture-of-Gaussians and self-organizing maps"><meta name="description" content="Clustering methods–Hierarchical dissimilarity matrix(cluster)dengrogram(cut)clustering bottom-up or agglomerative(as opposed to top-down or divisive) Disadavantages: once clustered, objects stay clustered, hard clustering: objects are assigned to a single cluster Clustering methods–K-means Data, Criterion, Clusters K(cluster)ClusteringFor every selected number of clusters K, choose optimal clusteringWhat is optimal? K-means definitions 都是并集，clusters do not overlap Optimal clustering:squared distances between all pairs in each cluster or, equivalently, squared distances to cluster means, are minimal ![[6ceedf778cb91a68b5119210d4113fa.png]] problem: need known W(C_k), but there are possibilities (!) iterative optimization![[429f8e6de6ce3059449c8ecdeab34cc.png]] K-means algorithm 就是k means是表示xi只属于一个簇，所以我定下有k个簇，我随机把数据点放到这些簇里，然后求每个簇的平均值（和方差），找离这个簇最近的点，迭代一遍所有点，让分类的结果最后不变 Choose number of clusters K and randomly assign each sample to a cluster Iterate until nothing changes:(a) for each cluster, calculate the centroid (mean)(b) re-assign each sample to the cluster whose mean is closest (in the Euclidean sense) Guaranteed to only decrease the criterion (why?):这个过程可以保证每次迭代都至少不会增加目标函数的值。exercise 3![[8138d00283e7a1b64c3cdf3e41a51c9.png]] Choice of K Rule of thumb: look for “drop” in criterion K-means problems Clusters can lose all samples Why 初始质心选择不当：如果初始质心选取不当，有些质心可能会被分配到一个不包含任何样本的区域。这会导致该簇失去所有的样本。 非凸形状的簇：如果数据集包含非凸形状的簇，例如环状或月牙形状的簇，K-means 算法可能会将其分割成多个较小的簇，导致某个簇失去所有的样本。 数据量不均衡：如果某个簇中的样本数量太少，而其他簇中的样本数量较多，则某个簇可能会失去所有的样本，因为K-means 算法的更新过程是基于样本的平均值。 Solutions remove cluster and continue with K – 1 means alternatively, split largest cluster into two or add a random mean to continue with K means Clustering result depends on initialization –Algorithm can get stuck in local minima Solution: start from (many) different random initialisations keep the best clustering(lowest sumW(C_k)) Limitations K-means model not necessarily optimalEqual cluster models not necessarily optimalHard clustering not necessarily optimalexercise The K-means model What cluster model actually underlies K-means?● spherical, uniform● implicit in criterionChoosing an explicit model can help to:● understand the result● quantify the model fit● try alternative models● make assumptions explicit![[db0c8d1c3fe1bf749e4b0a324843c71.png]] Mixture Models Model the probability distribution of the data gives model for overall data distribution "soft" clustering: captures uncertainty in assignments parameters can be found using maximum likelihood Distribution-based clustering Each cluster is described by a probability density function Total dataset described by a mixture of density functions Clustering means maximizing the mixture fit, cluster assignment is based on posterior probabilities 就是我要知道x这个点属于每个簇的概率，把x分配给概率最高的簇，但是给定k簇的情况下求x(就是已经知道一个点属于k，求这个点是x点的概率)更好求（高斯的分布的密度函数求,就是用平均值和平方和求），所以先求这个。![[db989798961eae8109821c2b4e1f593.png]]![[61eaaf63f70c50e24e149679f2929a3.png]] Fitting mixture models ![[da96a8555fd1a079d45d6c6a71cb331.png]]对于混合高斯模型，我们有以下步骤： 初始化参数：我们首先随机初始化每个高斯分布的均值、方差和权重。 计算每个样本属于每个高斯分布的概率：对于每个样本xi​，我们计算它属于每个高斯分布 k 的概率 P(xi​∣k)。（Note that mixture models also work for other component densities!） 更新参数：对于每个高斯分布 k，我们根据每个样本属于该分布的概率来更新均值、方差和权重。这个更新过程使用最大似然估计的方法。(说白了就是选最大的概率) 重复步骤2和3：我们重复步骤2和3，直到参数不再改变或达到最大迭代次数。 输出参数：最终，我们输出参数的估计值，这些参数可以用来描述数据的分布情况。 Mixture of Gaussians ![[9ee78120286e692893be54730373505.png]] Latent variable Problem: need to simultaneously estimate two interdependent things… no closed form solution! cluster membership of each object solutions ![[b538f77c870e99997ab8161352e661a.png]]![[395a1aec9892c7805ce71a1f67a50dc.png]] The EM algorithm Expectation-Maximization algorithm: general class of algorithms for this type of problem repeatedly: recalculate cluster membership of each sample (E) recalculate density parameters of each cluster (M)EM算法是一种求解含有隐变量的概率模型的参数估计方法。它的基本思想是：假设观测数据的生成过程包含两个步骤，即隐变量的生成和观测数据的生成。在E步，算法利用当前参数估计隐变量的后验概率，即估计隐变量的分布；在M步，算法利用E步的结果估计模型参数。通过反复迭代E步和M步，最终达到收敛。EM算法的目标是最大化似然函数，使得观测数据的生成概率最大，从而得到最优的参数估计。 The EM algorithm for MoGs EM 就是隐变量的概率我知道（但是会随着高斯分布的均值和协方差变化而变化），求浅变量的概率就是求responsibility，（把这个代入这个高斯求浅变量的式子里面就是Estep），用最大似然法找到最大的（把式子求导找极值）就是Mstep![[de47c321f0fb728d4503ff496b0304f.png]]![[ae1eaf43d0d9f2f442c7a5750b7b4f4.png]]EM 就是隐变量的概率我知道（但是会随着高斯分布的均值和协方差变化而变化），求浅变量的概率就是求responsibility，用最大似然法找到最大的（在M步中，我们通过最大化似然函数来更新模型参数估计，例如我们可以通过对似然函数求导，令导数为0，找到似然函数的最大值点，这个过程通常称为最大似然估计（MLE）） "><meta name="og:description" content="Clustering methods–Hierarchical dissimilarity matrix(cluster)dengrogram(cut)clustering bottom-up or agglomerative(as opposed to top-down or divisive) Disadavantages: once clustered, objects stay clustered, hard clustering: objects are assigned to a single cluster Clustering methods–K-means Data, Criterion, Clusters K(cluster)ClusteringFor every selected number of clusters K, choose optimal clusteringWhat is optimal? K-means definitions 都是并集，clusters do not overlap Optimal clustering:squared distances between all pairs in each cluster or, equivalently, squared distances to cluster means, are minimal ![[6ceedf778cb91a68b5119210d4113fa.png]] problem: need known W(C_k), but there are possibilities (!) iterative optimization![[429f8e6de6ce3059449c8ecdeab34cc.png]] K-means algorithm 就是k means是表示xi只属于一个簇，所以我定下有k个簇，我随机把数据点放到这些簇里，然后求每个簇的平均值（和方差），找离这个簇最近的点，迭代一遍所有点，让分类的结果最后不变 Choose number of clusters K and randomly assign each sample to a cluster Iterate until nothing changes:(a) for each cluster, calculate the centroid (mean)(b) re-assign each sample to the cluster whose mean is closest (in the Euclidean sense) Guaranteed to only decrease the criterion (why?):这个过程可以保证每次迭代都至少不会增加目标函数的值。exercise 3![[8138d00283e7a1b64c3cdf3e41a51c9.png]] Choice of K Rule of thumb: look for “drop” in criterion K-means problems Clusters can lose all samples Why 初始质心选择不当：如果初始质心选取不当，有些质心可能会被分配到一个不包含任何样本的区域。这会导致该簇失去所有的样本。 非凸形状的簇：如果数据集包含非凸形状的簇，例如环状或月牙形状的簇，K-means 算法可能会将其分割成多个较小的簇，导致某个簇失去所有的样本。 数据量不均衡：如果某个簇中的样本数量太少，而其他簇中的样本数量较多，则某个簇可能会失去所有的样本，因为K-means 算法的更新过程是基于样本的平均值。 Solutions remove cluster and continue with K – 1 means alternatively, split largest cluster into two or add a random mean to continue with K means Clustering result depends on initialization –Algorithm can get stuck in local minima Solution: start from (many) different random initialisations keep the best clustering(lowest sumW(C_k)) Limitations K-means model not necessarily optimalEqual cluster models not necessarily optimalHard clustering not necessarily optimalexercise The K-means model What cluster model actually underlies K-means?● spherical, uniform● implicit in criterionChoosing an explicit model can help to:● understand the result● quantify the model fit● try alternative models● make assumptions explicit![[db0c8d1c3fe1bf749e4b0a324843c71.png]] Mixture Models Model the probability distribution of the data gives model for overall data distribution "soft" clustering: captures uncertainty in assignments parameters can be found using maximum likelihood Distribution-based clustering Each cluster is described by a probability density function Total dataset described by a mixture of density functions Clustering means maximizing the mixture fit, cluster assignment is based on posterior probabilities 就是我要知道x这个点属于每个簇的概率，把x分配给概率最高的簇，但是给定k簇的情况下求x(就是已经知道一个点属于k，求这个点是x点的概率)更好求（高斯的分布的密度函数求,就是用平均值和平方和求），所以先求这个。![[db989798961eae8109821c2b4e1f593.png]]![[61eaaf63f70c50e24e149679f2929a3.png]] Fitting mixture models ![[da96a8555fd1a079d45d6c6a71cb331.png]]对于混合高斯模型，我们有以下步骤： 初始化参数：我们首先随机初始化每个高斯分布的均值、方差和权重。 计算每个样本属于每个高斯分布的概率：对于每个样本xi​，我们计算它属于每个高斯分布 k 的概率 P(xi​∣k)。（Note that mixture models also work for other component densities!） 更新参数：对于每个高斯分布 k，我们根据每个样本属于该分布的概率来更新均值、方差和权重。这个更新过程使用最大似然估计的方法。(说白了就是选最大的概率) 重复步骤2和3：我们重复步骤2和3，直到参数不再改变或达到最大迭代次数。 输出参数：最终，我们输出参数的估计值，这些参数可以用来描述数据的分布情况。 Mixture of Gaussians ![[9ee78120286e692893be54730373505.png]] Latent variable Problem: need to simultaneously estimate two interdependent things… no closed form solution! cluster membership of each object solutions ![[b538f77c870e99997ab8161352e661a.png]]![[395a1aec9892c7805ce71a1f67a50dc.png]] The EM algorithm Expectation-Maximization algorithm: general class of algorithms for this type of problem repeatedly: recalculate cluster membership of each sample (E) recalculate density parameters of each cluster (M)EM算法是一种求解含有隐变量的概率模型的参数估计方法。它的基本思想是：假设观测数据的生成过程包含两个步骤，即隐变量的生成和观测数据的生成。在E步，算法利用当前参数估计隐变量的后验概率，即估计隐变量的分布；在M步，算法利用E步的结果估计模型参数。通过反复迭代E步和M步，最终达到收敛。EM算法的目标是最大化似然函数，使得观测数据的生成概率最大，从而得到最优的参数估计。 The EM algorithm for MoGs EM 就是隐变量的概率我知道（但是会随着高斯分布的均值和协方差变化而变化），求浅变量的概率就是求responsibility，（把这个代入这个高斯求浅变量的式子里面就是Estep），用最大似然法找到最大的（把式子求导找极值）就是Mstep![[de47c321f0fb728d4503ff496b0304f.png]]![[ae1eaf43d0d9f2f442c7a5750b7b4f4.png]]EM 就是隐变量的概率我知道（但是会随着高斯分布的均值和协方差变化而变化），求浅变量的概率就是求responsibility，用最大似然法找到最大的（在M步中，我们通过最大化似然函数来更新模型参数估计，例如我们可以通过对似然函数求导，令导数为0，找到似然函数的最大值点，这个过程通常称为最大似然估计（MLE）） "><meta property="og:url" content="https://buliangzhang24.github.io/2024/02/16/MachineLearning-10.-Un-K-means,mixture-of-Gaussians-and-self-organizing-maps/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /><meta property="article:published_time" content="2024-02-16"> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="" data-mz=""><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><section class="collection-head small geopattern" data-pattern-id="MachineLearning"><div class="container"><div class="columns"><div class="column three-fourths"><div class="collection-title"><h1 class="collection-header">MachineLearning｜10. Un-K-means,mixture-of-Gaussians and self-organizing maps</h1><div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/16 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 4490 字，约 13 分钟 </span></div></div></div><div class="column one-fourth mobile-hidden"><div class="collection-title"></div></div></div></div></section><section class="container content"><div class="columns"><div class="column three-fourths" ><article class="article-content markdown-body"><h2 id="clustering-methodshierarchical">Clustering methods–Hierarchical</h2><ul><li>dissimilarity matrix(cluster)dengrogram(cut)clustering</li><li>bottom-up or agglomerative(as opposed to top-down or divisive)</li><li>Disadavantages: <mark class="hltr-pink">once clustered</mark>, objects stay clustered, <mark class="hltr-pink">hard clustering</mark>: objects are assigned to a single cluster<h1 id="clustering-methodsk-means">Clustering methods–K-means</h1><p>Data, Criterion, Clusters K(cluster)Clustering For every selected number of clusters K, choose optimal clustering What is optimal?</p><h2 id="k-means-definitions">K-means definitions</h2></li><li>都是并集，clusters do not overlap</li><li>Optimal clustering:squared distances between <mark class="hltr-purple">all pairs in each cluster</mark> or, <mark class="hltr-pink">equivalently, </mark>squared distances to cluster means, are minimal</li><li>![[6ceedf778cb91a68b5119210d4113fa.png]]</li><li>problem: <mark class="hltr-pink">need known W(C_k), but there are possibilities (!)</mark></li><li>iterative optimization![[429f8e6de6ce3059449c8ecdeab34cc.png]]<h2 id="k-means-algorithm">K-means algorithm</h2><p>就是k means是表示xi只属于一个簇，所以我定下有k个簇，我随机把数据点放到这些簇里，然后求每个簇的平均值（和方差），找离这个簇最近的点，迭代一遍所有点，让分类的结果最后不变</p><ol><li>Choose number of clusters K and randomly assign each sample to a cluster</li><li>Iterate until nothing changes: (a) for each cluster, calculate the centroid (mean) (b) re-assign each sample to the cluster whose mean is closest (in the Euclidean sense)</li></ol></li><li>Guaranteed to only decrease the criterion (why?):这个过程可以保证每次迭代都至少不会增加目标函数的值。 <strong>exercise 3</strong> ![[8138d00283e7a1b64c3cdf3e41a51c9.png]]<h2 id="choice-of-k">Choice of K</h2><p>Rule of thumb: look for “drop” in criterion</p><h2 id="k-means-problems">K-means problems</h2><h3 id="clusters-can-lose-all-samples">Clusters can lose all samples</h3><h4 id="why">Why</h4><ol><li>初始质心选择不当：如果初始质心选取不当，有些质心可能会被分配到一个不包含任何样本的区域。这会导致该簇失去所有的样本。</li><li>非凸形状的簇：如果数据集包含非凸形状的簇，例如环状或月牙形状的簇，K-means 算法可能会将其分割成多个较小的簇，导致某个簇失去所有的样本。</li><li>数据量不均衡：如果某个簇中的样本数量太少，而其他簇中的样本数量较多，则某个簇可能会失去所有的样本，因为K-means 算法的更新过程是基于样本的平均值。<h4 id="solutions">Solutions</h4></li><li>remove cluster and continue with K – 1 means</li><li>alternatively, split largest cluster into two or add a random mean to continue with K means<h3 id="clustering-result-depends-on-initialization-algorithm-can-get-stuck-in-local-minima">Clustering result depends on initialization –Algorithm can get stuck in local minima</h3><h4 id="solution">Solution:</h4></li><li>start from (many) different random initialisations</li><li>keep the best clustering(lowest sumW(C_k))<h2 id="limitations">Limitations</h2><p>K-means model not necessarily optimal Equal cluster models not necessarily optimal Hard clustering not necessarily optimal <mark class="hltr-red">exercise </mark></p><h2 id="the-k-means-model">The K-means model</h2><p>What cluster model actually underlies K-means? ● spherical, uniform ● implicit in criterion Choosing an explicit model can help to: ● understand the result ● quantify the model fit ● try alternative models ● make assumptions explicit ![[db0c8d1c3fe1bf749e4b0a324843c71.png]]</p><h1 id="mixture-models">Mixture Models</h1><h2 id="model-the-probability-distribution-of-the-data">Model the probability <mark class="hltr-pink">distribution</mark> of the data</h2></li></ol></li><li>gives model for overall data distribution</li><li><mark class="hltr-pink"> "soft" clustering</mark>: captures uncertainty in assignments</li><li>parameters can be found using maximum likelihood<h2 id="distribution-based-clustering">Distribution-based clustering</h2></li><li>Each cluster is described by a probability density function</li><li>Total dataset described by a mixture of density functions</li><li>Clustering means maximizing the mixture fit,</li><li>cluster assignment is based on <mark class="hltr-pink">posterior probabilities </mark> 就是我要知道x这个点属于每个簇的概率，把x分配给概率最高的簇，但是给定k簇的情况下求x(就是已经知道一个点属于k，求这个点是x点的概率)更好求（高斯的分布的密度函数求,就是用平均值和平方和求），所以先求这个。 ![[db989798961eae8109821c2b4e1f593.png]]![[61eaaf63f70c50e24e149679f2929a3.png]]<h2 id="fitting-mixture-models">Fitting mixture models</h2><p>![[da96a8555fd1a079d45d6c6a71cb331.png]] 对于混合高斯模型，我们有以下步骤：</p><ol><li>初始化参数：我们首先<mark class="hltr-pink">随机</mark>初始化每个高斯分布的均值、方差和权重。</li><li>计算<mark class="hltr-pink">每个样本属于每个高斯分布的概率</mark>：对于每个样本xi​，我们计算它属于每个高斯分布 k 的概率 P(xi​∣k)。（Note that mixture models also work for other component densities!）</li><li>更新参数：对于每个高斯分布 k，我们根据每个样本属于该分布的概率来更新均值、方差和权重。这个更新过程使用<mark class="hltr-pink">最大似然估计</mark>的方法。<mark class="hltr-pink">(说白了就是选最大的概率)</mark></li><li>重复步骤2和3：我们重复步骤2和3，直到参数不再改变或达到最大迭代次数。</li><li>输出参数：最终，我们输出参数的估计值，这些参数可以用来描述数据的分布情况。<h2 id="mixture-of-gaussians">Mixture of Gaussians</h2><p>![[9ee78120286e692893be54730373505.png]]</p><h2 id="latent-variable">Latent variable</h2><h3 id="problem">Problem:</h3><p>need to simultaneously estimate two interdependent things… no closed form solution! cluster membership of each object</p><h3 id="solutions-1">solutions</h3><p>![[b538f77c870e99997ab8161352e661a.png]] ![[395a1aec9892c7805ce71a1f67a50dc.png]]</p><h2 id="the-em-algorithm">The EM algorithm</h2><p>Expectation-Maximization algorithm:</p></li></ol></li><li>general class of algorithms for this type of problem</li><li>repeatedly: recalculate cluster membership of each sample (E) recalculate density parameters of each cluster (M) EM算法是一种求解含有隐变量的概率模型的参数估计方法。它的基本思想是：假设观测数据的生成过程包含两个步骤，即隐变量的生成和观测数据的生成。在E步，算法利用<mark class="hltr-pink">当前参数估计隐变量的后验概率</mark>，即估计隐变量的分布；在M步，算法利用E步的结果<mark class="hltr-pink">估计模型参数</mark>。通过反复迭代E步和M步，最终达到收敛。EM算法的目标是最大化似然函数，使得观测数据的生成概率最大，从而得到最优的参数估计。<h3 id="the-em-algorithm-for-mogs">The EM algorithm for MoGs</h3><p>EM 就是隐变量的概率我知道（但是会随着高斯分布的均值和协方差变化而变化），求浅变量的概率就是求responsibility，（把这个代入这个高斯求浅变量的式子里面就是Estep），用最大似然法找到最大的（把式子求导找极值）就是Mstep ![[de47c321f0fb728d4503ff496b0304f.png]] ![[ae1eaf43d0d9f2f442c7a5750b7b4f4.png]] EM 就是隐变量的概率我知道（但是会随着高斯分布的均值和协方差变化而变化），求浅变量的概率就是求responsibility，用最大似然法找到最大的（在M步中，我们通过最大化似然函数来更新模型参数估计，例如我们可以通过对似然函数求导，令导数为0，找到似然函数的最大值点，这个过程通常称为最大似然估计（MLE））</p></li></ul><p>K-means is a special case of a mixture-of-Gaussians <mark class="hltr-orange">如果每一个点，到每一个簇的概率都是相等的，就是Kmeans,但是如果不是相等的，就是mog。</mark> 在K-means算法中，<mark class="hltr-orange">每个簇的协方差矩阵是对角矩阵，并且每个元素相等</mark>。</p><h3 id="limitations-1">Limitations</h3><ul><li>Disadvantages (similar to K-means): ● depends on initial conditions, can get stuck in local minima: same solution局部最优 ● convergence can be slow收敛很慢 ● problems with covariance estimates: if too few samples are members of a cluster, there will not be enough data to base estimate on 样本太少</li><li>Advantages: ● simple to implement简单 ● can use prior knowledge of cluster distribution可以用先验概率</li></ul><div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"><h3>文档信息</h3><ul><li>本文作者：<a href="https://buliangzhang24.github.io" target="_blank">Xinyi He</a></li><li>本文链接：<a href="https://buliangzhang24.github.io/2024/02/16/MachineLearning-10.-Un-K-means,mixture-of-Gaussians-and-self-organizing-maps/" target="_blank">https://buliangzhang24.github.io/2024/02/16/MachineLearning-10.-Un-K-means,mixture-of-Gaussians-and-self-organizing-maps/</a></li><li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li></ul></div></article><div class="share"></div><div class="comment"> <script src="https://giscus.app/client.js" data-repo="Buliangzhang24/Buliangzhang24.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnk5MzEyNzkxNw==" data-category="Announcements" data-category-id="DIC_kwDOBY0E7c4CRtg9" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script></div></div><div class="column one-fourth"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727793652', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3 class="post-directory-title mobile-hidden">Table of Contents</h3><div id="post-directory-module" class="mobile-hidden"><section class="post-directory"><dl></dl></section></div><script src="https://buliangzhang24.github.io/assets/js/jquery.toc.js"></script></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> © 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
