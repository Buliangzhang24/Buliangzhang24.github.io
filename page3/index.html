<!DOCTYPE html><html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /><title>Xinyi He</title><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/primer-css/css/primer.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/collection.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/repo-card.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/sections/repo-list.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/components/boxed-group.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/common.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/globals/responsive.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/css/posts/index.css"><link rel="stylesheet" href="https://buliangzhang24.github.io/assets/vendor/octicons/octicons/octicons.css"><link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"><link rel="canonical" href="https://buliangzhang24.github.io/page3/"><link rel="alternate" type="application/atom+xml" title="Xinyi He" href="https://buliangzhang24.github.io/feed.xml"><link rel="shortcut icon" href="https://buliangzhang24.github.io/favicon.ico"><meta name="keywords" content="Xinyi He, GIS, Remote Sensing, LiDAR"><meta name="description" content="Feel free to reach out!"><meta property="og:url" content="https://buliangzhang24.github.io/page3/"><meta property="og:site_name" content="Xinyi He"><meta property="og:type" content="article"><meta property="og:locale" content="zh_CN" /> <script src="https://buliangzhang24.github.io/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://buliangzhang24.github.io/assets/js/main.js"></script></head><body class="home" data-mz="home"><header class="site-header"><div class="container"><h1><a href="https://buliangzhang24.github.io/" title="Xinyi He"><span class="octicon octicon-mark-github"></span> Xinyi He</a></h1><button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button><nav class="site-header-nav" role="navigation"> <a href="https://buliangzhang24.github.io/" class="site-header-nav-item" target="" title="Home">Home</a> <a href="https://buliangzhang24.github.io/categories/" class="site-header-nav-item" target="" title="Categories">Categories</a> <a href="https://buliangzhang24.github.io/archives/" class="mobile-hidden site-header-nav-item" target="" title="Archives">Archives</a> <a href="https://buliangzhang24.github.io/fragments/" class="site-header-nav-item" target="" title="Fragments">Fragments</a> <a href="https://buliangzhang24.github.io/wiki/" class="site-header-nav-item" target="" title="Projects">Projects</a> <a href="https://buliangzhang24.github.io/links/" class="mobile-hidden site-header-nav-item" target="" title="Useful Links">Useful Links</a> <a href="https://buliangzhang24.github.io/about/" class="site-header-nav-item" target="" title="About">About</a> <a class="mobile-hidden" href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></nav></div></header><style> .home .banner, .home .site-header, .home .banner .collection-head, .home .banner .collection-head a, .home .site-header h1 a, .home .site-header .site-header-nav-item:hover { color: #fff; } .home .site-header .site-header-nav-item { color: rgba(255, 255, 255, .7); } .home .banner, .home .site-header { background-color: #4183c4; } .home .banner .collection-head { background: 0 0; box-shadow: none; -webkit-box-shadow: none } .home .site-header { border-bottom: none } @media (max-width:50em) { .home .collapsed .icon-bar { background-color: white; } .home .collection-head .collection-header { font-size: 1.25em; } }</style><section class="banner"><div class="collection-head"><div class="container"><div class="columns"><div class="column two-thirds"><div class="collection-title"><h1 class="collection-header" id="sub-title"><span>Passionate GISer, Remote Sensing Enthusiast and Point Cloud Processor</span></h1><div class="collection-info"> <span class="meta-info mobile-hidden"> <span class="octicon octicon-location"></span> Wagenningen, Netherlands </span> <span class="meta-info"> <span class="octicon octicon-mark-github"></span> <a href="https://github.com/Buliangzhang24" target="_blank">Buliangzhang24</a> </span></div></div></div><div class="column one-third mobile-hidden"><div class="collection-title"></div></div></div></div></div></section><section class="container content"><div class="columns"><div class="column two-thirds" ><ol class="repo-list"><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/13/MachineLearning-8.-Support-Vector-Machines-and-Kernels/">MachineLearningï½œ8. Support Vector Machines and Kernels</a></h3><p class="repo-list-description"> ![[cd673ddf50ef87de550f7a2cba69a53.png]] Separating Hyperplanes when you want to find a decision boundary, avoid estimating densities linear decision boundry,å°±æ˜¯è®¾ç½®è¿™ä¸ªå¼å­ä¸º0ï¼Œå°±æ˜¯å®ƒçš„decision boundry hyperplanes separate feature space into regions ![[ca46b3d627febe82dbbee7126a1c74a.png]] for new X, å¯ä»¥å¸¦å…¥æœ€åä¸€ä¸ªå¼å­ï¼Œç„¶åçœ‹y=1,è¿˜æ˜¯y=-1æ¥åˆ†ç±» exercise1. This problem involves hyperplanes in two dimensions. (a) Sketch the hyperplane 1 + 3X1 âˆ’ X2 = 0. Indicate the set of points for which 1 + 3X1 âˆ’ X2 &gt; 0, as well as the set of points for which 1 + 3X1 âˆ’ X2 &lt; 0. (b) On the same plot, sketch the hyperplane âˆ’2 + X1 + 2X2 = 0. Indicate the set of points for which âˆ’2 + X1 + 2X2 &gt; 0, as well as the set of points for which âˆ’2 + X1 + 2X2 &lt; 0. Maximum margin classifiers ï¼ˆæ€ä¹ˆæ‰¾ä¸€ä¸ªæœ€ä½³çš„hyperplaneï¼‰ generalization link to regularization for generalization, the decision boundary should lie between the class boundaries Maximize perpendicular distance(æœ€å¤§åŒ–å‚ç›´è·ç¦») between the decision boundary and the ==nearest observations==: the margin M the decision boundary then only depends on a few points on the margin, the support vectors:if you remove one from other ob, nothing changes æ‰€ä»¥ï¼Œ leave one out will fail Construction maximize the marginï¼ˆå°è¯•æœ€å¤§è¾¹é™…åŒ–ï¼‰, under the constraint that training observations are classified correctly Limitation â— separable classes â— linear separability â— two classes Problems Maximum margin classifier is prone to ==overfitting==: very sensitive to training set When classes ==overlap==, separating hyperplane does not exist We need to make a trade-off between errors on the training set and predicted performance on the test set (generalization) Solution: The soft margin ä¸ºäº†è§£å†³ä¸Šé¢çš„é—®é¢˜ Solution: ==allow (some, small) errors on the training set,==introducing slack ï¼ˆæ¾å¼›ï¼‰variables âˆŠi â‰¥ 0Â»&gt;Add slack variables to maximum margin classifier, but limit total slack to C: the trade-off parameter å˜åŒ–ï¼šMÂ» M(1-e)ï¼Œç„¶åå¼•å…¥ä¸€ä¸ªCï¼Œè¯¯å·®å¹³æ–¹å’Œ&lt;=C C influence solution: There is no a prori best choice for C ![[7b9e34d7c7461765eca88a763603261.png]] The multi-class case one-versus-one one- versus-all exercise ![[b5b665fde36990c4b2cb1fd1525e3e6.png]] Optimization(optional) ä»æœ€å¤§åŒ–M(margin)åˆ°æœ€å°åŒ–ä¸€ä¸ªç³»æ•°wå’Œå¸¸æ•°bçš„å¹³æ–¹å’Œ ä½†æ˜¯ä¸ºäº†è§£å†³is a (large) quadratic programming (QP) problem åˆå¼•å…¥äº†lagrange multipliers alphai The nonlinear case Ideal: classes may become linearly separable if higher order terms are added(cf. nonlinear regression) ![[22e573c5252d8db1948d586ddaa54dd.png]] Questions featureæ˜¯è¦å¹³æ–¹è¿˜æ˜¯å¼€æ–¹ï¼Œç­‰ç­‰ï¼Œæˆ‘ä¸çŸ¥é“ efficiently train the SVC,å°±æ˜¯p= 10 ,å¦‚æœè¦3æ¬¡ï¼Œå°±ä¼šæœ‰286ç§ç»„åˆæ–¹å¼äº† The Kernel Trick ![[4a4865d0f12df10785319a4f7353e67.png]] è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„å†…ç§¯ï¼Œå¦‚ xi å’Œ xiâ€™ ä¹‹é—´çš„å†…ç§¯ã€‚è®­ç»ƒå®Œæˆåï¼Œå½“æˆ‘ä»¬éœ€è¦å¯¹æ–°çš„æ ·æœ¬è¿›è¡Œåˆ†ç±»æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—æ”¯æŒå‘é‡ (support vectors) ä¸æ–°æ ·æœ¬ä¹‹é—´çš„å†…ç§¯ã€‚ ![[bc20a49a601a7f34eaf96a7d8938959.png]] å¼•å…¥ä¸€ä¸ªç”¨äºæ³›åŒ–çš„æ ¸å‡½æ•°Think of kernel functions as similarities: large when the inputs are very alike, small when they are not ![[ff68f02e96fdb61695f150a047643ef.png]] ![[50b2ca5c56b26fa1a3d05635b85e2dd.png]] åˆ°è¿™ä¸€æ­¥äº†ï¼Œæ‰æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥é€‰æ‹©Kï¼ˆx_i, xï¼‰ Polynomial kernel Radial kernel The support vector machine Choosing Kernels What kernel functions should we use? type: prior knowledge of problem, trial-and-error parameters: cross-validation, like for C exercise ![[f915649b7e14afa4dc5ab759f53800c.png]] More Kernels A large number of kernels have been proposed,not limited to numerical/vector data! â— Vector kernels â— Set kernels â— String kernels â— Empirical kernel map â— Kernel kernels â— Kernel combination â— Kernels on graphs â— Kernels in graphs â— Kernels on probabilistic models Recap Separating hyperplane: any plane that separates classes The support vector machine has evolved from fundamental work by Vapnik: separable, linear problems: maximum margin classifier non-separable, linear problems - add slack variables:support vector classifier non-separable, non-linear problems - use kernel trick:support vector machine Training involves quadratic programming (optimization) The final classifier only depends on the support vectors</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/13 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/12/MachineLearning-7.Decision-Tree-to-Random-Forests/">MachineLearningï½œ7.Decision Tree to Random Forests</a></h3><p class="repo-list-description"> Decision Trees We categorize data through chain of simple decisions. a binary tree, which is a graph data structure in which each node has at most two children Terminology input features: data to be classified root node: starts the decision process decision nodes: Each decision node (split node) implements a test function with discrete outcomes The test function of each decision node splits the input space into regions leaf nodes: A leaf node indicates the end of a sequence of decisions A single (output) class is associated to each leaf node Algorithm to make predictions ![[66c68d76a596659140c2d38d0326bbb.png]] Increasing node purity every decision should increase the node purity Quantifying Node Purity Measuring Node Purity Low Entropy: more pure;Higher Entropy: less pureÂ»H(p) how flat Gini Index: higher gini index ,less pure ![[f365b77a8cb7cc4a22d13ad62d2231e.png]] ![[c4449526b1dcb9a2458ef1c41d9fc19.png]] 0.25* 0.75 4 times 0.45* 0.55 2 times + 0.05 * 0.95 2 times Regression Trees leaf nodes return the average sample variance as node purity,high variance before splitting, low variacne after splitting Classification versus Regression Trees How to find the thresholde t? Try different thresholds optimum as best recordede threshold![[f4313c7f3b7060b0b96d5e09bb6fc92.png]] Binary Splitting we test as many thresholds as we have points ![[ab2533bf07de0f47d6c10fc874cc20f.png]] Growing a Decision tree with Recursive Binary Splitting: perform binary splitting recursively for each decision node to maximize node purity We end uo with one sample per node = a 1-NN regressor/classifier Overfitting of DTs Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Regularization of DTs Restricting Depth Tree fits training data perfectly (zero variance) Tree extremely large, and uninterpretable Tree Pruning Slow Growsï¼ˆé™åˆ¶ç”Ÿé•¿ï¼Œgreedyï¼‰ Strategy: Grow first the tree slowly with additional checks during growing â€¢ Simple checks: maximum depth,max number of leaf nodes. â€¢ Add decision node only if increase in node purity is above a certain threshold Fast Cutters(å…ˆé•¿å†åˆ‡, non-greedy) Strategy: Grow the full tree and then remove not important nodes later. â€¢ Cost complexity pruning: we identify the weakest link in the tree to remove Cost Complexity Pruning Remove leafs to reduce the complexity, while maintaining a low training error, i.e., cost. The hyperparameter ğ›¼ balances both objectives. Clamada(T) = C(T) + lamada T å°±æ˜¯cost(training error)+complexity(number leafs) higher alpha: produces shallower tree with less nodes å¦‚ä½•åˆ©ç”¨ cost complexity pruningæ¥æ„å»ºæ ‘ ![[638c2052311e5c539c5d454e45e1a73.png]] Advantages and Disadvantages of Trees Advantages: â— Inexpensive to construct â— Extremely fast at classifying unknown records â— Easy to interpret for small-sized trees â— Robust to noise (especially when methods to avoid overfitting are employed) â— Can easily handle redundant or irrelevant attributes (unless the attributes are interacting) Disadvantages: â— Space of possible decision trees is exponentially large. Greedy approaches are often unable to find the best tree. â— Does not take into account interactions between attributes â— Each decision boundary involves only a single attribute â— too deep ,more sensitive to noise Bagging, Forest,Boosting Baggingæ˜¯åœ¨å¹²å•¥ï¼š we split the feature space into regions The modes(i.e., centers) of the data distributions are well assigned to the classes. But the uncertain overlapping region is a mess low bias each tree fiting vey well high variance &gt; difference between different trees higher Bias - Variance Trade-off: Finding a balance between fitting and overfitting for tree-like classifiiers Strategy 1: Reduce DT complexity Greedy (restrict DT complexity) Non-greedy (post-hoc pruning of the decision tree) Stragegy 2 Ensemble multiple Decision Trees togetheré›†æˆå¤šä¸ªå†³ç­–æ ‘ï¼šæ£®æ—ï¼ï¼ Step 1 Bagging split different data subset: randomly sampled subsets (bootstrap) ensemble prediction: majority voting for classification output averaging for regression effect: Decision boundaries become blurry: more realistic boundary Out-of-Bag Error Estimation Process: Fit decision tree DT_i on ğ’Ÿ_i , Test its performance on the out-of-bag set ğ’Ÿ_oob Repeat for each tree in the bag Idea: - This OOB error approximates the test error. Similar idea to cross-validation, but for every DT separately Re-gaining Interpretability Bagging trees improves accuracy over a single tree through increasing diversity in the decisions æ„æ€å°±æ˜¯éšæœºæ€§å¢åŠ äº†ï¼Œæ‰€ä»¥å¢åŠ äº†ç²¾åº¦ But we loose interpretability: Each tree has different decision rules Interpreting many decision rules in parallel is difficult Strategy: Measure the increase in node purity across trees ![[7010e107b505f8fef371cbe4e3f6219.png]] å°±æ˜¯çœ‹æ¯ä¸ªæ„Ÿå…´è¶£ç‚¹ï¼ˆæ©˜è‰²ç‚¹ï¼‰çš„node purityï¼Œç„¶åæ±‚å¹³å‡ if bagging trees are still highly correlated with each other ![[Pasted image 20240226091312.png]]</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/12 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/11/MachineLearning-6.Model-selection-and-Regularization/">MachineLearningï½œ6.Model selection and Regularization</a></h3><p class="repo-list-description"> Motivation prediction accuracy avoid overfitting ç‰¹åˆ«æ˜¯p&gt;n çš„æ—¶å€™ï¼Œå°±æ˜¯å˜é‡å¤§äºæ ·æœ¬é‡ model interpreteability lower practical requirements:fewer measurements, less computation visualization, if selecting 2-3 features Introduction linear model, least squares minimization Solution feature subset selection: select feature extraction(PCA): map to shrinkage: adjust coefficients so that some features are used to a lesser extent (or not at all) regularization: reducing the model flexibility Feature subset selection Basic approach: select the best features Problem: the k best individual features are not necessarily the best set of k features(bc, the corelated ) Best Subset Selection(Exhaustive) Search space 2^p -1 subsets Why not try all possible subsets then? è™½ç„¶å°è¯•æ‰€æœ‰å¯èƒ½çš„å­é›†æ˜¯ç†è®ºä¸Šçš„å®Œç¾æ–¹æ³•ï¼Œä½†åœ¨å®é™…æƒ…å†µä¸‹ï¼Œç”±äºæœç´¢ç©ºé—´çš„å·¨å¤§ï¼Œè¿™å‡ ä¹æ˜¯ä¸å¯è¡Œçš„ algorithm: ä¸ºäº†åœ¨è¿™ä¸ªåºå¤§çš„æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³çš„å­é›†ï¼Œæœ€ä½³å­é›†é€‰æ‹©æ–¹æ³•é€šå¸¸ä½¿ç”¨è´ªå©ªç®—æ³•ã€‚è¯¥ç®—æ³•ä»ç©ºé›†å¼€å§‹ï¼Œç„¶ååœ¨æ¯ä¸€æ­¥ä¸­æ·»åŠ ä¸€ä¸ªç‰¹å¾ï¼Œä»è€Œæ„å»ºä¸€ä¸ªç‰¹å¾å­é›†ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•è¯„ä¼°å½“å‰ç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©å¯¹æ€§èƒ½æœ‰æœ€å¤§è´¡çŒ®çš„ç‰¹å¾æ·»åŠ åˆ°å­é›†ä¸­ã€‚è¯¥è¿‡ç¨‹ä¸€ç›´æŒç»­åˆ°è¾¾åˆ°æ‰€éœ€çš„ç‰¹å¾æ•°é‡kä¸ºæ­¢ã€‚ let M_0 denote the null model for k =1,2â€¦.p&gt;fit all models that use k predictors&gt; M_k is the best of these(smallest RSS or largest R^2)&gt; select a single best model from M_0â€¦.M_p based on CV error, C,AIC,BIC,adjusted R^2 ç¼ºç‚¹ computationally intensive when ğ‘ is large, risk finding models that do well on training data but do not generalize well Stepwise Selection Forward Selection start with no predictors, iteratively add predictors until a stop criterion is reached Computational advantage is clear, but may miss optimal subset</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/11 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/10/MachineLearning-5.-Resampling-methods/">MachineLearningï½œ5. Resampling methods</a></h3><p class="repo-list-description"> Motivation Bias and Variance Prediction errors are due to Bias: the inability of the model to capture essential interactions, processes, or dynamics of the true system. Bias is associated with ==non-flexibility==, or ==lacking information in the data set== Variance: sensitivity of coefficients towards errors or variations in the data. Variance is associated with ==flexibility.== Irreducible errors: measurement errors that cannot be predicted ä¸ºä»€ä¹ˆ test ä¼šUå½¢ï¼Œtraining éšç€flexibility ä¸‹é™ The challenge lies in finding a method for which both the variance and the squared bias are low. ä¸¤ä¸ªè¯„ä»·æ–¹é¢ Model assessment (performance) Model selection (flexibility) Test error and training error test error:used in its training training error: not used in its training å…¶å®ä¸‹é¢è¿™ä¸ªé—®é¢˜å°±æ˜¯é‚£ä¸ªå›¾ The training error rate often is quite different from the test error rate, and in particular the training error can considerably ==underestimate== the test error What is the most likely cause, bias or variance? Variance, a sensitivity towards the configuration of the data (training error is small, test error is large) Best estimation of test error: ==a large== designated test set. (å¤§é‡çš„test set) Sometimes not available mathematical ==adjustment== to the training error rate in order to estimate the test error rate For example: R2adjusted. Others: Cp statistic, AIC and BIC In this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations The Validation Set Approach Here we ==randomly== divide the available set of samples into two parts: a ==training set ==and a ==validation or hold-out set== Drawbacks test error is highly variable, and only half of the obsevationsÂ Â» hence, the validation set error tends to overestimate the test error 50% of the data: inability to incorporate all information from the data set (bias) Leave one out cross validation Steps spliting the set of ob into two parts single ob is used for the validation set remaining ob make up the training set prediction y^_1 ,using x_1 Since (x1, y1) was not used in the fitting process, MSE1 = (y1 âˆ’ y Ì‚1)2 provides an approximately unbiased estimate for the test error repeat MSE1â€¦.MSEn - average these: CV(n) Advantages and Disadvantages (compared to the validation set approach) It tends not to overestimate the test error rate as much as the validation set (there is no randomness in the training/validation set splits) LOOCV has the potential to be ==expensive== to implement, since the model has to be fit n times (if n is large, and if each individual model is slow to fit) k-Fold Cross-Validation step divide data into K roughly equal-sized parts We leave out part k, fit the model to the other K âˆ’ 1 parts (combined), and then obtain predictions for the left-out kth part CV(K) Algorithm ![[ff05b5c22a054f8e811eee32315b7f9.png]] we are interested only in the location of the minimum point in the estimated test MSE curve K-fold cross-validation Since each training set is only (KÂ âˆ’Â 1)/K as big as the original training set, the prediction error will typically be biased upwards. Why? ==Bias is increased by reducing the data set == This ==bias is minimized when KÂ =Â n (LOOCV), but this estimate has high variance ==(since all training sets are nearly equal, so the MSEâ€™s are highly correlated, and depend much on which data set is used) K = 5 or 10 provides a good compromise for the bias-variance trade-off ![[8f1efdf723e520b4db9ecbbc0d4050b.png]] The Bootstrap data set is too small The bootstrap can be used to ==estimate the uncertainty associated with a given estimator== or statistical learning method The bootstrap works by==repeatedly== sampling from the same data set, thereby creating multiple data sets å°±æ˜¯æœ‰æ”¾å›æŠ½æ · ![[f6a78ccd91fcac3b53a4b82b8704b55.png]] minimize the risk (åæ­£å°±æœ‰ä¸ªå…¬å¼) Time series if the data is a time series, we canâ€™t simply sample the observations with replacement. Why not? The data is correlated over time; this correlation represents the dynamics. Randomly resampling will completely alter the dynamics</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/10 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/07/MachineLearning-4.Bayesian-Classification-KNN,LDA-and-QDA/">MachineLearningï½œ 4.Bayesian Classification KNN LDA and QDA</a></h3><p class="repo-list-description"> KNN,LDA and QDA are Bayesian classification Classification Finding class k that maximizing the conditional probability ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€å¼ å›¾ç‰‡åˆ†ç±»ä¸ºç‹—ã€çŒ«ã€æˆ–é¸Ÿï¼Œé‚£ä¹ˆä½ å¯èƒ½ä¼šå¾—åˆ°æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œæ¯”å¦‚ï¼š ç‹—ï¼š0.8 çŒ«ï¼š0.1 é¸Ÿï¼š0.05 åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒPæœ€å¤§åŒ–çš„ç±»åˆ«æ˜¯ç‹—ï¼Œå› ä¸ºå®ƒæœ‰æœ€é«˜çš„æ¦‚ç‡ã€‚ K-nearest neighbor classifier ![[3e88de363372e84e89e86b1b6e3e538.png]] non-parametric what probability the point belongs to the class j,as the fraction of points in ==N_0== whose reponse values equal ==j== N_0 contains the ==K== points that are closest to x_0 Example ![[8901136cb31a6453aa079dee0193561.png]] K= 3 è¡¨ç¤ºæœ‰ä¸‰ä¸ªæœ€è¿‘çš„ç‚¹åœ¨è¿™ä¸ªsample ç‚¹çš„é™„è¿‘ the region of the cricle :the last nearest neighbor Size of the green circle varies, depending on the location of the black cross x1 10000 but x2 78. the scale , the dominated by x1 Decision boundary ![[f4558634bdc194c0c0d463969e5fa1d.png]] Corresponding KNN ==decision boundary== P(Y = blue | X) =P(Y = orange | X) = 0.5 if there are three classesï¼Œ=0.33 è¿­ä»£error, test overfiting on the nosiy We have the Bayes Decision boundary automatically How to choose K K= 1: decision boundary overly flexible high variance ,overfitting K=100: decision boundary close to linear high bias underfitting K= 10 just right in this case only ==validation set== to choose which K is the best Training and test error ![[8c4be1a107e87038c7033c72ebe0d80.png]] 1/K related to flexibility of kNN K smaller, 1/K bigger, more flexible</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/07 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/05/MachineLearning-3.Logistic-Regression/">MachineLearningï½œ3.Logistic Regression</a></h3><p class="repo-list-description"> used for classification Motivation linear regression quantitative vs. qualitative(categorical) data quantitative data &gt; regression quantitative Y qualitative data&gt; classification qualitative Y class and regression can be used together Why not linear regression in case of variable Y with &gt;2 categories: Coding categories as numbers (ordering and distance between categories) is arbitrary In case of variable Y with 2 categories: Coding with 0/1 would work Even with 2 categories there is an issue: Example: credit card default / not default ä½†æ˜¯è¿˜æ˜¯ä¸å¥½predict probability The Logistic Model main idea: predict probability P(Y=1| X=..) Logistic function S-shaped curve between 0 and 1: ![[Pasted image 20240214104238.png]] è¿™é‡ŒLinear regression èŒƒå›´ä¼šå°äº0ï¼Œä½†æ˜¯å®ƒçš„æ„ä¹‰æ˜¯æ¦‚ç‡ How do we see that the curves have same B0? They cross the x= 0 at same point Is B0 &gt;0 or B0 &lt;0? if at the mentioned point the Y value is smaller than 0.5 , the B0&lt;0 ![[f241f122c9709f63d673fddf84a0a00.png]] temperature of meidian low and high Odds The ratio of the probability of one event to that of an alternative event ![[Pasted image 20240214110223.png]] Logistic function leads to logit that is linear in X Logit = log(Odds) ![[Pasted image 20240214110419.png]] B1 in linear regression: gives average change of Y associated with one-unit increase in X B1 in logistic regression: gives average change of log odds (logit) with one-unit increase in X</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/05 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/03/MachineLearning-2.-Linear-Regression/">MachineLearningï½œ2. Linear Regression</a></h3><p class="repo-list-description"> Simple linear regression assume a model : (Yâ‰ˆÎ²_0+Î²_1X) Î² Ì‚0intercept Î² Ì‚1 slope Given some estimates Î² Ì‚0 and Î² Ì‚1, we can predict future sales y Ì‚=Î² Ì‚0+Î² Ì‚1x</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/03 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/02/MachineLearning-1.-Statistical-learning/">MachineLearningï½œ1. Statistical learning</a></h3><p class="repo-list-description"> Y= f(X)+e Y: (response, dependent variable, predicted value) X: (predictor,independent variable,feature) f: unknow relationship e: random error(1. with å¹³å‡å€¼ç­‰äº0 mean zero 2. æ¨¡æ‹Ÿäº†ä¸ªä½“ä¹‹é—´çš„å·®å¼‚ï¼‰ the multivariate caseï¼šUsually more than 2 input dimensions! p: number of input dimensions è¦åˆ†æå‡ ä¸ªå˜é‡ n: number of data points(samples in the data) in a sample æ ·æœ¬é‡ Prediction æ˜¯å•¥ï¼Ÿå°±æ˜¯æ‰¾åˆ°Yå°±è¡Œäº†ï¼Œfè¿™ä¸ªå…³ç³»å¯ä»¥æ˜¯ä¸€ä¸ªé»‘ç›’ Y and f are ==unknown== , so we estimate f in order to predict Y from konwn X values æœ‰å¸½å­çš„f å’ŒYæ˜¯estimated / predicted f is ==estimated== using ==training data==, consisting of X values and corresponding Y values. Then, the Y values can be ==predicted== for new X values å¦‚æœè¯´e çš„å¹³å‡å€¼æ˜¯0, é‚£ä¹ˆ Y^ = f(X)^ Error of the model Y- Y with hat Can be estimated from the data set: mean squared error: 1/N sum(yi-yi with hat)^2 Reducible and irreducible error reducible : change the ==learning techniques and models ==and ==better training data==Â Â» minimized while estimating f (inference) irreducible: cannot be reduced because of ==unmeasured but relevant inputs ==or ==unmeasured variation(nosie)==Â» set upper bound on the accuracy of predicting Y(prediction) Inference å°±æ˜¯è™½ç„¶ä¿ºä»¬ä¹Ÿé¢„æµ‹äº†Y,ä½†æ˜¯é‡è¦çš„æ˜¯æ‰¾åˆ°f å…³ç³»çš„æ–¹ç¨‹ estimate f , but ==understanding how== X influence Y Do not treat f with hat as black box Summary Prediction: estimating f^ to get good prediction of Y^ Inference: estimating f ^ to get an understanding of the relationship between X1-Xp and Y Prediction accuracy vs. model interpretablity é¢„æµ‹å‡†ç¡®åº¦ï¼šé¢„æµ‹å‡†ç¡®åº¦æ˜¯æŒ‡æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ç»“æœä¸å®é™…è§‚æµ‹å€¼ä¹‹é—´çš„ä¸€è‡´ç¨‹åº¦ æ¨¡å‹è§£é‡Šåº¦ï¼šçº¿æ€§å›å½’æ¨¡å‹é€šå¸¸å…·æœ‰å¾ˆé«˜çš„è§£é‡Šæ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥æ˜ç¡®åœ°è¡¨ç¤ºå˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼Œå¹¶ä¸”å¯ä»¥è§£é‡Šæ¯ä¸ªè‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“ç¨‹åº¦ ==linear models==: high interpretability and sometimes high accuracyÂ»inference ==highly non-linear==: low interpretability, high accuracyÂ»prediction Choice dependsï¼šprediction or inference Parametric methods choose the functional form of f, then learn its parameters from training data,using least squares or a differenct method å°±æ˜¯å…ˆå‡è®¾ç¡®å®šäº†ä¸€ä¸ªæ–¹æ³•å»ä½œä¸ºfï¼Œç„¶åæ±‚å‚æ•° ad: much easier to estimate a set of parameters than to fit an arbitrary function ==less training data== dis: if the chosen functional form is too far from the truth, prediction and inference results can be poor &gt;we donâ€™t know the relationship in advance Non parametric based on training data itselféå‚æ•°æ–¹æ³•ä¸éœ€è¦äº‹å…ˆç¡®å®šæ¨¡å‹çš„å‡½æ•°å½¢å¼æˆ–è€…å‚æ•°çš„æ•°é‡ï¼Œè€Œæ˜¯ä»æ•°æ®ä¸­ç›´æ¥å­¦ä¹ æ¨¡å‹çš„ç»“æ„ã€‚è¿™ä½¿å¾—éå‚æ•°æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„æ•°æ®ç»“æ„æˆ–è€…å¯¹æ•°æ®åˆ†å¸ƒäº†è§£ä¸å……åˆ†æ—¶æ›´å…·æœ‰çµæ´»æ€§ã€‚ ad: ==good fit== ,even if the input-output relations are complex dis: require much ==more training data== risk of ==overfitting== modelling äº† the nosie e (donâ€™ t have enough data) Overfittingæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®ï¼ˆæˆ–æ–°æ•°æ®ï¼‰ä¸Šè¡¨ç°è¾ƒå·®çš„ç°è±¡ï¼Œå› ä¸ºå™ªå£°ç­‰Â»æŒ‡æ¨¡å‹è¿‡äºå¤æ‚ï¼Œè¿‡åº¦æ‹Ÿåˆäº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°å’Œç»†å¾®ç‰¹å¾çš„æƒ…å†µã€‚ Overfitting çš„åŸå› æœ‰ï¼š æ¨¡å‹è¿‡äºå¤æ‚ï¼Œè®­ç»ƒæ•°æ®é‡ä¸è¶³ï¼Œç‰¹å¾é€‰æ‹©ä¸å½“ï¼Œè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„åˆ†å¸ƒä¸ä¸€è‡´ ![[737a24037115d610d6b55c525374370.png]] Supervised vs Unsupervised Learning æœ€å¤§åŒºåˆ«åœ¨æœ‰æ²¡æœ‰æ ‡ç­¾ Assessing Model Accuracy Measuring the quality of fit å›å½’å’Œåˆ†ç±»çš„åŒºåˆ« å›å½’ï¼šé¢„æµ‹è¿ç»­çš„å˜é‡ åˆ†ç±»ï¼šé¢„æµ‹ç¦»æ•£çš„å˜é‡ Mean squared error(regression) training MSE isnot important: å¢åŠ æ¨¡å‹çš„çµæ´»æ€§å¯ä»¥ä½¿å…¶æ›´å®¹æ˜“é€‚åº”å¤æ‚çš„æ•°æ®æ¨¡å¼å’Œå…³ç³»ï¼Œä»è€Œå¯èƒ½é™ä½è®­ç»ƒ MSEã€‚more flexiblity less taining MSEã€‚ test MSE(unseen) different parts of field ==goal: select the model with the smallest test MSE== ![[8edabe3d0987a7a37cb37de6b2494ca.png]] Underfitting:æ¨¡å‹è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰æ•°æ®ä¸­çš„çœŸå®æ¨¡å¼å’Œå…³ç³»çš„æƒ…å†µã€‚æ¬ æ‹Ÿåˆçš„æ¨¡å‹é€šå¸¸å¯¹è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„è¡¨ç°éƒ½è¾ƒå·® Overfitting:æ¨¡å‹è¿‡äºå¤æ‚ï¼Œè¿‡åº¦æ‹Ÿåˆäº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°å’Œç»†å¾®ç‰¹å¾çš„æƒ…å†µã€‚è¿‡æ‹Ÿåˆçš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æœªè§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°è¾ƒå·® Bias vs. variance ![[860be21fdebfc9318057e0881fc7d5d.png]] å› ä¸ºBiaseå’Œvarianceï¼Œæ‰€ä»¥MSEæ‰æ˜¯Uå½¢çš„ Bias : model too simple Bias refers to the error that is introduced by approximating a real-life problem by a too simpler modelçœŸå®å€¼å’ŒæœŸæœ›å€¼ä¹‹é—´çš„å·®å¼‚ æ— è®ºæˆ‘ä»¬ä½¿ç”¨å¤šå°‘è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸ªè¯¯å·®éƒ½ä¼šå­˜åœ¨ more flexible methods have less bias Variance: model too complex å¦‚æœæ¢æˆä¸åŒçš„training sampleï¼Œ fçš„é¢„æµ‹æœ‰å¤šå¤§çš„å˜åŒ– å¦‚æœmodel æœ‰High varianceï¼Œå°±æ˜¯å¾ˆå°çš„å˜åŒ–éƒ½ä¼šåœ¨fçš„é¢„æµ‹ç»“æœä¸Šäº§ç”Ÿå¾ˆå¤§çš„å˜åŒ–æ–¹å·®è¶Šå¤§æ„å‘³ç€æ¨¡å‹å¯¹æ•°æ®çš„å˜åŒ–æ›´æ•æ„Ÿã€‚ more flexible methods have higher variance ==more flexible, less bias, more variance== Good test set performance requires ==low variance as well as low squared bias.== ![[04ce4e8bb2e16bc6e413ec0d16581f9.png]] (a) inflexible biaseÂ Â»fiexible Flexible is generally better. A flexible method has many degrees of freedom, so it can follow the patterns in the data, even if they are highly non-linear. If the data is more linear, there are enough data points to train the parameters so that the model turns out more linear as well. If flexibility is chosen extremely large, overfitting could still occur. å¤§æ ·æœ¬é‡ï¼šå¯¹äºæå¤§çš„æ ·æœ¬é‡ï¼Œçµæ´»çš„æ¨¡å‹å¾€å¾€è¡¨ç°æ›´å¥½ï¼Œå› ä¸ºå®ƒä»¬æœ‰æ›´å¤šçš„æ•°æ®å¯ä¾›å­¦ä¹  predictor é‡å°‘ï¼šå½“é¢„æµ‹å˜é‡çš„æ•°é‡è¾ƒå°‘æ—¶ï¼Œæ„å‘³ç€æ•°æ®å¯èƒ½å…·æœ‰ç®€å•çš„å…³ç³»ï¼Œè¿™ç§å…³ç³»å¯ä»¥ç”±çµæ´»çš„æ¨¡å‹å……åˆ†æ•è·è€Œæ— éœ€æ‹…å¿ƒè¿‡æ‹Ÿåˆ (b) inflexible There is a high risk of overfitting. (c) flexible (d) inflexible (flexible modelÂ» there lots of nosiy Â Â» fiting the nosiy) å°±æ˜¯ç”¨å¯¹äº†ï¼Œå°±æ˜¯low,lowã€‚ ![[b67f650f37b2f3d0d6f112246fcc8f5.png]] For Classification Setting Instead of MSE, we get ==error rate==:I= 1 if the pefect model Again, there is a ==training error rate and a test error rate==. They express the fraction of incorrect classificationsä¸æ­£ç¡®åˆ†ç±»çš„æ¯”ä¾‹ Training set and test set å°±æ˜¯ç”¨training set å»è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨è°ƒæ•´å‚æ•°çš„æ—¶å€™ç”¨validation set æ‰¾å“ªä¸ªå‚æ•°åˆé€‚ï¼ŒåŒæ—¶ä¹Ÿæ‰¾å“ªä¸ªæ¨¡å‹æ˜¯æ›´é€‚åˆè¿™ä¸ªæ•°æ®çš„ï¼Œå†å»test seté‡Œé¢éªŒè¯è¿™ä¸ªæ¨¡å‹å¥½ä¸å¥½ Training set to ==train the model== Validation set to ==optimize the hyper parameters== Test set to test the performance of the model on an ==independent ==part of the data set. To get an estimate on how good it will ==work in practice== with limited amount of data å°‘é‡æ•°æ® å¯ä»¥ç”¨ cross validation ![[74721a6689f929962208e7a3b8ca670.png]]</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/02 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/02/01/MachineLearning-0.-Introduction/">MachineLearningï½œ0. Introduction</a></h3><p class="repo-list-description"> What is machine learning? åˆ†ç±» supervised learning : classification and regression unsupervised learning: cluastering and structure Supervised learning the relationship f between input X(predictor,independent variable,feature) and output Y(response, dependent variable, predicted value) based on training data&gt; Y= f(X) Classification and regressionåŒºåˆ« 1. tomato ripe&gt; regression methods for regression: linear regression, decision trees/random forest/neural networks 2. object database&gt; representation/object featrues&gt; feature space(distance measure to deteccted object)&gt; when new image poped up, we can use this model to detect object methods for classification: Logistic regression K-Nearest Neighbors Linear/Quadratic Discriminant Analysisçº¿æ€§ã€äºŒæ¬¡åˆ¤åˆ«åˆ†æ Decision Trees/Random Forest Support Vector Machines Neural Networls Unsupervised learning learning structure, no Y, only X åˆ†ä¸ºï¼šclustering example(K-Means Clustering, Expectation Maximization,Hierarchical Clustering ) dimensionality reduction(Principal Component Analysis) Which model is the bestï¼Ÿ Model and feature selection How to optimally use the training /test data? Resampling methods Cross-validation Bootstrapping</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/02/01 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" title="MachineLearning">MachineLearning</a> </span></p></li><li class="repo-list-item"><h3 class="repo-list-name"> <a href="https://buliangzhang24.github.io/2024/01/25/Advanced-Earth-Observation-10.Marine-Applications/">Advanced Earth Observationï½œ10.Marine Applications</a></h3><p class="repo-list-description"> Harmful Algal Bloomsæœ‰å®³è—»å What is algal blooms Algal blooms are the rapid growth of algaeè—»ç±» or cyanobacteriaè“è—» å…‰åˆä½œç”¨photosynthesisï¼ŒçœŸæ ¸ç”Ÿç‰©eukaryotic organismsï¼Œ æµ·æ´‹å¾®å‹æœ‰æœºç‰©marine microorganisms Why it is harmful? recreational uses, toxins, ecosystem health What causes Algal Blooms Warm temperature Nitrogen and Phosphorusæ°®å’Œç£· No wind How can RS helpï¼Ÿ find algal bloomsÂ Â» detectable Most sensors have limitationsÂ»Resolution trade-offs: spatial, spectral, temporal True color is useful but not best Spectral library of cyanobacterialè“è—» in a laboratory setting Chlorophyll-a &amp; Phycocyanin (PC)è—»è“è›‹ç™½ unique for Cyano å†…å…±ç”Ÿç†è®ºendosymbiotic theoryï¼šæ¤ç‰©å’Œè—»ç±»çš„å¶ç»¿ä½“éƒ½æ˜¯èµ·æºäºä¸€ç§è“è—»çš„å†…å…±ç”Ÿä½“ What causes Red-edge in terrestrial vegetation? absorption of blue (400 nm) and red (700nm) due to CHL-a/bÂ» palisade: main site of photosynthetic absorption of red and blue light reflectance of near-infrared (&gt;750nm) due to leaf scatteringÂ Â» spongy: cell geometries scatter and reflect near infrared light Why is there only little red-edge in cyanobacteria? æ²¡æœ‰2ï¼Œåªæœ‰1ï¼Œå°±æ˜¯ä¸Šä¸€æ­¥çš„1,2 Compared to vegetation ==there is no NIR reflectance==è¿™å¥è¯æ„æ€å°±æ˜¯æ²¡æœ‰2 NDCIï¼š Normalized difference chlorophy II index Spectral Shape(SS) Indices Floating Algae Index(FAI) Fluorescence Line Height (FLH)è§å…‰çº¿é«˜åº¦ï¼š FLH is a measure of the absolute amount of energy released by phytoplankton in the form of fluorescence. function of the radiation absorbed by phytoplankton and the probability for a given absorbed photon to be re-emitted as fluorescence. indicators for CHL-a and Phycocyanin(PC) which bands needed will be based on sensers and types Cyano Index(CI) Marine Litter Detection Scales of marine plastics Macroplastics (&gt; 2.5 cm) Mesoplastics (5mm â€“ 2.5cm) Microplastics (&lt; 5mm) Marine Plastics are part of Marine Litter Marine Litterï¼šåƒåœ¾ consists of solid materials that have been made or used by people and deliberately discarded or unintentionally lost in in marine and coastal environments, such as wood, metals, glass, rubber, textiles, paper and plastics (UNEP, 2005). Marine Debrisï¼šåºŸå¼ƒç‰© as any aggregation of floating materials on the sea surface that may or may not contain marine litter of anthropogenicäººä¸º origins. The impact of Marine Plastics on the Ecosystem Marine micro-plastics can have a toxic effect on fish and other aquatic life: reducing food intake, delaying growth, causing oxidative damage and abnormal behavior. Scales of Monitoring of Marine Litter with Remote Sensing In-situ studies: counting and measuring individual objects macro and meso-plastics UAV-based surveys: counting of individual macro plastic objects Satellite Earth Observation: detection of agglomerations of generic â€œmarine debrisâ€ that can contain marine litter. River Plastic Monitoring Citizen Science-based beach surveys UAV Monitoring of Debris ==Remote Sensing Scales==å„ç§å°ºåº¦çš„ä¼˜ç¼ºç‚¹ UAV-based Pro: High spatial resolution(detection of individual objects) Contra: expensive to scale, usually single acquisition, usually only RGB Satellite-based Pro: daily or weekly monitoring, multi-spectral signals Contra: low spatial resolution, complex mixed pixels (especially on beaches) Monitoring of Agglomerationsèšé›† through surface currents ä¸ºä»€ä¹ˆï¼š Most objects are much smaller than the pixel size of available satellites Debris agglomerate in ==windrows== and ocean frontsåºŸå¼ƒç‰©èšé›†åœ¨==é£å †==å’Œæµ·æ´‹å‰ç¼˜ Remote Sensing Limitation: Trade-off between spatial and spectral resolution Floating Debris Index Floating Debris Index (FDI) as a Spectral Shape (SS) index designed for various heterogeneous marine debris.</p><p class="repo-list-meta"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2024/01/25 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://buliangzhang24.github.io/categories/#Advanced Earth Observation" title="Advanced Earth Observation">Advanced Earth Observation</a> </span></p></li></ol></div><div class="column one-third"><h3>Search</h3><div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"></div><ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul><script src="https://buliangzhang24.github.io/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://buliangzhang24.github.io/assets/search_data.json?v=1727806135', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script><h3>Categories Cloud</h3><a href="https://buliangzhang24.github.io/categories/#RemoteSensing" style="font-size: 15.5pt; color: #333;">RemoteSensing</a> <a href="https://buliangzhang24.github.io/categories/#GIS Tools" style="font-size: 9pt; color: #999;">GIS Tools</a> <a href="https://buliangzhang24.github.io/categories/#Advanced Earth Observation" style="font-size: 15.5pt; color: #333;">Advanced Earth Observation</a> <a href="https://buliangzhang24.github.io/categories/#MachineLearning" style="font-size: 18pt; color: #000;">MachineLearning</a> <a href="https://buliangzhang24.github.io/categories/#SQL" style="font-size: 13.5pt; color: #444;">SQL</a> <a href="https://buliangzhang24.github.io/categories/#ThesisNote" style="font-size: 9.5pt; color: #888;">ThesisNote</a></div></div><div class="pagination text-align"><div class="btn-group"> <a href="https://buliangzhang24.github.io/page2" class="btn btn-outline">&laquo;</a> <a href="https://buliangzhang24.github.io/" class="btn btn-outline">1</a> <a href="https://buliangzhang24.github.io/page2" class="btn btn-outline">2</a> <a href="jaavascript:;" class="active btn btn-outline">3</a> <a href="https://buliangzhang24.github.io/page4" class="btn btn-outline">4</a> <a href="https://buliangzhang24.github.io/page5" class="btn btn-outline">5</a> <a href="https://buliangzhang24.github.io/page6" class="btn btn-outline">6</a> <a href="https://buliangzhang24.github.io/page4" class="btn btn-outline">&raquo;</a></div></div></section><footer class="container"><div class="site-footer" role="contentinfo"><div class="copyright left mobile-block"> Â© 2024 <span title="Xinyi He">Xinyi He</span> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a></div><ul class="site-footer-links right mobile-hidden"><li> <a href="javascript:window.scrollTo(0,0)" >TOP</a></li></ul><a href="https://github.com/Buliangzhang24/Buliangzhang24.github.io" target="_blank" aria-label="view source code"> <span class="mega-octicon octicon-mark-github" title="GitHub"></span> </a><ul class="site-footer-links mobile-hidden"><li> <a href="https://buliangzhang24.github.io/" title="Home" target="">Home</a></li><li> <a href="https://buliangzhang24.github.io/categories/" title="Categories" target="">Categories</a></li><li> <a href="https://buliangzhang24.github.io/archives/" title="Archives" target="">Archives</a></li><li> <a href="https://buliangzhang24.github.io/fragments/" title="Fragments" target="">Fragments</a></li><li> <a href="https://buliangzhang24.github.io/wiki/" title="Projects" target="">Projects</a></li><li> <a href="https://buliangzhang24.github.io/links/" title="Useful Links" target="">Useful Links</a></li><li> <a href="https://buliangzhang24.github.io/about/" title="About" target="">About</a></li><li><a href="https://buliangzhang24.github.io/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li></ul></div></footer><div class="tools-wrapper"> <a class="gotop" href="#" title="å›åˆ°é¡¶éƒ¨"><span class="octicon octicon-arrow-up"></span></a></div><script src="https://buliangzhang24.github.io/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script></body></html>
